{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is/kosuke-t/.pyenv/versions/3.8.13/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import abc\n",
    "import glob\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import tarfile\n",
    "import urllib\n",
    "import urllib.request\n",
    "import subprocess\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import sys\n",
    "from logging import getLogger\n",
    "import pandas as pd\n",
    "import six\n",
    "import copy\n",
    "import random\n",
    "# import tensorflow.compat.v1 as tf\n",
    "from distutils.dir_util import copy_tree\n",
    "from logging import getLogger, StreamHandler, DEBUG\n",
    "logger = getLogger(__name__)\n",
    "handler = StreamHandler()\n",
    "handler.setLevel(DEBUG)\n",
    "logger.setLevel(DEBUG)\n",
    "logger.addHandler(handler)\n",
    "logger.propagate = False\n",
    "from transformers import AutoTokenizer\n",
    "from polyleven import levenshtein\n",
    "from tqdm import tqdm\n",
    "from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_EVEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WMT_LOCATIONS = {'15': {\"eval_data\": (\"DAseg-wmt-newstest2015\", \n",
    "                                      \"DAseg-wmt-newstest2015.tar.gz\",\n",
    "                                      \"http://www.computing.dcu.ie/~ygraham/\")},\n",
    "                 '16': {\"eval_data\": (\"DAseg-wmt-newstest2016\", \n",
    "                                      \"DAseg-wmt-newstest2016.tar.gz\",\n",
    "                                      \"http://www.computing.dcu.ie/~ygraham/\")},\n",
    "                 '17': {\"full_package\":(\"wmt17-metrics-task-no-hybrids\", \n",
    "                                        \"wmt17-metrics-task-package.tgz\",\n",
    "                                        \"http://ufallab.ms.mff.cuni.cz/~bojar/\")},\n",
    "                 '18': {\"submissions\":(\"wmt18-metrics-task-nohybrids\", \n",
    "                                       \"wmt18-metrics-task-nohybrids.tgz\",\n",
    "                                       \"http://ufallab.ms.mff.cuni.cz/~bojar/wmt18/\"),\n",
    "                        \"eval_data\": (\"newstest2018-humaneval\", \n",
    "                                      \"newstest2018-humaneval.tar.gz\",\n",
    "                                      \"http://computing.dcu.ie/~ygraham/\")},\n",
    "                 '19': {\"submissions\": (\"wmt19-submitted-data-v3\",\n",
    "                                        \"wmt19-submitted-data-v3-txt-minimal.tgz\",\n",
    "                                        \"http://ufallab.ms.mff.cuni.cz/~bojar/wmt19/\"),\n",
    "                        \"eval_data\": (\"newstest2019-humaneval\", \n",
    "                                      \"newstest2019-humaneval.tar.gz\",\n",
    "                                      \"https://www.computing.dcu.ie/~ygraham/\")},\n",
    "                 '20': {\"submissions\":(\"WMT20_data\", \"\", \"https://drive.google.com/drive/folders/1n_alr6WFQZfw4dcAmyxow4V8FC67XD8p\"), \n",
    "                        \"eval_data\":(\"wmt20-metrics\", \"\", \"https://github.com/WMT-Metrics-task/wmt20-metrics\"), \n",
    "                        \"MQM\":(\"wmt-mqm-human-evaluation\", \"\", \"https://github.com/google/wmt-mqm-human-evaluation\"), \n",
    "                        'PSQM':(\"wmt-mqm-human-evaluation\", \"\", \"https://github.com/google/wmt-mqm-human-evaluation\")},\n",
    "                 '21':{'submissions':(\"WMT21-data\", \"\", \"\")},\n",
    "                 'original':{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_lang_pair(lang_pair):\n",
    "    lang_expr = re.compile(\"([a-z]{2})-([a-z]{2})\")\n",
    "    match = lang_expr.match(lang_pair)\n",
    "    if match:\n",
    "        return match.group(1), match.group(2)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def postprocess_segment(segment):\n",
    "    \"\"\"Various string post-processing necessary to clean the records.\"\"\"\n",
    "    # Identifies NULL values.\n",
    "    if segment == \"NO REFERENCE AVAILABLE\\n\":\n",
    "        return None\n",
    "    # Removes trailing \\n's.\n",
    "    segment = segment.strip()\n",
    "    return segment\n",
    "\n",
    "def git_clone(url, destination_path):\n",
    "    return subprocess.check_call(['git', 'clone', url, destination_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@six.add_metaclass(abc.ABCMeta)\n",
    "class WMTImporter(object):\n",
    "    \"\"\"Base class for WMT Importers.\n",
    "\n",
    "    The aim of WMT importers is to fetch datafiles from the various WMT sources,\n",
    "    collect information (e.g., list language pairs) and aggregate them into\n",
    "    one big file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, year, target_path, cache_path, args):\n",
    "        self.year = year\n",
    "        self.location_info = WMT_LOCATIONS[year]\n",
    "        self.target_path = target_path\n",
    "        self.cache_path = cache_path\n",
    "        self.temp_directory = cache_path\n",
    "        self.args = args\n",
    "    \n",
    "    def open_tar(self, cache_tar_path):\n",
    "        logger.info(\"Untaring...\")\n",
    "        tar = tarfile.open(cache_tar_path)\n",
    "        if self.year == '17':\n",
    "            self.cache_path = os.path.join(self.cache_path, 'wmt17-metrics-task-package')\n",
    "            if not os.path.isdir(self.cache_path):\n",
    "                os.makedirs(self.cache_path)\n",
    "        tar.extractall(path=self.cache_path)\n",
    "        tar.close()\n",
    "        logger.info(\"Done.\")\n",
    "    \n",
    "    def fetch_files(self):\n",
    "        \"\"\"Downloads raw datafiles from various WMT sources.\"\"\"\n",
    "        cache = self.cache_path\n",
    "        if cache and not os.path.isdir(cache):\n",
    "            logger.info(\"Initializing cache {}\".format(cache))\n",
    "            os.makedirs(cache)\n",
    "        \n",
    "        for file_type in self.location_info:\n",
    "            folder_name, archive_name, url_prefix = self.location_info[file_type]\n",
    "            url = url_prefix + archive_name\n",
    "            cache_tar_path = os.path.join(cache, archive_name)\n",
    "            cache_untar_path = os.path.join(cache, archive_name).replace(\".tgz\", \"\", 1).replace(\".tar.gz\", \"\", 1)\n",
    "            if cache:\n",
    "                logger.info(\"Checking cached tar file {}.\".format(cache_tar_path))\n",
    "                if os.path.exists(cache_untar_path) :\n",
    "                    logger.info(\"Cache and untar directory found, skipping\")\n",
    "        #           tf.io.gfile.copy(cache_untar_path, os.path.join(self.temp_directory, os.path.basename(cache_untar_path)), overwrite=True)\n",
    "                    continue\n",
    "                if os.path.isfile(cache_tar_path):\n",
    "                    logger.info(\"Cache tar file found\")\n",
    "                    self.open_tar(cache_tar_path)\n",
    "\n",
    "            logger.info(\"File not found in cache.\")\n",
    "            logger.info(\"Downloading {} from {}\".format(folder_name, url))\n",
    "            urllib.request.urlretrieve(url, cache_tar_path)\n",
    "            logger.info(\"Done.\")\n",
    "            self.open_tar(cache_tar_path)\n",
    "\n",
    "    def list_lang_pairs(self):\n",
    "        \"\"\"List all language pairs included in the WMT files for the target year.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def generate_records_for_lang(self, lang):\n",
    "        \"\"\"Consolidates all the files for a given language pair and year.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Housekeeping--we want to erase all the temp files created.\"\"\"\n",
    "        for file_type in self.location_info:\n",
    "            folder_name, archive_name, _ = self.location_info[file_type]\n",
    "\n",
    "            # Removes data folder\n",
    "            folder_path = os.path.join(self.temp_directory, folder_name)\n",
    "            logger.info(\"Removing\", folder_path)\n",
    "            try:\n",
    "                shutil.rmtree(folder_path)\n",
    "            except OSError:\n",
    "                logger.info(\"OS Error--skipping\")\n",
    "\n",
    "            # Removes downloaded archive\n",
    "            archive_path = os.path.join(self.temp_directory, archive_name)\n",
    "            logger.info(\"Removing\", archive_path)\n",
    "            try:\n",
    "                os.remove(archive_path)\n",
    "            except OSError:\n",
    "                logger.info(\"OS Error--skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Importer1516(WMTImporter):\n",
    "    \"\"\"Importer for years 2015 and 2016.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def to_json(year, lang, source, reference, candidate, rating, seg_id, system):\n",
    "        \"\"\"Converts record to JSON.\"\"\"\n",
    "        json_dict = {\"year\": int(year),\n",
    "                     \"lang\": lang,\n",
    "                     \"source\": postprocess_segment(source),\n",
    "                     \"reference\": postprocess_segment(reference),\n",
    "                     \"candidate\": postprocess_segment(candidate),\n",
    "                     \"raw_rating\": None,\n",
    "                     \"rating\": float(rating.strip()),\n",
    "                     \"segment_id\": seg_id,\n",
    "                     \"system\": system,\n",
    "                     \"n_ratings\": None}\n",
    "        return json.dumps(json_dict)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_file_name(fname):\n",
    "        wmt_pattern = re.compile(r\"^DAseg\\.newstest([0-9]+)\\.[a-z\\-]+\\.([a-z\\-]+)\")\n",
    "        match = re.match(wmt_pattern, fname)\n",
    "        if match:\n",
    "            year, lang_pair = int(match.group(1)), match.group(2)\n",
    "            return year, lang_pair\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    def get_full_folder_path(self):\n",
    "        \"\"\"Returns path of directory with all the extracted files.\"\"\"\n",
    "        file_type = \"eval_data\"\n",
    "        folder_name, _, _ = self.location_info[file_type]\n",
    "        folder = os.path.join(self.cache_path, folder_name)\n",
    "        return folder\n",
    "\n",
    "    def list_files_for_lang(self, lang):\n",
    "        \"\"\"Lists the full paths of all the files for a given language pair.\"\"\"\n",
    "        year = \"20\"+self.year\n",
    "        source_file = \"DAseg.newstest{}.source.{}\".format(str(year), lang)\n",
    "        reference_file = \"DAseg.newstest{}.reference.{}\".format(str(year), lang)\n",
    "        candidate_file = \"DAseg.newstest{}.mt-system.{}\".format(str(year), lang)\n",
    "        rating_file = \"DAseg.newstest{}.human.{}\".format(str(year), lang)\n",
    "        folder = self.get_full_folder_path()\n",
    "        return {\"source\": os.path.join(folder, source_file),\n",
    "                \"reference\": os.path.join(folder, reference_file),\n",
    "                \"candidate\": os.path.join(folder, candidate_file),\n",
    "                \"rating\": os.path.join(folder, rating_file)}\n",
    "\n",
    "    def list_lang_pairs(self):\n",
    "        folder = self.get_full_folder_path()\n",
    "        file_names = os.listdir(folder)\n",
    "        file_data = [Importer1516.parse_file_name(f) for f in file_names]\n",
    "        lang_pairs = [lang_pair for year, lang_pair in file_data if year and lang_pair]\n",
    "        return list(set(lang_pairs))\n",
    "\n",
    "    def generate_records_for_lang(self, lang):\n",
    "        year = '20'+self.year\n",
    "        input_files = self.list_files_for_lang(lang)\n",
    "\n",
    "        # pylint: disable=g-backslash-continuation\n",
    "        with open(input_files[\"source\"], \"r\", encoding=\"utf-8\") as source_file, \\\n",
    "             open(input_files[\"reference\"], \"r\", encoding=\"utf-8\") as reference_file, \\\n",
    "             open(input_files[\"candidate\"], \"r\", encoding=\"utf-8\") as candidate_file, \\\n",
    "             open(input_files[\"rating\"], \"r\", encoding=\"utf-8\") as rating_file:\n",
    "            # pylint: enable=g-backslash-continuation\n",
    "            n_records = 0\n",
    "            with open(self.target_path, \"a+\") as dest_file:\n",
    "                for source, reference, candidate, rating in itertools.zip_longest(\n",
    "                    source_file, reference_file, candidate_file, rating_file):\n",
    "                    example = Importer1516.to_json(year, lang, source, reference, candidate, rating, n_records + 1, None)\n",
    "                    dest_file.write(example)\n",
    "                    dest_file.write(\"\\n\")\n",
    "                    n_records += 1\n",
    "                logger.info(\"Processed {} records of {}'s {}\".format(str(n_records), year, lang))\n",
    "                return n_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Importer17(WMTImporter):\n",
    "    \"\"\"Importer for year 2017.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Importer17, self).__init__(*args, **kwargs)\n",
    "        self.lang_pairs = None\n",
    "        self.temp_directory = os.path.join(self.cache_path, \"wmt17-metrics-task-package\")\n",
    "\n",
    "    def get_folder_path(self):\n",
    "        \"\"\"Returns path of directory with all the extracted files.\"\"\"\n",
    "        return self.temp_directory\n",
    "\n",
    "    def agg_ratings_path(self):\n",
    "        return os.path.join(self.temp_directory, \"manual-evaluation\", \"DA-seglevel.csv\")\n",
    "\n",
    "    def segments_path(self, subset=\"root\"):\n",
    "        \"\"\"Return the path to the source, reference, and candidate segments.\n",
    "\n",
    "        Args:\n",
    "          subset: one if \"root\", \"source\", \"reference\", or \"candidate\".\n",
    "\n",
    "        Returns:\n",
    "          Path to the relevant folder.\n",
    "        \"\"\"\n",
    "        assert subset in [\"root\", \"source\", \"reference\", \"candidate\"]\n",
    "        #     root_dir = os.path.join(self.temp_directory, \"extracted_wmt_package\")\n",
    "        root_dir = os.path.join(self.temp_directory, \"input\")\n",
    "        if subset == \"root\":\n",
    "            return root_dir\n",
    "\n",
    "        root_dir = os.path.join(root_dir, \"wmt17-metrics-task-no-hybrids\")\n",
    "        if subset == \"source\":\n",
    "            return os.path.join(root_dir, \"wmt17-submitted-data\", \"txt\", \"sources\")\n",
    "        elif subset == \"reference\":\n",
    "            return os.path.join(root_dir, \"wmt17-submitted-data\", \"txt\", \"references\")\n",
    "        elif subset == \"candidate\":\n",
    "            return os.path.join(root_dir, \"wmt17-submitted-data\", \"txt\", \"system-outputs\", \"newstest2017\")\n",
    "\n",
    "    def fetch_files(self):\n",
    "        \"\"\"Downloads the WMT eval files.\"\"\"\n",
    "        # Downloads the main archive.\n",
    "        super(Importer17, self).fetch_files()\n",
    "    \n",
    "        #Unpacks the segments.\n",
    "        package_path = self.get_folder_path()\n",
    "        segments_archive = os.path.join(package_path, \"input\", \"wmt17-metrics-task-no-hybrids.tgz\")\n",
    "        with (tarfile.open(segments_archive, \"r:gz\")) as tar:\n",
    "            tar.extractall(path=self.segments_path())\n",
    "        logger.info(\"Unpacked the segments to {}.\".format(self.segments_path()))\n",
    "\n",
    "        # Gets the language pair names.\n",
    "        ratings_path = self.agg_ratings_path()\n",
    "        lang_pairs = set()\n",
    "        with open(ratings_path, \"r\") as ratings_file:\n",
    "            for l in itertools.islice(ratings_file, 1, None):\n",
    "                lang = l.split(\" \")[0]\n",
    "                assert re.match(\"[a-z][a-z]-[a-z][a-z]\", lang)\n",
    "                lang_pairs.add(lang)\n",
    "        self.lang_pairs = list(lang_pairs)\n",
    "        logger.info(\"fetching Done\")\n",
    "\n",
    "    def list_lang_pairs(self):\n",
    "        \"\"\"List all language pairs included in the WMT files for the target year.\"\"\"\n",
    "        if self.lang_pairs == None:\n",
    "            ratings_path = self.agg_ratings_path()\n",
    "            lang_pairs = set()\n",
    "            with open(ratings_path, \"r\") as ratings_file:\n",
    "                for l in itertools.islice(ratings_file, 1, None):\n",
    "                    lang = l.split(\" \")[0]\n",
    "                    assert re.match(\"[a-z][a-z]-[a-z][a-z]\", lang)\n",
    "                    lang_pairs.add(lang)\n",
    "            self.lang_pairs = list(lang_pairs)\n",
    "        return self.lang_pairs\n",
    "\n",
    "    def get_ref_segments(self, lang):\n",
    "        \"\"\"Fetches source and reference translation segments for language pair.\"\"\"\n",
    "        src_subfolder = self.segments_path(\"source\")\n",
    "        ref_subfolder = self.segments_path(\"reference\")\n",
    "        src_lang, tgt_lang = separate_lang_pair(lang)\n",
    "        src_file = \"newstest2017-{}{}-src.{}\".format(src_lang, tgt_lang, src_lang)\n",
    "        ref_file = \"newstest2017-{}{}-ref.{}\".format(src_lang, tgt_lang, tgt_lang)\n",
    "        src_path = os.path.join(src_subfolder, src_file)\n",
    "        ref_path = os.path.join(ref_subfolder, ref_file)\n",
    "\n",
    "#         logger.info(\"Reading data from files {} and {}\".format(src_path, ref_path))\n",
    "        with open(src_path, \"r\", encoding=\"utf-8\") as f_src:\n",
    "            src_segments = f_src.readlines()\n",
    "        with open(ref_path, \"r\", encoding=\"utf-8\") as f_ref:\n",
    "            ref_segments = f_ref.readlines()\n",
    "        src_segments = [postprocess_segment(s) for s in src_segments]\n",
    "        ref_segments = [postprocess_segment(s) for s in ref_segments]\n",
    "#         logger.info(\"Read {} source and {} reference segments.\".format(len(src_segments), len(ref_segments)))\n",
    "        return src_segments, ref_segments\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_submission_file_name(fname):\n",
    "        \"\"\"Extracts system names from the name of submission files.\"\"\"\n",
    "        wmt_pattern = re.compile(r\"^newstest2017\\.([a-zA-Z0-9\\-\\.]+\\.[0-9]+)\\.[a-z]{2}-[a-z]{2}\")\n",
    "        match = re.match(wmt_pattern, fname)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_sys_segments(self, lang):\n",
    "        \"\"\"Builds a dictionary with the generated segments for each system.\"\"\"\n",
    "        # Gets all submission file paths.\n",
    "        root_folder = self.segments_path(\"candidate\")\n",
    "        folder = os.path.join(root_folder, lang)\n",
    "        all_files = os.listdir(folder)\n",
    "#         logger.info(\"Reading submission files from {}\".format(folder))\n",
    "\n",
    "        # Extracts the generated segments for each submission.\n",
    "        sys_segments = {}\n",
    "        for sys_file_name in all_files:\n",
    "            sys_name = Importer17.parse_submission_file_name(sys_file_name)\n",
    "            assert sys_name\n",
    "            sys_path = os.path.join(folder, sys_file_name)\n",
    "            with open(sys_path, \"r\", encoding=\"utf-8\") as f_sys:\n",
    "                sys_lines = f_sys.readlines()\n",
    "                sys_lines = [postprocess_segment(s) for s in sys_lines]\n",
    "                sys_segments[sys_name] = sys_lines\n",
    "\n",
    "#         logger.info(\"Read submissions from {} systems\".format(len(sys_segments.keys())))\n",
    "        return sys_segments\n",
    "\n",
    "    def parse_rating(self, line):\n",
    "        fields = line.split()\n",
    "        lang = fields[0]\n",
    "        sys_names = fields[2].split(\"+\")\n",
    "        seg_id = int(fields[3])\n",
    "        z_score = float(fields[4])\n",
    "        raw_score = None\n",
    "        for sys_name in sys_names:\n",
    "            yield lang, sys_name, seg_id, raw_score, z_score\n",
    "\n",
    "    def generate_records_for_lang(self, lang):\n",
    "        \"\"\"Consolidates all the files for a given language pair and year.\"\"\"\n",
    "        # Loads source, reference and system segments.\n",
    "        src_segments, ref_segments = self.get_ref_segments(lang)\n",
    "        sys_segments = self.get_sys_segments(lang)\n",
    "\n",
    "        # Streams the rating file and performs the join on-the-fly.\n",
    "        ratings_file_path = self.agg_ratings_path()\n",
    "#         logger.info(\"Reading file {}\".format(ratings_file_path))\n",
    "        n_records = 0\n",
    "        with open(ratings_file_path, \"r\", encoding=\"utf-8\") as f_ratings:\n",
    "            with open(self.target_path, \"a+\") as dest_file:\n",
    "                for line in itertools.islice(f_ratings, 1, None):\n",
    "                    for parsed_line in self.parse_rating(line):\n",
    "                        line_lang, sys_name, seg_id, raw_score, z_score = parsed_line\n",
    "                        if line_lang != lang:\n",
    "                            continue\n",
    "                        # The \"-1\" is necessary because seg_id starts counting at 1.\n",
    "                        src_segment = src_segments[seg_id - 1]\n",
    "                        ref_segment = ref_segments[seg_id - 1]\n",
    "                        sys_segment = sys_segments[sys_name][seg_id - 1]\n",
    "                        example = Importer18.to_json('20'+self.year, lang, src_segment,\n",
    "                                                     ref_segment, sys_segment, raw_score,\n",
    "                                                     z_score, seg_id, sys_name)\n",
    "                        dest_file.write(example)\n",
    "                        dest_file.write(\"\\n\")\n",
    "                        n_records += 1\n",
    "        logger.info(\"Processed {} records of {}'s {}\".format(str(n_records), self.year, lang))\n",
    "        return n_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Importer18(WMTImporter):\n",
    "    \"\"\"Importer for year 2018.\"\"\"\n",
    "\n",
    "    def parse_submission_file_name(self, fname):\n",
    "        \"\"\"Extracts system names from the name of submission files.\"\"\"\n",
    "        wmt_pattern = re.compile(r\"^newstest2018\\.([a-zA-Z0-9\\-\\.]+\\.[0-9]+)\\.[a-z]{2}-[a-z]{2}\")\n",
    "        match = re.match(wmt_pattern, fname)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def parse_eval_file_name(self, fname):\n",
    "        \"\"\"Extracts language pairs from the names of human rating files.\"\"\"\n",
    "        wmt_pattern = re.compile(r\"^ad-seg-scores-([a-z]{2}-[a-z]{2})\\.csv\")\n",
    "        match = re.match(wmt_pattern, fname)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def list_lang_pairs(self):\n",
    "        \"\"\"List all language pairs included in the WMT files for 2018.\"\"\"\n",
    "        folder_name, _, _ = self.location_info[\"eval_data\"]\n",
    "        subfolder = \"analysis\"\n",
    "        folder = os.path.join(self.temp_directory, folder_name, subfolder)\n",
    "        all_files = os.listdir(folder)\n",
    "        cand_lang_pairs = [self.parse_eval_file_name(fname) for fname in all_files]\n",
    "        # We need to remove None values in cand_lang_pair:\n",
    "        lang_pairs = [lang_pair for lang_pair in cand_lang_pairs if lang_pair]\n",
    "        return list(set(lang_pairs))\n",
    "\n",
    "    def get_ref_segments(self, lang):\n",
    "        \"\"\"Fetches source and reference translation segments for language pair.\"\"\"\n",
    "        folder, _, _ = self.location_info[\"submissions\"]\n",
    "        src_subfolder = os.path.join(\"sources\")\n",
    "        ref_subfolder = os.path.join(\"references\")\n",
    "        src_lang, tgt_lang = separate_lang_pair(lang)\n",
    "        src_file = \"newstest2018-{}{}-src.{}\".format(src_lang, tgt_lang, src_lang)\n",
    "        ref_file = \"newstest2018-{}{}-ref.{}\".format(src_lang, tgt_lang, tgt_lang)\n",
    "        src_path = os.path.join(self.temp_directory, folder, src_subfolder, src_file)\n",
    "        ref_path = os.path.join(self.temp_directory, folder, ref_subfolder, ref_file)\n",
    "\n",
    "#         logger.info(\"Reading data from files {} and {}\".format(src_path, ref_path))\n",
    "        with open(src_path, \"r\", encoding=\"utf-8\") as f_src:\n",
    "            src_segments = f_src.readlines()\n",
    "        with open(ref_path, \"r\", encoding=\"utf-8\") as f_ref:\n",
    "            ref_segments = f_ref.readlines()\n",
    "\n",
    "        src_segments = [postprocess_segment(s) for s in src_segments]\n",
    "        ref_segments = [postprocess_segment(s) for s in ref_segments]\n",
    "\n",
    "        return src_segments, ref_segments\n",
    "\n",
    "    def get_sys_segments(self, lang):\n",
    "        \"\"\"Builds a dictionary with the generated segments for each system.\"\"\"\n",
    "        # Gets all submission file paths.\n",
    "        folder_name, _, _ = self.location_info[\"submissions\"]\n",
    "        subfolder = os.path.join(\"system-outputs\", \"newstest2018\")\n",
    "        folder = os.path.join(self.temp_directory, folder_name, subfolder, lang)\n",
    "        all_files = os.listdir(folder)\n",
    "#         logger.info(\"Reading submission files from {}\".format(folder))\n",
    "\n",
    "        # Extracts the generated segments for each submission.\n",
    "        sys_segments = {}\n",
    "        for sys_file_name in all_files:\n",
    "            if sys_file_name == '.ipynb_checkpoints':\n",
    "                continue\n",
    "            sys_name = self.parse_submission_file_name(sys_file_name)\n",
    "            assert sys_name\n",
    "            sys_path = os.path.join(folder, sys_file_name)\n",
    "            with open(sys_path, \"r\", encoding=\"utf-8\") as f_sys:\n",
    "                sys_lines = f_sys.readlines()\n",
    "                sys_lines = [postprocess_segment(s) for s in sys_lines]\n",
    "                sys_segments[sys_name] = sys_lines\n",
    "\n",
    "        return sys_segments\n",
    "\n",
    "    def get_ratings_path(self, lang):\n",
    "        folder, _, _ = self.location_info[\"eval_data\"]\n",
    "        subfolder = \"analysis\"\n",
    "        file_name = \"ad-seg-scores-{}.csv\".format(lang)\n",
    "        return os.path.join(self.temp_directory, folder, subfolder, file_name)\n",
    "\n",
    "    def parse_rating(self, rating_line):\n",
    "        rating_tuple = tuple(rating_line.split(\" \"))\n",
    "        # I have a feeling that the last field is the number of ratings\n",
    "        # but I'm not 100% sure .\n",
    "        sys_name, seg_id, raw_score, z_score, n_ratings = rating_tuple\n",
    "        seg_id = int(seg_id)\n",
    "        raw_score = float(raw_score)\n",
    "        z_score = float(z_score)\n",
    "        n_ratings = int(n_ratings)\n",
    "        return sys_name, seg_id, raw_score, z_score, n_ratings\n",
    "\n",
    "    @staticmethod\n",
    "    def to_json(year, lang, src_segment, ref_segment, sys_segment,\n",
    "                raw_score, z_score, seg_id, sys_name, n_ratings=0):\n",
    "        \"\"\"Converts record to JSON.\"\"\"\n",
    "        json_dict = {\"year\": year, \"lang\": lang, \"source\": src_segment, \n",
    "                     \"reference\": ref_segment, \"candidate\": sys_segment, \"raw_rating\": raw_score,\n",
    "                     \"rating\": z_score, \"segment_id\": seg_id, \"system\": sys_name,\n",
    "                     \"n_ratings\": n_ratings}\n",
    "        return json.dumps(json_dict)\n",
    "\n",
    "    def generate_records_for_lang(self, lang):\n",
    "        \"\"\"Consolidates all the files for a given language pair and year.\"\"\"\n",
    "\n",
    "        # Loads source, reference and system segments.\n",
    "        src_segments, ref_segments = self.get_ref_segments(lang)\n",
    "        sys_segments = self.get_sys_segments(lang)\n",
    "\n",
    "        # Streams the rating file and performs the join on-the-fly.\n",
    "        ratings_file_path = self.get_ratings_path(lang)\n",
    "#         logger.info(\"Reading file {}\".format(ratings_file_path))\n",
    "        n_records = 0\n",
    "        with open(ratings_file_path, \"r\", encoding=\"utf-8\") as f_ratings:\n",
    "            with open(self.target_path, \"a+\") as dest_file:\n",
    "                for line in itertools.islice(f_ratings, 1, None):\n",
    "                    line = line.rstrip()\n",
    "                    parsed_tuple = self.parse_rating(line)\n",
    "                    sys_name, seg_id, raw_score, z_score, n_ratings = parsed_tuple\n",
    "\n",
    "                    # Those weird rules come from the WMT 2019 DA2RR script.\n",
    "                    # Name of the script: seglevel-ken-rr.py, in Metrics results package.\n",
    "                    if sys_name == \"UAlacant_-_NM\":\n",
    "                        sys_name = \"UAlacant_-_NMT+RBMT.6722\"\n",
    "                    if sys_name == \"HUMAN\":\n",
    "                        continue\n",
    "                    if sys_name == \"RBMT.6722\":\n",
    "                        continue\n",
    "\n",
    "                    # The following rules were added by me to unblock WMT2019:\n",
    "                    if sys_name == \"Helsinki-NLP.6889\":\n",
    "                        sys_name = \"Helsinki_NLP.6889\"\n",
    "                    if sys_name == \"Facebook-FAIR.6937\":\n",
    "                        sys_name = \"Facebook_FAIR.6937\"\n",
    "                    if sys_name == \"Facebook-FAIR.6937\":\n",
    "                        sys_name = \"Facebook_FAIR.6937\"\n",
    "                    if sys_name == \"DBMS-KU-KKEN.6726\":\n",
    "                        sys_name = \"DBMS-KU_KKEN.6726\"\n",
    "                    if sys_name == \"Ju-Saarland.6525\":\n",
    "                        sys_name = \"Ju_Saarland.6525\"\n",
    "                    if sys_name == \"aylien-mt-gu-en-multilingual.6826\":\n",
    "                        sys_name = \"aylien_mt_gu-en_multilingual.6826\"\n",
    "                    if sys_name == \"rug-kken-morfessor.6677\":\n",
    "                        sys_name = \"rug_kken_morfessor.6677\"\n",
    "                    if sys_name == \"talp-upc-2019-kken.6657\":\n",
    "                        sys_name = \"talp_upc_2019_kken.6657\"\n",
    "                    if sys_name == \"Frank-s-MT.6127\":\n",
    "                        sys_name = \"Frank_s_MT.6127\"\n",
    "\n",
    "                    if lang == \"de-cs\" and sys_name == \"Unsupervised.6935\":\n",
    "                        sys_name = \"Unsupervised.de-cs.6935\"\n",
    "                    if lang == \"de-cs\" and sys_name == \"Unsupervised.6929\":\n",
    "                        sys_name = \"Unsupervised.de-cs.6929\"\n",
    "\n",
    "                    # The \"-1\" is necessary because seg_id starts counting at 1.\n",
    "                    src_segment = src_segments[seg_id - 1]\n",
    "                    ref_segment = ref_segments[seg_id - 1]\n",
    "                    sys_segment = sys_segments[sys_name][seg_id - 1]\n",
    "                    if not src_segment or not sys_segment:\n",
    "                        logger.info(\"* Missing value!\")\n",
    "                        logger.info(\"* System: {}\".format(sys_name))\n",
    "                        logger.info(\"* Segment:\" + str(seg_id))\n",
    "                        logger.info(\"* Source segment:\" + src_segment)\n",
    "                        logger.info(\"* Sys segment:\" + sys_segment)\n",
    "                        logger.info(\"* Parsed line:\" + line)\n",
    "                        logger.info(\"* Lang:\" + lang)\n",
    "                    example = Importer18.to_json(self.year, lang, src_segment, \n",
    "                                                 ref_segment, sys_segment, raw_score, \n",
    "                                                 z_score, seg_id, sys_name, n_ratings)\n",
    "                    dest_file.write(example)\n",
    "                    dest_file.write(\"\\n\")\n",
    "                    n_records += 1\n",
    "        logger.info(\"Processed {} records of {}'s {}\".format(str(n_records), self.year, lang))\n",
    "        return n_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Importer19(Importer18):\n",
    "    \"\"\"Importer for WMT19 Metrics challenge.\"\"\"\n",
    "\n",
    "    def parse_rating(self, rating_line):\n",
    "        rating_tuple = tuple(rating_line.split(\" \"))\n",
    "        # I have a feeling that the last field is the number of ratings\n",
    "        # but I'm not 100% sure.\n",
    "        sys_name, seg_id, raw_score, z_score, n_ratings = rating_tuple\n",
    "\n",
    "        # For some reason, all the systems for pair zh-en have an extra suffix.\n",
    "        if sys_name.endswith(\"zh-en\"):\n",
    "            sys_name = sys_name[:-6]\n",
    "\n",
    "        seg_id = int(seg_id)\n",
    "        raw_score = float(raw_score)\n",
    "        z_score = float(z_score)\n",
    "        n_ratings = int(n_ratings)\n",
    "        return sys_name, seg_id, raw_score, z_score, n_ratings\n",
    "\n",
    "    def parse_submission_file_name(self, fname):\n",
    "        \"\"\"Extracts system names from the name of submission files.\"\"\"\n",
    "\n",
    "        # I added those rules to unblock the pipeline.\n",
    "        if fname == \"newstest2019.Unsupervised.de-cs.6929.de-cs\":\n",
    "            return \"Unsupervised.de-cs.6929\"\n",
    "        elif fname == \"newstest2019.Unsupervised.de-cs.6935.de-cs\":\n",
    "            return \"Unsupervised.de-cs.6935\"\n",
    "\n",
    "        wmt_pattern = re.compile(r\"^newstest2019\\.([a-zA-Z0-9\\-\\.\\_\\+]+\\.[0-9]+)\\.[a-z]{2}-[a-z]{2}\")\n",
    "        match = re.match(wmt_pattern, fname)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def list_lang_pairs(self):\n",
    "        \"\"\"List all language pairs included in the WMT files for 2019.\"\"\"\n",
    "        folder_name, _, _ = self.location_info[\"eval_data\"]\n",
    "        folder = os.path.join(self.temp_directory, folder_name, \"*\", \"analysis\", \"ad-seg-scores-*.csv\")\n",
    "        all_full_paths = glob.glob(folder)\n",
    "        all_files = [os.path.basename(f) for f in all_full_paths]\n",
    "        cand_lang_pairs = [self.parse_eval_file_name(fname) for fname in all_files]\n",
    "        # We need to remove None values in cand_lang_pair:\n",
    "        lang_pairs = [lang_pair for lang_pair in cand_lang_pairs if lang_pair]\n",
    "        return list(set(lang_pairs))\n",
    "\n",
    "    def get_ratings_path(self, lang):\n",
    "        folder, _, _ = self.location_info[\"eval_data\"]\n",
    "\n",
    "        # The pair zh-en has two versions in the WMT 2019 human eval folder.\n",
    "        if lang == \"zh-en\":\n",
    "            path = os.path.join(self.temp_directory, folder, \n",
    "                                \"turkle-sntlevel-humaneval-newstest2019\", \"analysis\", \"ad-seg-scores-zh-en.csv\")\n",
    "            return path\n",
    "\n",
    "        file_name = \"ad-seg-scores-{}.csv\".format(lang)\n",
    "        folder = os.path.join(self.temp_directory, folder, \"*\", \"analysis\", \"ad-seg-scores-*.csv\")\n",
    "        all_files = glob.glob(folder)\n",
    "        for cand_file in all_files:\n",
    "            if cand_file.endswith(file_name):\n",
    "                return cand_file\n",
    "        raise ValueError(\"Can't find ratings for lang {}\".format(lang))\n",
    "\n",
    "    def get_ref_segments(self, lang):\n",
    "        \"\"\"Fetches source and reference translation segments for language pair.\"\"\"\n",
    "        folder, _, _ = self.location_info[\"submissions\"]\n",
    "        src_subfolder = os.path.join(\"txt\", \"sources\")\n",
    "        ref_subfolder = os.path.join(\"txt\", \"references\")\n",
    "        src_lang, tgt_lang = separate_lang_pair(lang)\n",
    "        src_file = \"newstest2019-{}{}-src.{}\".format(src_lang, tgt_lang, src_lang)\n",
    "        ref_file = \"newstest2019-{}{}-ref.{}\".format(src_lang, tgt_lang, tgt_lang)\n",
    "        src_path = os.path.join(self.temp_directory, folder, src_subfolder, src_file)\n",
    "        ref_path = os.path.join(self.temp_directory, folder, ref_subfolder, ref_file)\n",
    "\n",
    "#         logger.info(\"Reading data from files {} and {}\".format(src_path, ref_path))\n",
    "        with open(src_path, \"r\", encoding=\"utf-8\") as f_src:\n",
    "            src_segments = f_src.readlines()\n",
    "        with open(ref_path, \"r\", encoding=\"utf-8\") as f_ref:\n",
    "            ref_segments = f_ref.readlines()\n",
    "\n",
    "        src_segments = [postprocess_segment(s) for s in src_segments]\n",
    "        ref_segments = [postprocess_segment(s) for s in ref_segments]\n",
    "\n",
    "        return src_segments, ref_segments\n",
    "\n",
    "    def get_sys_segments(self, lang):\n",
    "        \"\"\"Builds a dictionary with the generated segments for each system.\"\"\"\n",
    "        # Gets all submission file paths.\n",
    "        folder_name, _, _ = self.location_info[\"submissions\"]\n",
    "        subfolder = os.path.join(\"txt\", \"system-outputs\", \"newstest2019\")\n",
    "        folder = os.path.join(self.temp_directory, folder_name, subfolder, lang)\n",
    "        all_files = os.listdir(folder)\n",
    "#         logger.info(\"Reading submission files from {}\".format(folder))\n",
    "\n",
    "        # Extracts the generated segments for each submission.\n",
    "        sys_segments = {}\n",
    "        for sys_file_name in all_files:\n",
    "            sys_name = self.parse_submission_file_name(sys_file_name)\n",
    "            assert sys_name\n",
    "            sys_path = os.path.join(folder, sys_file_name)\n",
    "            with open(sys_path, \"r\", encoding=\"utf-8\") as f_sys:\n",
    "                sys_lines = f_sys.readlines()\n",
    "                sys_lines = [postprocess_segment(s) for s in sys_lines]\n",
    "                sys_segments[sys_name] = sys_lines\n",
    "\n",
    "        return sys_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Importer20(Importer18):\n",
    "    \"\"\"Importer for WMT20 Metrics challenge.\"\"\"\n",
    "    \n",
    "    def __init__(self, year, target_path, cache_path, args, include_unreliables, onlyMQM=False, onlyPSQM=False):\n",
    "        super(Importer20, self).__init__(year, target_path, cache_path, args)\n",
    "        self.include_unreliables = include_unreliables\n",
    "        self.onlyMQM = onlyMQM\n",
    "        self.onlyPSQM = onlyPSQM\n",
    "        self.args = args\n",
    "        assert not (onlyMQM and onlyPSQM), \"only one of onlyMQM or onlyPSQM can stand\"\n",
    "    \n",
    "    def open_tar(self, tar_path, open_dir):\n",
    "        logger.info(\"Untaring...\")\n",
    "        tar = tarfile.open(tar_path)\n",
    "        tar.extractall(path=open_dir)\n",
    "        tar.close()\n",
    "        logger.info(\"Done.\")\n",
    "    \n",
    "    def fetch_files(self):\n",
    "        \"\"\"Downloads raw datafiles from various WMT sources.\"\"\"\n",
    "        cache = self.cache_path\n",
    "        \n",
    "        if cache and not os.path.isdir(cache):\n",
    "            logger.info(\"Initializing cache {}\".format(cache))\n",
    "            os.makedirs(cache)\n",
    "        \n",
    "        for file_type in self.location_info:\n",
    "            if self.onlyMQM:\n",
    "                if file_type in ['submissions', 'eval_data', 'PSQM']:\n",
    "                    continue\n",
    "            elif self.onlyPSQM:\n",
    "                if file_type in ['submissions', 'eval_data', 'MQM']:\n",
    "                    continue\n",
    "            \n",
    "            folder_name, _, url = self.location_info[file_type]\n",
    "            cache_untar_path = os.path.join(cache, folder_name)\n",
    "            \n",
    "            if cache:\n",
    "                logger.info(\"Checking cached tar file {}.\".format(cache_untar_path))\n",
    "                if os.path.exists(cache_untar_path) :\n",
    "                    if file_type == 'submissions':\n",
    "                        tars = os.path.join(cache_untar_path, '*.tar.gz')\n",
    "                        tar_paths = glob.glob(tars)\n",
    "                        untar_paths = [path.replace(\".tar.gz\", \"\", 1) for path in tar_paths]\n",
    "                        for tar_path, untar_paths in zip(tar_paths, untar_paths):\n",
    "                            if not os.path.exists(untar_paths):\n",
    "                                self.open_tar(tar_path, cache_untar_path)\n",
    "                            else:\n",
    "                                logger.info(\"Cache and untar directory found, skipping\")\n",
    "                    else:\n",
    "                        logger.info(\"Cache and untar directory found, skipping\")\n",
    "                    continue\n",
    "            logger.info(\"File not found in cache.\")\n",
    "            if file_type == 'submissions':\n",
    "                logger.info(\"Cannot download {} with this script. Download from {}\".format(folder_name, url))\n",
    "                exit(-1)\n",
    "            logger.info(\"Downloading {} from {}\".format(folder_name, url))\n",
    "            git_clone(url, cache_untar_path)\n",
    "            logger.info(\"Done.\")  \n",
    "    \n",
    "    def parse_rating(self, rating_line, lang):\n",
    "        rating_tuple = tuple(rating_line.split(\" \"))\n",
    "        # I have a feeling that the last field is the number of ratings\n",
    "        # but I'm not 100% sure.\n",
    "        sys_name, seg_id, raw_score, z_score, n_ratings = rating_tuple\n",
    "        \n",
    "        # en-zh has unknown seg_id probablly tagged with other format name\n",
    "        if lang in ['en-zh', 'en-ja', 'en-iu', 'en-cs', 'en-ta', 'en-ru', 'en-de', 'en-pl']:\n",
    "            seg_id = seg_id.split('_')[-1] \n",
    "        \n",
    "        try:\n",
    "            seg_id = int(seg_id)\n",
    "            raw_score = float(raw_score)\n",
    "            z_score = float(z_score)\n",
    "            n_ratings = int(n_ratings)\n",
    "        except:\n",
    "            logger.info(lang)\n",
    "            logger.info(rating_line)\n",
    "        return sys_name, seg_id, raw_score, z_score, n_ratings\n",
    "\n",
    "    def parse_submission_file_name(self, fname, lang):\n",
    "        \"\"\"Extracts system names from the name of submission files.\"\"\"\n",
    "\n",
    "        # I added those rules to unblock the pipeline.\n",
    "\n",
    "        sys_name = fname.replace(\"newstest2020.{}.\".format(lang), \"\", 1).replace(\".txt\", \"\", 1)\n",
    "\n",
    "        return sys_name\n",
    "\n",
    "    def list_lang_pairs(self):\n",
    "        \"\"\"List all language pairs included in the WMT files for 2020.\"\"\"\n",
    "        if self.onlyMQM or self.onlyPSQM:\n",
    "            return ['en-de', 'zh-en']\n",
    "        \n",
    "        folder_name, _, _ = self.location_info[\"eval_data\"]\n",
    "        folder = os.path.join(self.temp_directory, folder_name, \"manual-evaluation\", \"DA\", \"ad-seg-scores-*.csv\")\n",
    "        all_full_paths = glob.glob(folder)\n",
    "        all_files = [os.path.basename(f) for f in all_full_paths]\n",
    "        cand_lang_pairs = [self.parse_eval_file_name(fname) for fname in all_files]\n",
    "        # We need to remove None values in cand_lang_pair:\n",
    "        lang_pairs = [lang_pair for lang_pair in cand_lang_pairs if lang_pair]\n",
    "        return list(set(lang_pairs))\n",
    "\n",
    "    def get_ratings_path(self, lang):\n",
    "        folder, _, _ = self.location_info[\"eval_data\"]\n",
    "\n",
    "        file_name = \"ad-seg-scores-{}.csv\".format(lang)\n",
    "        folder = os.path.join(self.temp_directory, folder, \"manual-evaluation\", \"DA\", \"ad-seg-scores-*.csv\")\n",
    "        all_files = glob.glob(folder)\n",
    "        for cand_file in all_files:\n",
    "            if cand_file.endswith(file_name):\n",
    "                return cand_file\n",
    "        raise ValueError(\"Can't find ratings for lang {}\".format(lang))\n",
    "\n",
    "    def get_ref_segments(self, lang):\n",
    "        \"\"\"Fetches source and reference translation segments for language pair.\"\"\"\n",
    "        folder, _, _ = self.location_info[\"submissions\"]\n",
    "        src_subfolder = os.path.join(\"txt\", \"sources\")\n",
    "        ref_subfolder = os.path.join(\"txt\", \"references\")\n",
    "        src_lang, tgt_lang = separate_lang_pair(lang)\n",
    "        src_file = \"newstest2020-{}{}-src.{}.txt\".format(src_lang, tgt_lang, src_lang)\n",
    "        ref_file = \"newstest2020-{}{}-ref.{}.txt\".format(src_lang, tgt_lang, tgt_lang)\n",
    "        src_path = os.path.join(self.temp_directory, folder, src_subfolder, src_file)\n",
    "        ref_path = os.path.join(self.temp_directory, folder, ref_subfolder, ref_file)\n",
    "\n",
    "#         logger.info(\"Reading data from files {} and {}\".format(src_path, ref_path))\n",
    "        with open(src_path, \"r\", encoding=\"utf-8\") as f_src:\n",
    "            src_segments = f_src.readlines()\n",
    "        with open(ref_path, \"r\", encoding=\"utf-8\") as f_ref:\n",
    "            ref_segments = f_ref.readlines()\n",
    "\n",
    "        src_segments = [postprocess_segment(s) for s in src_segments]\n",
    "        ref_segments = [postprocess_segment(s) for s in ref_segments]\n",
    "\n",
    "        return src_segments, ref_segments\n",
    "\n",
    "    def get_sys_segments(self, lang):\n",
    "        \"\"\"Builds a dictionary with the generated segments for each system.\"\"\"\n",
    "        # Gets all submission file paths.\n",
    "        folder_name, _, _ = self.location_info[\"submissions\"]\n",
    "        subfolder = os.path.join(\"txt\", \"system-outputs\")\n",
    "        folder = os.path.join(self.temp_directory, folder_name, subfolder, lang)\n",
    "        all_files = os.listdir(folder)\n",
    "#         logger.info(\"Reading submission files from {}\".format(folder))\n",
    "\n",
    "        # Extracts the generated segments for each submission.\n",
    "        sys_segments = {}\n",
    "        for sys_file_name in all_files:\n",
    "            sys_name = self.parse_submission_file_name(sys_file_name, lang)\n",
    "            assert sys_name\n",
    "            sys_path = os.path.join(folder, sys_file_name)\n",
    "            with open(sys_path, \"r\", encoding=\"utf-8\") as f_sys:\n",
    "                sys_lines = f_sys.readlines()\n",
    "                sys_lines = [postprocess_segment(s) for s in sys_lines]\n",
    "                sys_segments[sys_name] = sys_lines\n",
    "\n",
    "        return sys_segments\n",
    "    \n",
    "    def parse_mqm(self, line, lang):\n",
    "        rating_tuple = tuple(line.split(\"\\t\"))\n",
    "        system, doc, doc_id, seg_id, rater, source, target, category, severity = rating_tuple\n",
    "        \n",
    "        score = 0.0\n",
    "        assert severity in ['Major', 'Minor', 'Neutral', 'no-error'], 'unknown severity:{}'.format(severity)\n",
    "\n",
    "        if severity == 'Major':\n",
    "            if category == 'Non-translation!':\n",
    "                score = -25.0\n",
    "            else:\n",
    "                score = -5.0\n",
    "        elif severity == 'Minor':\n",
    "            if category == 'Fluency/Punctuation':\n",
    "                score = -0.1\n",
    "            else:\n",
    "                score = -1.0\n",
    "        \n",
    "        try:\n",
    "            doc_id = int(doc_id)\n",
    "            seg_id = int(seg_id)\n",
    "        except:\n",
    "            logger.info(lang)\n",
    "            logger.info(line)\n",
    "        return system, doc, doc_id, seg_id, rater, source, target, category, severity, score\n",
    "    \n",
    "    def get_mqm_segments(self, lang):\n",
    "        src_lang, tgt_lang = separate_lang_pair(lang)\n",
    "        folder_name, _, _ = self.location_info[\"MQM\"]\n",
    "        file = os.path.join(self.temp_directory, folder_name, \"{}{}\".format(src_lang, tgt_lang), \"mqm_newstest2020_{}{}.tsv\".format(src_lang, tgt_lang))\n",
    "        rater_score = {}\n",
    "        seg_scores = {}\n",
    "        with open(file, mode='r', encoding='utf-8') as r:\n",
    "            for line in itertools.islice(r, 1, None):\n",
    "                line = line.rstrip()\n",
    "                system, doc, doc_id, seg_id, rater, source, target, category, severity, score = self.parse_mqm(line, lang)\n",
    "                if rater not in rater_score:\n",
    "                    rater_score[rater] = {'score':[score], \n",
    "                                          'source':[source.rstrip()],\n",
    "                                          'target':[target.rstrip()], \n",
    "                                          'system':[system], \n",
    "                                          'seg_id':[seg_id]}\n",
    "                else:\n",
    "                    rater_score[rater]['score'].append(score)\n",
    "                    rater_score[rater]['source'].append(source.rstrip())\n",
    "                    rater_score[rater]['target'].append(target.rstrip())\n",
    "                    rater_score[rater]['system'].append(system)\n",
    "                    rater_score[rater]['seg_id'].append(seg_id)\n",
    "        for rater in rater_score.keys():\n",
    "            rater_score[rater]['z_score'] = list(preprocessing.scale(rater_score[rater]['score']))\n",
    "        for rater in rater_score.keys():\n",
    "            for seg_id, src, tgt, score, z_score, system in zip(rater_score[rater]['seg_id'], \n",
    "                                                                rater_score[rater]['source'], \n",
    "                                                                rater_score[rater]['target'], \n",
    "                                                                rater_score[rater]['score'], \n",
    "                                                                rater_score[rater]['z_score'], \n",
    "                                                                rater_score[rater]['system']):\n",
    "                sys_id = (system, seg_id)\n",
    "                if sys_id not in seg_scores:\n",
    "                    seg_scores[sys_id] = {'rater':[rater],\n",
    "                                          'score':[score], \n",
    "                                          'z_score':[z_score],\n",
    "                                          'source':[source],\n",
    "                                          'target':[target]}\n",
    "                else:\n",
    "                    seg_scores[sys_id]['rater'].append(rater)\n",
    "                    seg_scores[sys_id]['score'].append(score)\n",
    "                    seg_scores[sys_id]['z_score'].append(z_score)\n",
    "                    seg_scores[sys_id]['source'].append(source)\n",
    "                    seg_scores[sys_id]['target'].append(target)\n",
    "        for sys_id in seg_scores.keys():\n",
    "            seg_scores[sys_id]['z_mean_score'] = np.mean(seg_scores[sys_id]['z_score'])\n",
    "            \n",
    "        return rater_score, seg_scores\n",
    "                \n",
    "    def get_mqm_avg_segments(self, lang, seg_scores):\n",
    "        src_lang, tgt_lang = separate_lang_pair(lang)\n",
    "        folder_name, _, _ = self.location_info[\"MQM\"]\n",
    "        file = os.path.join(self.temp_directory, folder_name, \"{}{}\".format(src_lang, tgt_lang), \"mqm_newstest2020_{}{}.avg_seg_scores.tsv\".format(src_lang, tgt_lang))\n",
    "        with open(file, mode='r', encoding='utf-8') as r:\n",
    "            for line in itertools.islice(r, 1, None):\n",
    "                line = line.rstrip()\n",
    "                system, mqm_avg_score, seg_id = tuple(line.split(' '))\n",
    "                sys_id = (system, int(seg_id))\n",
    "                seg_scores[sys_id]['raw_score'] = mqm_avg_score\n",
    "        return seg_scores\n",
    "    \n",
    "    def generate_mqm_records_for_lang(self, lang):\n",
    "        rater_score, seg_scores = self.get_mqm_segments(lang)\n",
    "        src_segments, ref_segments = self.get_ref_segments(lang)\n",
    "        sys_segments = self.get_sys_segments(lang)\n",
    "        seg_scores = self.get_mqm_avg_segments(lang, seg_scores)\n",
    "        \n",
    "        n_records = 0\n",
    "        skipped_n_records = 0\n",
    "        with open(self.target_path, \"a+\") as dest_file:\n",
    "            for sys_id in seg_scores.keys():\n",
    "                sys_name, seg_id = sys_id\n",
    "                raw_score = seg_scores[sys_id]['raw_score']\n",
    "                z_score = seg_scores[sys_id]['z_score']\n",
    "                \n",
    "                # The \"-1\" is necessary because seg_id starts counting at 1.\n",
    "                src_segment = src_segments[seg_id - 1]\n",
    "                ref_segment = ref_segments[seg_id - 1]\n",
    "                if sys_name not in sys_segments:\n",
    "                    logger.info(sys_name, lang)\n",
    "                else:\n",
    "                    sys_segment = sys_segments[sys_name][seg_id - 1]\n",
    "\n",
    "                if not src_segment or not sys_segment:\n",
    "                    logger.info(\"* Missing value!\")\n",
    "                    logger.info(\"* System: {}\".format(sys_name))\n",
    "                    logger.info(\"* Segment:\" + str(seg_id))\n",
    "                    logger.info(\"* Source segment:\" + src_segment)\n",
    "                    logger.info(\"* Sys segment:\" + sys_segment)\n",
    "                    logger.info(\"* Parsed line:\" + line)\n",
    "                    logger.info(\"* Lang:\" + lang)\n",
    "                example = self.to_json(self.year, lang, src_segment, \n",
    "                                             ref_segment, sys_segment, raw_score, \n",
    "                                             z_score, seg_id, sys_name)\n",
    "                dest_file.write(example)\n",
    "                dest_file.write(\"\\n\")\n",
    "                n_records += 1\n",
    "        logger.info(\"Processed {} records of {}'s {}\".format(str(n_records), self.year, lang))\n",
    "        logger.info(\"Skipped {} records of {}'s {}\".format(str(skipped_n_records), self.year, lang))\n",
    "        return n_records\n",
    "    \n",
    "    def parse_psqm(self, line, lang):\n",
    "        rating_tuple = tuple(line.split(\"\\t\"))\n",
    "        system, doc, doc_id, seg_id, rater, source, target, score = rating_tuple\n",
    "        \n",
    "        score = 0.0\n",
    "        try:\n",
    "            doc_id = int(doc_id)\n",
    "            seg_id = int(seg_id)\n",
    "        except:\n",
    "            logger.info(lang)\n",
    "            logger.info(line)\n",
    "        return system, doc, doc_id, seg_id, rater, source, target, score\n",
    "    \n",
    "    def get_psqm_segments(self, lang):\n",
    "        src_lang, tgt_lang = separate_lang_pair(lang)\n",
    "        folder_name, _, _ = self.location_info[\"PSQM\"]\n",
    "        file = os.path.join(self.temp_directory, folder_name, \"{}{}\".format(src_lang, tgt_lang), \"psqm_newstest2020_{}{}.tsv\".format(src_lang, tgt_lang))\n",
    "        rater_score = {}\n",
    "        seg_scores = {}\n",
    "        with open(file, mode='r', encoding='utf-8') as r:\n",
    "            for line in itertools.islice(r, 1, None):\n",
    "                line = line.rstrip()\n",
    "                system, doc, doc_id, seg_id, rater, source, target, score = self.parse_psqm(line, lang)\n",
    "                if rater not in rater_score:\n",
    "                    rater_score[rater] = {'score':[score], \n",
    "                                          'source':[source.rstrip()],\n",
    "                                          'target':[target.rstrip()], \n",
    "                                          'system':[system], \n",
    "                                          'seg_id':[seg_id]}\n",
    "                else:\n",
    "                    rater_score[rater]['score'].append(score)\n",
    "                    rater_score[rater]['source'].append(source.rstrip())\n",
    "                    rater_score[rater]['target'].append(target.rstrip())\n",
    "                    rater_score[rater]['system'].append(system)\n",
    "                    rater_score[rater]['seg_id'].append(seg_id)\n",
    "        for rater in rater_score.keys():\n",
    "            rater_score[rater]['z_score'] = list(preprocessing.scale(rater_score[rater]['score']))\n",
    "        for rater in rater_score.keys():\n",
    "            for seg_id, src, tgt, score, z_score, system in zip(rater_score[rater]['seg_id'], \n",
    "                                                                rater_score[rater]['source'], \n",
    "                                                                rater_score[rater]['target'], \n",
    "                                                                rater_score[rater]['score'], \n",
    "                                                                rater_score[rater]['z_score'], \n",
    "                                                                rater_score[rater]['system']):\n",
    "                sys_id = (system, seg_id)\n",
    "                if sys_id not in seg_scores:\n",
    "                    seg_scores[sys_id] = {'rater':[rater],\n",
    "                                          'score':[score], \n",
    "                                          'z_score':[z_score],\n",
    "                                          'source':[source],\n",
    "                                          'target':[target]}\n",
    "                else:\n",
    "                    seg_scores[sys_id]['rater'].append(rater)\n",
    "                    seg_scores[sys_id]['score'].append(score)\n",
    "                    seg_scores[sys_id]['z_score'].append(z_score)\n",
    "                    seg_scores[sys_id]['source'].append(source)\n",
    "                    seg_scores[sys_id]['target'].append(target)\n",
    "        for sys_id in seg_scores.keys():\n",
    "            seg_scores[sys_id]['z_mean_score'] = np.mean(seg_scores[sys_id]['z_score'])\n",
    "        \n",
    "        return rater_score, seg_scores\n",
    "    \n",
    "    \n",
    "    def generate_psqm_records_for_lang(self, lang):\n",
    "        rater_score, seg_scores = self.get_psqm_segments(lang)\n",
    "        src_segments, ref_segments = self.get_ref_segments(lang)\n",
    "        sys_segments = self.get_sys_segments(lang)\n",
    "        \n",
    "        n_records = 0\n",
    "        skipped_n_records = 0\n",
    "        with open(self.target_path, \"a+\") as dest_file:\n",
    "            for sys_id in seg_scores.keys():\n",
    "                sys_name, seg_id = sys_id\n",
    "                raw_score = 'n/a'\n",
    "                z_score = seg_scores[sys_id]['z_score']\n",
    "                \n",
    "                # The \"-1\" is necessary because seg_id starts counting at 1.\n",
    "                src_segment = src_segments[seg_id - 1]\n",
    "                ref_segment = ref_segments[seg_id - 1]\n",
    "                if sys_name not in sys_segments:\n",
    "                    logger.info(sys_name, lang)\n",
    "                else:\n",
    "                    sys_segment = sys_segments[sys_name][seg_id - 1]\n",
    "\n",
    "                if not src_segment or not sys_segment:\n",
    "                    logger.info(\"* Missing value!\")\n",
    "                    logger.info(\"* System: {}\".format(sys_name))\n",
    "                    logger.info(\"* Segment:\" + str(seg_id))\n",
    "                    logger.info(\"* Source segment:\" + src_segment)\n",
    "                    logger.info(\"* Sys segment:\" + sys_segment)\n",
    "                    logger.info(\"* Parsed line:\" + line)\n",
    "                    logger.info(\"* Lang:\" + lang)\n",
    "                example = self.to_json(self.year, lang, src_segment, \n",
    "                                             ref_segment, sys_segment, raw_score, \n",
    "                                             z_score, seg_id, sys_name)\n",
    "                dest_file.write(example)\n",
    "                dest_file.write(\"\\n\")\n",
    "                n_records += 1\n",
    "        logger.info(\"Processed {} records of {}'s {}\".format(str(n_records), self.year, lang))\n",
    "        logger.info(\"Skipped {} records of {}'s {}\".format(str(skipped_n_records), self.year, lang))\n",
    "        \n",
    "        return n_records\n",
    "    \n",
    "    def to_json(self, year, lang, src_segment, ref_segment, sys_segment,\n",
    "                raw_score, z_score, seg_id, sys_name, n_ratings=0):\n",
    "        \"\"\"Converts record to JSON.\"\"\"\n",
    "        if self.args.use_avg_seg_scores and (self.args.onlyMQM or self.args.onlyPSQM): \n",
    "            json_dict = {\"year\": year, \"lang\": lang, \"source\": src_segment, \n",
    "                         \"reference\": ref_segment, \"candidate\": sys_segment, \"z_rating\": z_score,\n",
    "                         \"rating\": raw_score, \"segment_id\": seg_id, \"system\": sys_name,\n",
    "                         \"n_ratings\": n_ratings}\n",
    "            return json.dumps(json_dict)\n",
    "        else:\n",
    "            json_dict = {\"year\": year, \"lang\": lang, \"source\": src_segment, \n",
    "                         \"reference\": ref_segment, \"candidate\": sys_segment, \"raw_rating\": raw_score,\n",
    "                         \"rating\": z_score, \"segment_id\": seg_id, \"system\": sys_name,\n",
    "                         \"n_ratings\": n_ratings}\n",
    "            return json.dumps(json_dict)\n",
    "    \n",
    "    def generate_records_for_lang(self, lang):\n",
    "        \"\"\"Consolidates all the files for a given language pair and year.\"\"\"\n",
    "        \n",
    "        if self.onlyMQM:\n",
    "            n_records = self.generate_mqm_records_for_lang(lang)\n",
    "            return n_records\n",
    "        elif self.onlyPSQM:\n",
    "            n_records = self.generate_psqm_records_for_lang(lang)\n",
    "            return n_records\n",
    "        \n",
    "        if self.args.priorMQM == False :\n",
    "            raise NotImplementedError\n",
    "        # if lang in ['en-de', 'zh-en']:\n",
    "        #     _, MQMsegments = self.get_mqm_segments(lang)\n",
    "        \n",
    "        \n",
    "        # Loads source, reference and system segments.\n",
    "        src_segments, ref_segments = self.get_ref_segments(lang)\n",
    "        sys_segments = self.get_sys_segments(lang)\n",
    "\n",
    "        # Streams the rating file and performs the join on-the-fly.\n",
    "        ratings_file_path = self.get_ratings_path(lang)\n",
    "#         logger.info(\"Reading file {}\".format(ratings_file_path))\n",
    "        n_records = 0\n",
    "        skipped_n_records = 0\n",
    "        \n",
    "        if lang in ['en-zh', 'en-ja', 'en-iu', 'en-cs', 'en-ta', 'en-ru', 'en-de', 'en-pl'] and (not self.include_unreliables):\n",
    "            return 0\n",
    "        \n",
    "        with open(ratings_file_path, \"r\", encoding=\"utf-8\") as f_ratings:\n",
    "            with open(self.target_path, \"a+\") as dest_file:\n",
    "                for line in itertools.islice(f_ratings, 1, None):\n",
    "                    line = line.rstrip()\n",
    "                    parsed_tuple = self.parse_rating(line, lang)\n",
    "                    sys_name, seg_id, raw_score, z_score, n_ratings = parsed_tuple\n",
    "                    \n",
    "                    if sys_name == 'HUMAN.0' and lang == 'de-en':\n",
    "                        skipped_n_records += 1\n",
    "                        continue\n",
    "                    if sys_name == 'HUMAN.0' and lang == 'zh-en':\n",
    "                        skipped_n_records += 1\n",
    "                        continue\n",
    "                    if sys_name == 'HUMAN.0' and lang == 'ru-en':\n",
    "                        skipped_n_records += 1\n",
    "                        continue\n",
    "                    if sys_name == 'HUMAN-B':\n",
    "                        sys_name == 'Human-B.0'\n",
    "                    if sys_name == 'Huoshan-Translate.1470' and lang == 'ps-en':\n",
    "                        sys_name = 'Huoshan_Translate.1470'\n",
    "                    if sys_name == 'Facebook-AI.729' and lang == 'iu-en':\n",
    "                        sys_name = 'Facebook_AI.729'\n",
    "                    if sys_name == 'Huawei-TSC.1533' and lang == 'ps-en':\n",
    "                        sys_name = 'Huawei_TSC.1533'\n",
    "                    if sys_name == 'UQAM-TanLe.520' and lang == 'iu-en':\n",
    "                        sys_name = 'UQAM_TanLe.520'\n",
    "                    if sys_name == 'HUMAN' and lang == 'ps-en':\n",
    "                        sys_name = 'Human-A.0'\n",
    "                    if sys_name == 'NICT-Kyoto.1220' and lang == 'iu-en':\n",
    "                        sys_name = 'NICT_Kyoto.1220'\n",
    "                    if sys_name == 'HUMAN' and lang == 'iu-en':\n",
    "                        sys_name = 'Human-A.0'\n",
    "                    if sys_name == 'Huawei-TSC.1539' and lang == 'km-en':\n",
    "                        sys_name = 'Huawei_TSC.1539'\n",
    "                    if sys_name == 'Huoshan-Translate.651' and lang == 'km-en':\n",
    "                        sys_name = 'Huoshan_Translate.651'\n",
    "                    if sys_name == 'HUMAN' and lang == 'km-en':\n",
    "                        sys_name = 'Human-A.0'\n",
    "                    \n",
    "                    if self.args.priorMQM and self.args.onlyDA == False and lang in ['en-de', 'zh-en']:\n",
    "                        if (sys_name, seg_id) in MQMsegments:\n",
    "                            skipped_n_records += 1\n",
    "                            continue\n",
    "                    \n",
    "                    # The \"-1\" is necessary because seg_id starts counting at 1.\n",
    "                    src_segment = src_segments[seg_id - 1]\n",
    "                    ref_segment = ref_segments[seg_id - 1]\n",
    "                    sys_segment = sys_segments[sys_name][seg_id - 1]\n",
    "                    \n",
    "                    if not src_segment or not sys_segment:\n",
    "                        logger.info(\"* Missing value!\")\n",
    "                        logger.info(\"* System: {}\".format(sys_name))\n",
    "                        logger.info(\"* Segment:\" + str(seg_id))\n",
    "                        logger.info(\"* Source segment:\" + src_segment)\n",
    "                        logger.info(\"* Sys segment:\" + sys_segment)\n",
    "                        logger.info(\"* Parsed line:\" + line)\n",
    "                        logger.info(\"* Lang:\" + lang)\n",
    "                    example = Importer18.to_json(self.year, lang, src_segment, \n",
    "                                                 ref_segment, sys_segment, raw_score, \n",
    "                                                 z_score, seg_id, sys_name, n_ratings)\n",
    "                    dest_file.write(example)\n",
    "                    dest_file.write(\"\\n\")\n",
    "                    n_records += 1\n",
    "        logger.info(\"Processed {} records of {}'s {}\".format(str(n_records), self.year, lang))\n",
    "        logger.info(\"Skipped {} records of {}'s {}\".format(str(skipped_n_records), self.year, lang))\n",
    "        return n_records\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Importer21(Importer18):\n",
    "    \"\"\"Importer for WMT20 Metrics challenge.\"\"\"\n",
    "    \n",
    "    def __init__(self, year, target_path, cache_path, args):\n",
    "        super(Importer21, self).__init__(year, target_path, cache_path, args)\n",
    "        self.args = args\n",
    "        self.tasks = ['challengeset', 'florestest2021', 'newstest2021', 'tedtalks']\n",
    "    \n",
    "    def open_tar(self, tar_path, open_dir):\n",
    "        logger.info(\"Untaring...\")\n",
    "        tar = tarfile.open(tar_path)\n",
    "        tar.extractall(path=open_dir)\n",
    "        tar.close()\n",
    "        logger.info(\"Done.\")\n",
    "    \n",
    "    def fetch_files(self):\n",
    "        \"\"\"Downloads raw datafiles from various WMT sources.\"\"\"\n",
    "        return\n",
    "    \n",
    "    def parse_rating(self, rating_line, lang):\n",
    "        rating_tuple = tuple(rating_line.split(\" \"))\n",
    "        # I have a feeling that the last field is the number of ratings\n",
    "        # but I'm not 100% sure.\n",
    "        sys_name, seg_id, raw_score, z_score, n_ratings = rating_tuple\n",
    "        \n",
    "        # en-zh has unknown seg_id probablly tagged with other format name\n",
    "#         if lang in ['en-zh', 'en-ja', 'en-iu', 'en-cs', 'en-ta', 'en-ru', 'en-de', 'en-pl']:\n",
    "#             seg_id = seg_id.split('_')[-1] \n",
    "        try:\n",
    "            seg_id = int(seg_id)\n",
    "            raw_score = float(raw_score)\n",
    "            z_score = float(z_score)\n",
    "            n_ratings = int(n_ratings)\n",
    "        except:\n",
    "            logger.info(lang)\n",
    "            logger.info(rating_line)\n",
    "        return sys_name, seg_id, raw_score, z_score, n_ratings\n",
    "\n",
    "    def parse_submission_file_name(self, fname, lang, task):\n",
    "        \"\"\"Extracts system names from the name of submission files.\"\"\"\n",
    "\n",
    "        # I added those rules to unblock the pipeline.\n",
    "\n",
    "        sys_name = fname.replace(\"{}.{}.{}.\".format(task, lang, 'hyp'), \"\", 1).replace(\".{}\".format(lang.split('-')[-1]), \"\", 1)\n",
    "\n",
    "        return sys_name\n",
    "    \n",
    "    def parse_source_file_name(self, fname, task):\n",
    "        lang = fname.replace(\"{}.\".format(task), \"\")\n",
    "        return lang[:5]\n",
    "    \n",
    "    def list_lang_pairs(self, task):\n",
    "        \"\"\"List all language pairs included in the WMT files for 2020.\"\"\"\n",
    "        \n",
    "        folder_name, _, _ = self.location_info[\"submissions\"]\n",
    "        folder = os.path.join(self.temp_directory, folder_name, 'sources', '*')\n",
    "        all_full_paths = glob.glob(folder)\n",
    "        all_files = [os.path.basename(f) for f in all_full_paths if os.path.basename(f).startswith(task)]\n",
    "        cand_lang_pairs = [self.parse_source_file_name(fname, task) for fname in all_files]\n",
    "        # We need to remove None values in cand_lang_pair:\n",
    "        lang_pairs = [lang_pair for lang_pair in cand_lang_pairs if lang_pair]\n",
    "        return list(set(lang_pairs))\n",
    "\n",
    "    def get_ratings_path(self, lang, task):\n",
    "        folder, _, _ = self.location_info[\"eval_data\"]\n",
    "\n",
    "        file_name = \"ad-seg-scores-{}.csv\".format(lang)\n",
    "        folder = os.path.join(self.temp_directory, folder, \"manual-evaluation\", \"DA\", \"ad-seg-scores-*.csv\")\n",
    "        all_files = glob.glob(folder)\n",
    "        for cand_file in all_files:\n",
    "            if cand_file.endswith(file_name):\n",
    "                return cand_file\n",
    "        raise ValueError(\"Can't find ratings for lang {}\".format(lang))\n",
    "\n",
    "    def get_src_segments(self, lang, task):\n",
    "        \"\"\"Fetches source translation segments for language pair.\"\"\"\n",
    "        folder, _, _ = self.location_info[\"submissions\"]\n",
    "        src_lang, tgt_lang = separate_lang_pair(lang)\n",
    "        src_subfolder = 'sources'\n",
    "        src_file = \"{}.{}-{}.src.{}\".format(task, src_lang, tgt_lang, src_lang)\n",
    "        src_path = os.path.join(self.temp_directory, folder, src_subfolder, src_file)\n",
    "        with open(src_path, \"r\", encoding=\"utf-8\") as f_src:\n",
    "            src_segments = f_src.readlines()\n",
    "\n",
    "        src_segments = [postprocess_segment(s) for s in src_segments]\n",
    "        \n",
    "        return src_segments\n",
    "    \n",
    "    def get_ref_segments(self, lang, task):\n",
    "        \"\"\"Fetches source and reference translation segments for language pair.\"\"\"\n",
    "        folder, _, _ = self.location_info[\"submissions\"]\n",
    "        src_lang, tgt_lang = separate_lang_pair(lang)\n",
    "        ref_subfolder = 'references'\n",
    "        ref_files = {}\n",
    "        ref_files['ref-A'] = \"{}.{}-{}.ref.ref-A.{}\".format(task, src_lang, tgt_lang, tgt_lang)\n",
    "        ref_files['ref-B'] = \"{}.{}-{}.ref.ref-B.{}\".format(task, src_lang, tgt_lang, tgt_lang)\n",
    "        ref_files['ref-C'] = \"{}.{}-{}.ref.ref-C.{}\".format(task, src_lang, tgt_lang, tgt_lang)\n",
    "        ref_files['ref-D'] = \"{}.{}-{}.ref.ref-D.{}\".format(task, src_lang, tgt_lang, tgt_lang)\n",
    "        ref_paths = {ref_id:os.path.join(self.temp_directory, folder, ref_subfolder, ref_files[ref_id]) for ref_id in ref_files.keys()}\n",
    "        ref_segments = {}\n",
    "        for ref_id in ref_paths.keys():\n",
    "            if os.path.isfile(ref_paths[ref_id]):\n",
    "                with open(ref_paths[ref_id], \"r\", encoding=\"utf-8\") as f_ref:\n",
    "                    ref_segments[ref_id] = f_ref.readlines()\n",
    "                    \n",
    "        ref_segments = {ref_id:[postprocess_segment(s) for s in ref_segments[ref_id]] for ref_id in ref_segments.keys()}\n",
    "        return ref_segments\n",
    "\n",
    "    def get_sys_segments(self, lang, task):\n",
    "        \"\"\"Builds a dictionary with the generated segments for each system.\"\"\"\n",
    "        # Gets all submission file paths.\n",
    "        folder_name, _, _ = self.location_info[\"submissions\"]\n",
    "        subfolder = os.path.join(\"system-outputs\", task)\n",
    "        folder = os.path.join(self.temp_directory, folder_name, subfolder, lang)\n",
    "        all_files = os.listdir(folder)\n",
    "#         logger.info(\"Reading submission files from {}\".format(folder))\n",
    "\n",
    "        # Extracts the generated segments for each submission.\n",
    "        sys_segments = {}\n",
    "        for sys_file_name in all_files:\n",
    "            sys_name = self.parse_submission_file_name(sys_file_name, lang, task)\n",
    "            assert sys_name\n",
    "            if sys_name.startswith(task):\n",
    "                continue\n",
    "            sys_path = os.path.join(folder, sys_file_name)\n",
    "            with open(sys_path, \"r\", encoding=\"utf-8\") as f_sys:\n",
    "                sys_lines = f_sys.readlines()\n",
    "                sys_lines = [postprocess_segment(s) for s in sys_lines]\n",
    "                sys_segments[sys_name] = sys_lines\n",
    "\n",
    "        return sys_segments\n",
    "    \n",
    "    \n",
    "    def check_dic(self, ref, sys, ref_dic, sys_dic):\n",
    "        if ref not in ref_dic:\n",
    "            ref_dic[ref] = 1\n",
    "        else:\n",
    "            ref_dic[ref] += 1\n",
    "        if sys not in sys_dic:\n",
    "            sys_dic[sys] = 1\n",
    "        else:\n",
    "            sys_dic[sys] += 1\n",
    "        return ref_dic, sys_dic\n",
    "    \n",
    "    def to_json(self, year, lang, src_segment, ref_segment, ref_id, sys_segment,\n",
    "                raw_score, z_score, seg_id, sys_name, n_ratings=0):\n",
    "        \"\"\"Converts record to JSON.\"\"\"\n",
    "        json_dict = {\"year\": year, \"lang\": lang, \"source\": src_segment, \n",
    "                     \"reference\": ref_segment, \"ref_id\":ref_id, \"candidate\": sys_segment, \"raw_score\": raw_score,\n",
    "                     \"score\": z_score, \"segment_id\": seg_id, \"system\": sys_name,\n",
    "                     \"n_ratings\": n_ratings}\n",
    "        return json.dumps(json_dict)\n",
    "    \n",
    "    def generate_records_for_lang(self, lang, task, ref_dic, sys_dic):\n",
    "        \"\"\"Consolidates all the files for a given language pair and year.\"\"\"\n",
    "        # Loads source, reference and system segments.\n",
    "        src_segments = self.get_src_segments(lang, task)\n",
    "        ref_segments = self.get_ref_segments(lang, task)\n",
    "        sys_segments = self.get_sys_segments(lang, task)\n",
    "    \n",
    "        # Streams the rating file and performs the join on-the-fly.\n",
    "#         ratings_file_path = self.get_ratings_path(lang)\n",
    "#         logger.info(\"Reading file {}\".format(ratings_file_path))\n",
    "        n_records = 0\n",
    "        skipped_n_records = 0\n",
    "#         if lang in ['en-zh', 'en-ja', 'en-iu', 'en-cs', 'en-ta', 'en-ru', 'en-de', 'en-pl'] and (not self.include_unreliables):\n",
    "#             return 0\n",
    "        with open(self.target_path, \"a+\") as dest_file:\n",
    "            for sys_name in sys_segments.keys():\n",
    "                for seg_id in range(len(src_segments)):\n",
    "                    if len(ref_segments) > 1:\n",
    "                        if lang != 'en-de':\n",
    "                            ref_id1 = list(ref_segments.keys())[0]\n",
    "                            ref_id2 = list(ref_segments.keys())[1]\n",
    "                            src_segment = src_segments[seg_id]\n",
    "                            ref_segment = ref_segments[ref_id1][seg_id]\n",
    "                            sys_segment = sys_segments[sys_name][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id1, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, sys_name, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id1, sys_name, ref_dic, sys_dic)\n",
    "                            \n",
    "                            ref_segment = ref_segments[ref_id2][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id2, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, sys_name, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id2, sys_name, ref_dic, sys_dic)\n",
    "                            \n",
    "                            ref_segment = ref_segments[ref_id1][seg_id]\n",
    "                            sys_segment = ref_segments[ref_id2][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id1, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, ref_id2, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id1, ref_id2, ref_dic, sys_dic)\n",
    "                            \n",
    "                            ref_segment = ref_segments[ref_id2][seg_id]\n",
    "                            sys_segment = ref_segments[ref_id1][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id2, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, ref_id1, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id2, ref_id1, ref_dic, sys_dic)\n",
    "                        else:\n",
    "                            ref_id1 = list(ref_segments.keys())[0]\n",
    "                            ref_id2 = list(ref_segments.keys())[1]\n",
    "                            ref_id3 = list(ref_segments.keys())[2]\n",
    "                            ref_id4 = list(ref_segments.keys())[3]\n",
    "                            src_segment = src_segments[seg_id]\n",
    "                            ref_segment = ref_segments[ref_id1][seg_id]\n",
    "                            sys_segment = sys_segments[sys_name][seg_id]\n",
    "                            ####\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id1, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, sys_name, 0)\n",
    "                            \n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id1, sys_name, ref_dic, sys_dic)\n",
    "                            \n",
    "                            ref_segment = ref_segments[ref_id2][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id2, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, sys_name, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id2, sys_name, ref_dic, sys_dic)\n",
    "                            \n",
    "                            ref_segment = ref_segments[ref_id2][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id3, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, sys_name, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id3, sys_name, ref_dic, sys_dic)\n",
    "                            \n",
    "                            ref_segment = ref_segments[ref_id2][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id4, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, sys_name, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id4, sys_name, ref_dic, sys_dic)\n",
    "                            #####\n",
    "                            ref_segment = ref_segments[ref_id1][seg_id]\n",
    "                            sys_segment = ref_segments[ref_id2][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id1, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, ref_id2, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id1, ref_id2, ref_dic, sys_dic)\n",
    "                            \n",
    "                            ref_segment = ref_segments[ref_id1][seg_id]\n",
    "                            sys_segment = ref_segments[ref_id3][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id1, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, ref_id3, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id1, ref_id3, ref_dic, sys_dic)\n",
    "                            \n",
    "                            ref_segment = ref_segments[ref_id1][seg_id]\n",
    "                            sys_segment = ref_segments[ref_id4][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id1, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, ref_id4, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id1, ref_id4, ref_dic, sys_dic)\n",
    "                            ####\n",
    "                            ref_segment = ref_segments[ref_id2][seg_id]\n",
    "                            sys_segment = ref_segments[ref_id1][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id2, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, ref_id1, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id2, ref_id1, ref_dic, sys_dic)\n",
    "                            \n",
    "                            ref_segment = ref_segments[ref_id2][seg_id]\n",
    "                            sys_segment = ref_segments[ref_id3][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id2, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, ref_id3, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id2, ref_id3, ref_dic, sys_dic)\n",
    "                            \n",
    "                            ref_segment = ref_segments[ref_id2][seg_id]\n",
    "                            sys_segment = ref_segments[ref_id4][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id2, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, ref_id4, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id2, ref_id4, ref_dic, sys_dic)\n",
    "                            \n",
    "                            #####\n",
    "                            ref_segment = ref_segments[ref_id3][seg_id]\n",
    "                            sys_segment = ref_segments[ref_id1][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id3, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, ref_id1, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id3, ref_id1, ref_dic, sys_dic)\n",
    "                            \n",
    "                            ref_segment = ref_segments[ref_id3][seg_id]\n",
    "                            sys_segment = ref_segments[ref_id2][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id3, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, ref_id2, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id3, ref_id2, ref_dic, sys_dic)\n",
    "                            \n",
    "                            ref_segment = ref_segments[ref_id3][seg_id]\n",
    "                            sys_segment = ref_segments[ref_id4][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id3, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, ref_id4, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id3, ref_id4, ref_dic, sys_dic)\n",
    "                            #####\n",
    "                            ref_segment = ref_segments[ref_id4][seg_id]\n",
    "                            sys_segment = ref_segments[ref_id1][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id4, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, ref_id1, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id4, ref_id1, ref_dic, sys_dic)\n",
    "                            \n",
    "                            ref_segment = ref_segments[ref_id4][seg_id]\n",
    "                            sys_segment = ref_segments[ref_id2][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id4, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, ref_id2, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id4, ref_id2, ref_dic, sys_dic)\n",
    "                            \n",
    "                            ref_segment = ref_segments[ref_id4][seg_id]\n",
    "                            sys_segment = ref_segments[ref_id3][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id4, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, ref_id3, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id4, ref_id3, ref_dic, sys_dic)\n",
    "                    else:\n",
    "                        ref_id = list(ref_segments.keys())[0]\n",
    "                        src_segment = src_segments[seg_id]\n",
    "                        ref_segment = ref_segments[ref_id][seg_id]\n",
    "                        sys_segment = sys_segments[sys_name][seg_id]\n",
    "                        example = self.to_json(self.year, lang, src_segment, \n",
    "                                                     ref_segment, ref_id, sys_segment, 0.0, \n",
    "                                                     0.0, seg_id+1, sys_name, 0)\n",
    "                        dest_file.write(example)\n",
    "                        dest_file.write(\"\\n\")\n",
    "                        n_records += 1\n",
    "                        ref_dic, sys_dic = self.check_dic(ref_id, sys_name, ref_dic, sys_dic)\n",
    "#         logger.info(\"Processed {} records of {}'s {}\".format(str(n_records), self.year, lang))\n",
    "#         logger.info(\"Skipped {} records of {}'s {}\".format(str(skipped_n_records), self.year, lang))\n",
    "        return n_records, ref_dic, sys_dic\n",
    "\n",
    "    def generate_records(self, task, ref_dic, sys_dic):\n",
    "        \"\"\"Consolidates all the files for a given language pair and year.\"\"\"\n",
    "        n_records = 0\n",
    "        skipped_n_records = 0\n",
    "        with open(self.target_path, \"w\") as dest_file:\n",
    "            for lang in importer.list_lang_pairs(task):\n",
    "                logger.info('{}, {}'.format(lang, task))\n",
    "                # Loads source, reference and system segments.\n",
    "                src_segments = self.get_src_segments(lang, task)\n",
    "                ref_segments = self.get_ref_segments(lang, task)\n",
    "                sys_segments = self.get_sys_segments(lang, task)\n",
    "\n",
    "            # Streams the rating file and performs the join on-the-fly.\n",
    "    #         ratings_file_path = self.get_ratings_path(lang)\n",
    "    #         logger.info(\"Reading file {}\".format(ratings_file_path))\n",
    "\n",
    "\n",
    "    #         if lang in ['en-zh', 'en-ja', 'en-iu', 'en-cs', 'en-ta', 'en-ru', 'en-de', 'en-pl'] and (not self.include_unreliables):\n",
    "    #             return 0\n",
    "\n",
    "                for sys_name in sys_segments.keys():\n",
    "                    for seg_id in range(len(src_segments)):\n",
    "                        if len(ref_segments) > 1:\n",
    "                            if lang != 'en-de':\n",
    "                                ref_id1 = list(ref_segments.keys())[0]\n",
    "                                ref_id2 = list(ref_segments.keys())[1]\n",
    "                                src_segment = src_segments[seg_id]\n",
    "                                ref_segment = ref_segments[ref_id1][seg_id]\n",
    "                                sys_segment = sys_segments[sys_name][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id1, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, sys_name, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id1, sys_name, ref_dic, sys_dic)\n",
    "\n",
    "                                ref_segment = ref_segments[ref_id2][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id2, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, sys_name, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id2, sys_name, ref_dic, sys_dic)\n",
    "\n",
    "                                ref_segment = ref_segments[ref_id1][seg_id]\n",
    "                                sys_segment = ref_segments[ref_id2][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id1, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, ref_id2, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id1, ref_id2, ref_dic, sys_dic)\n",
    "\n",
    "                                ref_segment = ref_segments[ref_id2][seg_id]\n",
    "                                sys_segment = ref_segments[ref_id1][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id2, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, ref_id1, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id2, ref_id1, ref_dic, sys_dic)\n",
    "                            else:\n",
    "                                ref_id1 = list(ref_segments.keys())[0]\n",
    "                                ref_id2 = list(ref_segments.keys())[1]\n",
    "                                ref_id3 = list(ref_segments.keys())[2]\n",
    "                                ref_id4 = list(ref_segments.keys())[3]\n",
    "                                src_segment = src_segments[seg_id]\n",
    "                                ref_segment = ref_segments[ref_id1][seg_id]\n",
    "                                sys_segment = sys_segments[sys_name][seg_id]\n",
    "                                ####\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id1, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, sys_name, 0)\n",
    "\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id1, sys_name, ref_dic, sys_dic)\n",
    "\n",
    "                                ref_segment = ref_segments[ref_id2][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id2, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, sys_name, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id2, sys_name, ref_dic, sys_dic)\n",
    "\n",
    "                                ref_segment = ref_segments[ref_id2][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id3, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, sys_name, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id3, sys_name, ref_dic, sys_dic)\n",
    "\n",
    "                                ref_segment = ref_segments[ref_id2][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id4, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, sys_name, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id4, sys_name, ref_dic, sys_dic)\n",
    "                                #####\n",
    "                                ref_segment = ref_segments[ref_id1][seg_id]\n",
    "                                sys_segment = ref_segments[ref_id2][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id1, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, ref_id2, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id1, ref_id2, ref_dic, sys_dic)\n",
    "\n",
    "                                ref_segment = ref_segments[ref_id1][seg_id]\n",
    "                                sys_segment = ref_segments[ref_id3][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id1, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, ref_id3, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id1, ref_id3, ref_dic, sys_dic)\n",
    "\n",
    "                                ref_segment = ref_segments[ref_id1][seg_id]\n",
    "                                sys_segment = ref_segments[ref_id4][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id1, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, ref_id4, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id1, ref_id4, ref_dic, sys_dic)\n",
    "                                ####\n",
    "                                ref_segment = ref_segments[ref_id2][seg_id]\n",
    "                                sys_segment = ref_segments[ref_id1][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id2, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, ref_id1, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id2, ref_id1, ref_dic, sys_dic)\n",
    "\n",
    "                                ref_segment = ref_segments[ref_id2][seg_id]\n",
    "                                sys_segment = ref_segments[ref_id3][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id2, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, ref_id3, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id2, ref_id3, ref_dic, sys_dic)\n",
    "\n",
    "                                ref_segment = ref_segments[ref_id2][seg_id]\n",
    "                                sys_segment = ref_segments[ref_id4][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id2, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, ref_id4, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id2, ref_id4, ref_dic, sys_dic)\n",
    "\n",
    "                                #####\n",
    "                                ref_segment = ref_segments[ref_id3][seg_id]\n",
    "                                sys_segment = ref_segments[ref_id1][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id3, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, ref_id1, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id3, ref_id1, ref_dic, sys_dic)\n",
    "\n",
    "                                ref_segment = ref_segments[ref_id3][seg_id]\n",
    "                                sys_segment = ref_segments[ref_id2][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id3, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, ref_id2, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id3, ref_id2, ref_dic, sys_dic)\n",
    "\n",
    "                                ref_segment = ref_segments[ref_id3][seg_id]\n",
    "                                sys_segment = ref_segments[ref_id4][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id3, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, ref_id4, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id3, ref_id4, ref_dic, sys_dic)\n",
    "                                #####\n",
    "                                ref_segment = ref_segments[ref_id4][seg_id]\n",
    "                                sys_segment = ref_segments[ref_id1][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id4, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, ref_id1, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id4, ref_id1, ref_dic, sys_dic)\n",
    "\n",
    "                                ref_segment = ref_segments[ref_id4][seg_id]\n",
    "                                sys_segment = ref_segments[ref_id2][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id4, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, ref_id2, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id4, ref_id2, ref_dic, sys_dic)\n",
    "\n",
    "                                ref_segment = ref_segments[ref_id4][seg_id]\n",
    "                                sys_segment = ref_segments[ref_id3][seg_id]\n",
    "                                example = self.to_json(self.year, lang, src_segment, \n",
    "                                                             ref_segment, ref_id4, sys_segment, 0.0, \n",
    "                                                             0.0, seg_id+1, ref_id3, 0)\n",
    "                                dest_file.write(example)\n",
    "                                dest_file.write(\"\\n\")\n",
    "                                n_records += 1\n",
    "                                ref_dic, sys_dic = self.check_dic(ref_id4, ref_id3, ref_dic, sys_dic)\n",
    "                        else:\n",
    "                            ref_id = list(ref_segments.keys())[0]\n",
    "                            src_segment = src_segments[seg_id]\n",
    "                            ref_segment = ref_segments[ref_id][seg_id]\n",
    "                            sys_segment = sys_segments[sys_name][seg_id]\n",
    "                            example = self.to_json(self.year, lang, src_segment, \n",
    "                                                         ref_segment, ref_id, sys_segment, 0.0, \n",
    "                                                         0.0, seg_id+1, sys_name, 0)\n",
    "                            dest_file.write(example)\n",
    "                            dest_file.write(\"\\n\")\n",
    "                            n_records += 1\n",
    "                            ref_dic, sys_dic = self.check_dic(ref_id, sys_name, ref_dic, sys_dic)\n",
    "        return n_records, ref_dic, sys_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImporterOriginal(Importer21):\n",
    "    def __init__(self, year, target_path, cache_path, args):\n",
    "        super(Importer21, self).__init__(year, target_path, cache_path, args)\n",
    "        self.args = args\n",
    "    \n",
    "    def generate_records_for_lang(self, src_file, ref_file, hyp_file, lang):\n",
    "        src_segments = []\n",
    "        ref_segments = []\n",
    "        hyp_segments = []\n",
    "        \n",
    "        with open(src_file, \"r\", encoding=\"utf-8\") as f_src:\n",
    "            src_segments = f_src.readlines()\n",
    "        src_segments = [postprocess_segment(s) for s in src_segments]\n",
    "        \n",
    "        with open(ref_file, \"r\", encoding=\"utf-8\") as f_ref:\n",
    "            ref_segments = f_ref.readlines()\n",
    "        ref_segments = [postprocess_segment(s) for s in ref_segments]\n",
    "        \n",
    "        with open(hyp_file, \"r\", encoding=\"utf-8\") as f_hyp:\n",
    "            hyp_segments = f_hyp.readlines()\n",
    "        hyp_segments = [postprocess_segment(s) for s in hyp_segments]\n",
    "        \n",
    "        assert len(src_segments) == len(ref_segments) and len(src_segments) == len(hyp_segments)\n",
    "        n_records = 0\n",
    "        with open(self.target_path, \"a+\") as dest_file:\n",
    "            for i in range(len(src_segments)):\n",
    "                seg_id = i + 1\n",
    "                # year, lang, src_segment, ref_segment, ref_id, sys_segment,\n",
    "                # raw_score, z_score, seg_id, sys_name, n_ratings=0\n",
    "                example = self.to_json(self.year, lang, src_segments[i], ref_segments[i], 'ref-A', hyp_segments[i], \n",
    "                                       0.0, 0.0, seg_id, '', 0)\n",
    "                dest_file.write(example)\n",
    "                dest_file.write(\"\\n\")\n",
    "                n_records += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading and processing wmt data...\n",
      "Saving clean file.\n",
      "Cleaning up old ratings file.\n"
     ]
    }
   ],
   "source": [
    "class Importer21_DA():\n",
    "    def __init__(self, save_dir, wo_MQM=True):\n",
    "        self.wo_MQM = wo_MQM\n",
    "        if wo_MQM:\n",
    "            save_dir = save_dir.replace('.json', '_wo_MQM.json')\n",
    "        self.save_dir = save_dir\n",
    "        self.save_tmp = save_dir + '_tmp'\n",
    "        self.DA21_HOME_DIR = '/home/is/kosuke-t/project_disc/WMT/wmt21-news-systems/humaneval'\n",
    "        self.folder_names = ['facebook-wikipedia-2021', 'newstest2021-EX_and_XY', 'newstest2021-toen']\n",
    "        \n",
    "    def parse_eval_file_name(self, fname):\n",
    "        \"\"\"Extracts language pairs from the names of human rating files.\"\"\"\n",
    "        wmt_pattern = re.compile(r\"^ad-seg-scores-([a-z]{2}-[a-z]{2})\\.csv\")\n",
    "        match = re.match(wmt_pattern, fname)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def list_lang_pairs(self, folder):\n",
    "        \"\"\"List all language pairs included in the WMT files for 2018.\"\"\"\n",
    "        all_files = os.listdir(folder)\n",
    "        cand_lang_pairs = [self.parse_eval_file_name(fname) for fname in all_files]\n",
    "        # We need to remove None values in cand_lang_pair:\n",
    "        lang_pairs = [lang_pair for lang_pair in cand_lang_pairs if lang_pair]\n",
    "        return list(set(lang_pairs))\n",
    "\n",
    "    def parse_rating(self, rating_line):\n",
    "        rating_tuple = tuple(rating_line.split(\" \"))\n",
    "        # I have a feeling that the last field is the number of ratings\n",
    "        # but I'm not 100% sure.\n",
    "        if len(rating_tuple) != 5:\n",
    "            logger.info(\"len of rating_tuple = {}\".format(len(rating_tuple)))\n",
    "            logger.info(rating_tuple)\n",
    "\n",
    "        sys_name, seg_id, raw_score, z_score, n_ratings = rating_tuple\n",
    "\n",
    "        # For some reason, all the systems for pair zh-en have an extra suffix.\n",
    "        if sys_name.endswith(\"zh-en\"):\n",
    "            sys_name = sys_name[:-6]\n",
    "\n",
    "        try:\n",
    "            seg_id = int(seg_id)\n",
    "            raw_score = float(raw_score)\n",
    "            z_score = float(z_score)\n",
    "            n_ratings = int(n_ratings)\n",
    "        except:\n",
    "            logger.info(rating_tuple)\n",
    "        return sys_name, seg_id, raw_score, z_score, n_ratings\n",
    "\n",
    "    def get_src_ref(self, lang):\n",
    "        \"\"\"Fetches source and reference translation segments for language pair.\"\"\"\n",
    "        folder = '/home/is/kosuke-t/project_disc/WMT/wmt21-news-systems'\n",
    "        src_subfolder = os.path.join(\"txt\", \"sources\")\n",
    "        ref_subfolder = os.path.join(\"txt\", \"references\")\n",
    "        src_lang = lang[:2]\n",
    "        tgt_lang = lang[-2:]\n",
    "        src_file = \"newstest2021.{}-{}.src.{}\".format(src_lang, tgt_lang, src_lang)\n",
    "        ref_file = \"newstest2021.{}-{}.ref.A.{}\".format(src_lang, tgt_lang, tgt_lang)\n",
    "        src_path = os.path.join(folder, src_subfolder, src_file)\n",
    "        ref_path = os.path.join(folder, ref_subfolder, ref_file)\n",
    "\n",
    "        with open(src_path, \"r\", encoding=\"utf-8\") as f_src:\n",
    "            src_segments = f_src.readlines()\n",
    "        with open(ref_path, \"r\", encoding=\"utf-8\") as f_ref:\n",
    "            ref_segments = f_ref.readlines()\n",
    "\n",
    "        src_segments = [s.strip() for s in src_segments]\n",
    "        ref_segments = [s.strip() for s in ref_segments]\n",
    "\n",
    "        return src_segments, ref_segments\n",
    "\n",
    "    def to_json(self, year, lang, src_segment, ref_segment, ref_id, sys_segment,\n",
    "                    raw_score, z_score, seg_id, sys_name, n_ratings=0):\n",
    "            \"\"\"Converts record to JSON.\"\"\"\n",
    "            json_dict = {\"year\": year, \"lang\": lang, \"source\": src_segment, \n",
    "                         \"reference\": ref_segment, \"ref_id\":ref_id, \"candidate\": sys_segment, \"raw_score\": raw_score,\n",
    "                         \"score\": z_score, \"segment_id\": seg_id, \"system\": sys_name,\n",
    "                         \"n_ratings\": n_ratings}\n",
    "            return json.dumps(json_dict)\n",
    "    \n",
    "    def get_sys(self, lang):\n",
    "        sys_segments = {}\n",
    "        src_lang = lang[:2]\n",
    "        tgt_lang = lang[-2:]\n",
    "        folder = '/home/is/kosuke-t/project_disc/WMT/wmt21-news-systems/txt/system-outputs'\n",
    "        paths = os.path.join(folder, \"newstest2021.{}.hyp.*.{}\".format(lang, tgt_lang))\n",
    "        all_files = glob.glob(paths)\n",
    "        for fname in all_files:\n",
    "            fname_base = os.path.basename(fname)\n",
    "            sys_name = re.sub(r'newstest2021\\.|{}\\.hyp\\.|\\.{}'.format(lang, tgt_lang), '',fname_base)\n",
    "            with open(fname, mode='r', encoding='utf-8') as r:\n",
    "                data = r.readlines()\n",
    "            data = [d.strip() for d in data]\n",
    "            sys_segments[sys_name] = data\n",
    "        return sys_segments\n",
    "    \n",
    "    def get_21_DA(self):\n",
    "        langs_list = []\n",
    "        DA_files_dic = {}\n",
    "        sys_names_exception = {'cs-en':['Facebook-AI.7', 'Online-A.5', 'Online-B.0',\n",
    "                                        'CUNI-Transformer2018.1', 'Online-Y.3', 'CUNI-DocTransformer.4', \n",
    "                                        'HUMAN.0', 'Online-W.2', 'Online-G.6'], \n",
    "                               'de-en':['UF.15', 'Manifold.2', 'Online-A.12', 'happypoet.8', 'SMU.17',\n",
    "                                        'HUMAN.0', 'P3AI.18', 'Online-Y.1', \n",
    "                                        'HuaweiTSC.6', 'VolcTrans-AT.0', 'Facebook-AI.16', \n",
    "                                        'Online-B.11', 'VolcTrans-GLAT.10', 'Nemo.7']}\n",
    "        for folder_name in self.folder_names:\n",
    "            langs_list.extend(self.list_lang_pairs(os.path.join(self.DA21_HOME_DIR, folder_name)))\n",
    "            for lang in langs_list:\n",
    "                folder = os.path.join(self.DA21_HOME_DIR, folder_name, \"ad-seg-scores-*.csv\")\n",
    "                all_files = glob.glob(folder)\n",
    "                for cand_file in all_files:\n",
    "                    if cand_file not in DA_files_dic:\n",
    "                        DA_files_dic[cand_file] = 1\n",
    "                    else:\n",
    "                        DA_files_dic[cand_file] += 1\n",
    "        for filename in DA_files_dic.keys():\n",
    "            lang = filename[-9:-4]\n",
    "            if self.wo_MQM:\n",
    "                except_langs = ['bn-hi', 'hi-bn', 'xh-zu', 'zu-xh', 'en-de', 'zh-en']\n",
    "            else:\n",
    "                except_langs = ['bn-hi', 'hi-bn', 'xh-zu', 'zu-xh']\n",
    "            if lang in except_langs:\n",
    "                continue\n",
    "            src_segments, ref_segments = self.get_src_ref(lang)\n",
    "            sys_segments = self.get_sys(lang)\n",
    "            r = open(filename, mode='r', encoding='utf-8')\n",
    "            data = r.readlines()\n",
    "            with open(self.save_tmp, mode='w+', encoding='utf-8') as w:\n",
    "                for i, raw in enumerate(data):\n",
    "                    raw = raw.strip()\n",
    "                    if i == 0:\n",
    "                        continue\n",
    "                    sys_name, seg_id, raw_score, z_score, n_ratings = self.parse_rating(raw)\n",
    "                    src_segment = src_segments[seg_id-1]\n",
    "                    ref_segment = ref_segments[seg_id-1]\n",
    "                    try:\n",
    "                        sys_segment = sys_segments[sys_name][seg_id-1]\n",
    "                    except:\n",
    "                        if lang not in sys_names_exception:\n",
    "                            sys_names_exception[lang] = []\n",
    "                        if sys_name not in sys_names_exception[lang]:\n",
    "                            sys_names_exception[lang].append(sys_name)\n",
    "                        sys_name = sys_name[:sys_name.find('.')]\n",
    "                        if sys_name == 'HUMAN':\n",
    "                            continue\n",
    "                        else:\n",
    "                            if sys_name == 'Nemo' and lang in ['de-en', 'ru-en']:\n",
    "                                sys_name = 'NVIDIA-NeMo'\n",
    "                            if sys_name == 'Allegro' and lang == 'is-en':\n",
    "                                sys_name = 'Allegro.eu'\n",
    "                            if sys_name not in sys_segments:\n",
    "                                logger.info(lang)\n",
    "                                logger.info(sys_name)\n",
    "                                logger.info(sys_segments.keys())\n",
    "                                return\n",
    "                            sys_segment = sys_segments[sys_name][seg_id-1]\n",
    "                    example = self.to_json('21', lang, src_segment, \n",
    "                                           ref_segment, 'ref-A', sys_segment, raw_score, \n",
    "                                           z_score, seg_id, sys_name, 0)\n",
    "                    w.write(example)\n",
    "                    w.write(\"\\n\")\n",
    "                    \n",
    "            r.close()\n",
    "    def postprocess(self, remove_null_refs=True, average_duplicates=True):\n",
    "        \"\"\"Postprocesses a JSONL file of ratings downloaded from WMT.\"\"\"\n",
    "        logger.info(\"Reading and processing wmt data...\")\n",
    "        with open(self.save_tmp, \"r\") as f:\n",
    "            ratings_df = pd.read_json(f, lines=True)\n",
    "        # ratings_df = ratings_df[[\"lang\", \"reference\", \"candidate\", \"rating\"]]\n",
    "        ratings_df.rename(columns={\"rating\": \"score\"}, inplace=True)\n",
    "\n",
    "        if remove_null_refs:\n",
    "            ratings_df = ratings_df[ratings_df[\"reference\"].notnull()]\n",
    "            assert not ratings_df.empty\n",
    "\n",
    "        if average_duplicates:\n",
    "            try:\n",
    "                ratings_df = ratings_df.groupby(by=[\"lang\", \"source\", \"candidate\", \"reference\"]).agg({\"score\": \"mean\",}).reset_index()\n",
    "            except:\n",
    "                logger.info('No duplicates.')\n",
    "        \n",
    "        logger.info(\"Saving clean file.\")\n",
    "        with open(self.save_dir, \"w\") as f:\n",
    "            ratings_df.to_json(f, orient=\"records\", lines=True)\n",
    "        logger.info(\"Cleaning up old ratings file.\")\n",
    "        os.remove(self.save_tmp)\n",
    "            \n",
    "    def _shuffle_leaky(self, all_ratings_df, n_train):\n",
    "        \"\"\"Shuffles and splits the ratings allowing overlap in the ref sentences.\"\"\"\n",
    "        all_ratings_df = all_ratings_df.sample(frac=1, random_state=555)\n",
    "        all_ratings_df = all_ratings_df.reset_index(drop=True)\n",
    "        train_ratings_df = all_ratings_df.iloc[:n_train].copy()\n",
    "        dev_ratings_df = all_ratings_df.iloc[n_train:].copy()\n",
    "        assert len(train_ratings_df) + len(dev_ratings_df) == len(all_ratings_df)\n",
    "        return train_ratings_df, dev_ratings_df \n",
    "    \n",
    "    def _shuffle_no_leak(self, all_ratings_df, n_train):\n",
    "        \"\"\"Splits and shuffles such that there is no train/dev example with the same ref.\"\"\"\n",
    "\n",
    "        def is_split_leaky(ix):\n",
    "            return (all_ratings_df.iloc[ix].reference == all_ratings_df.iloc[ix-1].reference)\n",
    "\n",
    "        assert 0 < n_train < len(all_ratings_df.index)\n",
    "\n",
    "        # Clusters the examples by reference sentence.\n",
    "        sentences = all_ratings_df.reference.sample(frac=1, random_state=555).unique()\n",
    "        sentence_to_ix = {s: i for i, s in enumerate(sentences)}\n",
    "        all_ratings_df[\"__sentence_ix__\"] = [sentence_to_ix[s] for s in all_ratings_df.reference]\n",
    "        all_ratings_df = all_ratings_df.sort_values(by=\"__sentence_ix__\")\n",
    "        all_ratings_df.drop(columns=[\"__sentence_ix__\"], inplace=True)\n",
    "\n",
    "        # Moves the split point until there is no leakage.\n",
    "        split_ix = n_train\n",
    "        n_dev_sentences = len(all_ratings_df.iloc[split_ix:].reference.unique())\n",
    "        if n_dev_sentences == 1 and is_split_leaky(split_ix):\n",
    "            raise ValueError(\"Failed splitting data--not enough distinct dev sentences to prevent leak.\")\n",
    "        while is_split_leaky(split_ix):\n",
    "            split_ix += 1\n",
    "        if n_train != split_ix:\n",
    "            logger.info(\"Moved split point from {} to {} to prevent sentence leaking\".format(n_train, split_ix))\n",
    "\n",
    "        # Shuffles the train and dev sets separately.\n",
    "        train_ratings_df = all_ratings_df.iloc[:split_ix].copy()\n",
    "        train_ratings_df = train_ratings_df.sample(frac=1, random_state=555)\n",
    "        dev_ratings_df = all_ratings_df.iloc[split_ix:].copy()\n",
    "        dev_ratings_df = dev_ratings_df.sample(frac=1, random_state=555)\n",
    "        assert len(train_ratings_df) + len(dev_ratings_df) == len(all_ratings_df)\n",
    "\n",
    "        # Checks that there is no leakage.\n",
    "        train_sentences = train_ratings_df.reference.unique()\n",
    "        dev_sentences = dev_ratings_df.reference.unique()\n",
    "        logger.info(\"Using {} and {} unique sentences for train and dev.\".format(len(train_sentences), len(dev_sentences)))\n",
    "        assert not bool(set(train_sentences) & set(dev_sentences))\n",
    "\n",
    "        return train_ratings_df, dev_ratings_df\n",
    "    \n",
    "    \n",
    "    def shuffle_split(self,\n",
    "                      train_file=None,\n",
    "                      dev_file=None,\n",
    "                      dev_ratio=0.1,\n",
    "                      prevent_leaks=True):\n",
    "        \n",
    "        \"\"\"Splits a JSONL WMT ratings file into train/dev.\"\"\"\n",
    "        logger.info(\"\\n*** Splitting WMT data in train/dev.\")\n",
    "        \n",
    "        ratings_file = self.save_dir\n",
    "        \n",
    "        assert os.path.isfile(ratings_file), \"WMT ratings file not found!\"\n",
    "        base_file = ratings_file + \"_raw\"\n",
    "        os.replace(ratings_file, base_file)\n",
    "\n",
    "        logger.info(\"Reading wmt data...\")\n",
    "        with open(base_file, \"r\") as f:\n",
    "            ratings_df = pd.read_json(f, lines=True)\n",
    "\n",
    "        logger.info(\"Doing the shuffle / split.\")\n",
    "        n_rows, n_train = len(ratings_df), int((1 - dev_ratio) * len(ratings_df))\n",
    "        logger.info(\"Will attempt to set aside {} out of {} rows for dev.\".format(n_rows - n_train, n_rows))\n",
    "        if prevent_leaks:\n",
    "            train_df, dev_df = self._shuffle_no_leak(ratings_df, n_train)\n",
    "        else:\n",
    "            train_df, dev_df = self._shuffle_leaky(ratings_df, n_train)\n",
    "        logger.info(\"Split train and dev files with {} and {} records.\".format(len(train_df), len(dev_df)))\n",
    "\n",
    "        logger.info(\"Saving clean file.\")\n",
    "        if not train_file:\n",
    "            train_file = ratings_file.replace(\".json\", \"_train.json\")\n",
    "        with open(train_file, \"w\") as f:\n",
    "            train_df.to_json(f, orient=\"records\", lines=True)\n",
    "        if not dev_file:\n",
    "            dev_file = ratings_file.replace(\".json\", \"_dev.json\")\n",
    "        with open(dev_file, \"w\") as f:\n",
    "            dev_df.to_json(f, orient=\"records\", lines=True)\n",
    "\n",
    "        logger.info(\"Cleaning up old ratings file.\")\n",
    "        logger.info(\"Created train and dev files with {} and {} records.\".format(len(train_df), len(dev_df)))\n",
    "        os.remove(base_file)\n",
    "    \n",
    "importer = Importer21_DA('/home/is/kosuke-t/project_disc/WMT/wmt21_DA.json', wo_MQM=True)\n",
    "importer.get_21_DA()\n",
    "importer.postprocess()\n",
    "# importer.shuffle_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                        | 0/2 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_HOME = '/ahc/work3/kosuke-t/WMT'\n",
    "src_20_files = os.path.join(DATA_HOME, 'WMT20_data/txt/sources/newstest2020-{}-src.{}.txt'.format('LANGWO_LETTERS', 'LANG1_LETTERS'))\n",
    "ref_20_files = os.path.join(DATA_HOME, 'WMT20_data/txt/references/newstest2020-{}-ref.{}.txt'.format('LANGWO_LETTERS', 'LANG2_LETTERS'))\n",
    "MQM_avg_20_files = os.path.join(DATA_HOME, 'wmt-mqm-human-evaluation/newstest2020/{}/mqm_newstest2020_{}.avg_seg_scores.tsv'.format('LANGWO_LETTERS', 'LANGWO_LETTERS'))\n",
    "MQM_tag_20_files = os.path.join(DATA_HOME, 'wmt-mqm-human-evaluation/newstest2020/{}/mqm_newstest2020_{}.tsv'.format('LANGWO_LETTERS', 'LANGWO_LETTERS'))\n",
    "\n",
    "MQM_1hot_only_severity_to_vec = {\"no-error\":np.asarray([1, 0, 0]), \n",
    "                                 \"No-error\":np.asarray([1, 0, 0]),\n",
    "                                 \"Neutral\":np.asarray([0, 1, 0]),\n",
    "                                 \"Minor\":np.asarray([0, 1, 0]),\n",
    "                                 \"Major\":np.asarray([0, 0, 1])}\n",
    "MQM_tag_list = [\"No-error\", \n",
    "                \"Neutral\",\n",
    "                \"Minor\",\n",
    "                \"Major\"]\n",
    "\n",
    "\n",
    "class MQM_importer20():\n",
    "    def __init__(self, save_dir, google_avg=False, my_avg=False,\n",
    "                 emb_label=False, emb_only_sev=True, \n",
    "                 no_error_score=np.asarray([1, 0, 0]),\n",
    "                 tokenizer_name='xlm-roberta-large',\n",
    "                 google_score=True, split_dev=False, dev_ratio=0.1):\n",
    "        self.save_dir = save_dir\n",
    "        self.save_tmp = save_dir + '_tmp'\n",
    "        self.year = '20'\n",
    "        self.google_avg = google_avg\n",
    "        self.my_avg = my_avg\n",
    "        self.emb_label = emb_label\n",
    "        self.emb_only_sev = emb_only_sev\n",
    "        self.no_error_score = no_error_score\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        self.google_score = google_score\n",
    "        self.split_dev = split_dev\n",
    "        self.dev_ratio = dev_ratio\n",
    "        if not google_avg:\n",
    "            self.my_avg = True\n",
    "    \n",
    "    \n",
    "    def get_MQM_fname(self, fname, lang):\n",
    "        lang1 = lang.split('-')[0]\n",
    "        lang2 = lang.split('-')[1]\n",
    "        langwo = lang.replace('-', '')\n",
    "        return fname.replace('LANGWO_LETTERS', langwo, 2)\n",
    "    \n",
    "    def load_file(self, fname):\n",
    "        with open(fname, mode='r', encoding='utf-8') as r:\n",
    "            return r.readlines()\n",
    "        \n",
    "    def get_langs(self):\n",
    "        return ['en-de', 'zh-en']\n",
    "    \n",
    "    def get_score(self, category, severity):\n",
    "        if self.google_score:\n",
    "            scores = [0.0, 0.1, 1.0, 5.0, 25.0]\n",
    "        else:\n",
    "            scores = [0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "        if severity == 'Major':\n",
    "            if category == 'Non-translation!' or category == 'Non-translation':\n",
    "                return scores[4]\n",
    "            else:\n",
    "                return scores[3]\n",
    "        elif severity == 'Minor':\n",
    "            if category == 'Fluency/Punctuation':\n",
    "                return scores[1]\n",
    "            else:\n",
    "                return scores[2]\n",
    "        elif severity == 'no-error' or severity == 'No-error' or severity == 'Neutral':\n",
    "            return scores[0]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    \n",
    "    def get_untagged_hyp(self, tagged_hyp):\n",
    "        return tagged_hyp.replace('<v>', '').replace('</v>', '')\n",
    "    \n",
    "    def get_untagged_token(self, tagged_hyp):\n",
    "        untagged_hyp = self.get_untagged_hyp(tagged_hyp)\n",
    "        token_untagged_hyp = self.tokenizer.encode(untagged_hyp)\n",
    "        return token_untagged_hyp\n",
    "    \n",
    "    def get_pos_start_tag(self, tagged_hyp):\n",
    "        token_untagged_hyp = self.get_untagged_token(tagged_hyp) \n",
    "        token_tagged_hyp = self.tokenizer.encode(tagged_hyp)\n",
    "        for idx, t_tagged in enumerate(token_tagged_hyp):\n",
    "            if t_tagged == token_untagged_hyp[idx]:\n",
    "                continue\n",
    "            else:\n",
    "                return idx\n",
    "        \n",
    "    def get_pos_end_tag(self, tagged_hyp):\n",
    "        token_untagged_hyp = self.get_untagged_token(tagged_hyp)[::-1]\n",
    "        token_tagged_hyp = self.tokenizer.encode(tagged_hyp)[::-1]\n",
    "        for idx, t_tagged in enumerate(token_tagged_hyp):\n",
    "            if t_tagged == token_untagged_hyp[idx]:\n",
    "                continue\n",
    "            else:\n",
    "                return len(token_untagged_hyp)-idx\n",
    "\n",
    "    def get_maj_hyp(self, dic_obj):\n",
    "        most_num = 0\n",
    "        maj_hyp = ''\n",
    "        for hyp, num in dic_obj.items():\n",
    "            if num > most_num:\n",
    "                most_num = num\n",
    "                maj_hyp = hyp\n",
    "        return hyp\n",
    "\n",
    "    def check_maj_num(self, dic_obj):\n",
    "        count = 0\n",
    "        most_num = 0\n",
    "        for num in dic_obj.values():\n",
    "            if num > most_num:\n",
    "                most_num = num\n",
    "        for num in dic_obj.values():\n",
    "            if num == most_num:\n",
    "                count += 1\n",
    "                if count > 1:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def get_except_kyes(self, MQM_dic):\n",
    "        bad_index = []\n",
    "        correct_hyps = {}\n",
    "        except_data = []\n",
    "        except_data2 = []\n",
    "        for i, key in enumerate(MQM_dic.keys()):\n",
    "            ok_rater_index = []\n",
    "            majority_hyp_dic = {}\n",
    "            mj = {}\n",
    "            for k, rater_dic in enumerate(MQM_dic[key]):\n",
    "                tagged_hyps = rater_dic['target']\n",
    "                untagged_hyps = [hyp.replace('<v>', '').replace('</v>', '') for hyp in tagged_hyps]\n",
    "                for uh in untagged_hyps:\n",
    "                    if uh not in mj:\n",
    "                        mj[uh] = 0\n",
    "                    mj[uh] += 1\n",
    "                ok_hyps = []\n",
    "                maj_hyps = {}\n",
    "                for tag_hyp, untag_hyp, sev in zip(tagged_hyps, untagged_hyps, rater_dic['severity']):\n",
    "                    if untag_hyp not in maj_hyps:\n",
    "                        maj_hyps[untag_hyp] = 1\n",
    "                    else:\n",
    "                        maj_hyps[untag_hyp] += 1\n",
    "                    if sev not in ['no-error', 'No-error', 'Neutral'] and tag_hyp == untag_hyp:\n",
    "                        bad_index.append({'key':key, 'index':k})\n",
    "                    else:\n",
    "                        ok_hyps.append(untag_hyp)\n",
    "                if len(ok_hyps) == 0:\n",
    "                    continue\n",
    "                if any([ok_hyps[0] != ok_hyps[j] for j in range(len(ok_hyps))]):\n",
    "                    bad_index.append({'key':key, 'index':k})\n",
    "                    h = get_maj_hyp(maj_hyps)\n",
    "                    if h not in majority_hyp_dic:\n",
    "                        majority_hyp_dic[h] = 1\n",
    "                    else:\n",
    "                        majority_hyp_dic[h] += 1\n",
    "                else:\n",
    "                    ok_rater_index.append({'key':key, 'index':k, 'untagged_hyp':ok_hyps[0]})\n",
    "                    if ok_hyps[0] not in majority_hyp_dic:\n",
    "                        majority_hyp_dic[ok_hyps[0]] = 1\n",
    "                    else:\n",
    "                        majority_hyp_dic[ok_hyps[0]] += 1\n",
    "            if any([ok_rater_index[0]['untagged_hyp'] != ok_rater_index[j]['untagged_hyp'] for j in range(len(ok_rater_index))]):\n",
    "                if check_maj_num(majority_hyp_dic):\n",
    "                    correct_hyps[key] = get_maj_hyp(majority_hyp_dic)\n",
    "                else:\n",
    "                    except_data.append(key)\n",
    "            else:\n",
    "                correct_hyps[key] = ok_rater_index[0]['untagged_hyp']\n",
    "\n",
    "            if not check_maj_num(mj):\n",
    "                except_data2.append(key)\n",
    "        return except_data, correct_hyps\n",
    "\n",
    "        def get_score_token(self, target, category, severity):\n",
    "            score = self.get_score(category, severity)\n",
    "            tagged_hyp = self.check_exception_hyp(target)\n",
    "            untagged_hyp = tagged_hyp.replace('<v>', '').replace('</v>', '')\n",
    "            untagged_token = self.get_untagged_token(tagged_hyp)\n",
    "            untagged_score = self.no_error_score if self.emb_label else 0.0\n",
    "            if severity == 'no-error' or severity == 'No-error' or severity == 'Neutral':\n",
    "                scored_token = [untagged_score for t in self.tokenizer.encode(tagged_hyp)]\n",
    "                return scored_token, untagged_token\n",
    "            else: \n",
    "                start_idx = self.get_pos_start_tag(tagged_hyp)\n",
    "                end_idx = self.get_pos_end_tag(tagged_hyp)\n",
    "                scored_token = []    \n",
    "                for i in range(len(untagged_token)):\n",
    "                    try:\n",
    "                        if i < start_idx:\n",
    "                            scored_token.append(untagged_score)\n",
    "                        elif i <= end_idx:\n",
    "                            scored_token.append(score)\n",
    "                        else:\n",
    "                            scored_token.append(untagged_score)\n",
    "                    except:\n",
    "                        from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "                return scored_token, untagged_token\n",
    "\n",
    "        def get_true_hyp(self, target_ls, severity_ls, category_ls):\n",
    "            untagged_ls = [t.replace('<v>', '').replace('</v>', '') for t in target_ls]\n",
    "            ok_target_ls = []\n",
    "            ok_untagged_ls = []\n",
    "            bad_ones = []\n",
    "            for i, (tgt, untag, sev, cat) in enumerate(zip(target_ls, untagged_ls, severity_ls, category_ls)):\n",
    "                if tgt == untag and sev not in ['no-error', 'No-error', 'Neutral']:\n",
    "                    bad_ones.append({'target':tgt, 'severity':sev, 'category':cat, 'index':i})\n",
    "                else:\n",
    "                    ok_target_ls.append(tgt)\n",
    "                    ok_untagged_ls.append(untag)\n",
    "            if len(bad_ones) == 0:\n",
    "                return target_ls, severity_ls, category_ls\n",
    "\n",
    "            if len(ok_untagged_ls) == 0:\n",
    "                from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "            else:\n",
    "                if not all([ok_untagged_ls[0] == ok_untagged_ls[i] for i in range(len(ok_untagged_ls))]):\n",
    "                    from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "                else:\n",
    "                    untagged = ok_untagged_ls[0]\n",
    "                    untagged_words = untagged.split(' |.')\n",
    "                    for bad_one in bad_ones:\n",
    "                        bad_hyp = bad_one['target']\n",
    "                        index = bad_one['index']\n",
    "                        bad_hyp_words = bad_hyp.split(' |.')\n",
    "                        missing_words = []\n",
    "                        missing_ids = []\n",
    "                        for i, untag_word in enumerate(untagged_words):\n",
    "                            if untag_word not in bad_hyp_words:\n",
    "                                missing_words.append(untag_word)\n",
    "                                missing_ids.append(i)\n",
    "                        missing_words[0] = '<v>'+missing_words[0]\n",
    "                        missing_words[-1] = missing_words[-1] + '</v>'\n",
    "                        missing_phrase = ' '.join(missing_words)\n",
    "                        correct_hyp_ls = []\n",
    "                        count = 0\n",
    "                        for untag_word in untagged_words:\n",
    "                            if untag_word in bad_hyp_words:\n",
    "                                correct_hyp_ls.append(untag_word)\n",
    "                            else:\n",
    "                                correct_hyp_ls.append(missing_words[count])\n",
    "                                count += 1\n",
    "                        correct_hyp = ' '.join(correct_hyp_ls)\n",
    "                        if correct_hyp.replace('<v>', '').replace('</v>', '') != untagged:\n",
    "                            from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "                        else:\n",
    "                            target_ls[index] = correct_hyp\n",
    "                    return target_ls, severity_ls, category_ls\n",
    "                    \n",
    "    def get_MQM_data(self, lang):\n",
    "        MQM_data = {}\n",
    "        MQM_dic = {}\n",
    "        fname = self.get_MQM_fname(MQM_tag_20_files, lang)\n",
    "        rlines = self.load_file(fname)\n",
    "        for i, line in enumerate(rlines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            line = line.rstrip()\n",
    "            split_line = line.split('\\t')\n",
    "            system = split_line[0]\n",
    "            doc = split_line[1]\n",
    "            doc_id = split_line[2]\n",
    "            seg_id = int(split_line[3])\n",
    "            rater = split_line[4]\n",
    "            source = split_line[5]\n",
    "            target = split_line[6]\n",
    "            category = split_line[7]\n",
    "            severity = split_line[8]\n",
    "            score = self.get_score(category, severity)\n",
    "            # score_token, target_token = self.get_score_token(target, category, severity)\n",
    "            key = (lang, system, seg_id)\n",
    "            if key in MQM_dic:\n",
    "                flag = False\n",
    "                for d in MQM_dic[key]:\n",
    "                    if d['rater'] == rater:\n",
    "                        d['severity'].append(severity)\n",
    "                        d['category'].append(category)\n",
    "                        d['score'].append(score)\n",
    "                        d['target'].append(target)\n",
    "                        flag = True\n",
    "                if flag == False:\n",
    "                    MQM_dic[key].append({'system':system, 'seg_id':seg_id, \n",
    "                                         'lang': lang,\n",
    "                                         'rater':rater, 'source':source,\n",
    "                                         'target':[target],\n",
    "                                         'category':[category],\n",
    "                                         'severity':[severity],\n",
    "                                         'score':[score]})\n",
    "            else:\n",
    "                MQM_dic[key] = [{'system':system, 'seg_id':seg_id,\n",
    "                                 'lang': lang,\n",
    "                                 'rater':rater, 'source':source,\n",
    "                                 'target':[target],\n",
    "                                 'category':[category],\n",
    "                                 'severity':[severity],\n",
    "                                 'score':[score]}]\n",
    "        # return _, MQM_dic      \n",
    "        for key in MQM_dic.keys():\n",
    "            \n",
    "            lang = key[0]\n",
    "            system = key[1]\n",
    "            seg_id = key[2]\n",
    "            score = np.mean([sum(a['score']) for a in MQM_dic[key]])\n",
    "            rater = []\n",
    "            source = []\n",
    "            target = []\n",
    "            category = []\n",
    "            severity = []\n",
    "            target_token = []\n",
    "            score_token = []\n",
    "            for md in MQM_dic[key]:\n",
    "                for tgt, cat, sev in zip(md['target'], md['category'], md['severity']):\n",
    "                    rater.append(md['rater'])\n",
    "                    source.append(md['source'])\n",
    "                    target.append(tgt)\n",
    "                    category.append(cat)\n",
    "                    severity.append(sev)\n",
    "            # target, severity, category = self.get_true_hyp(target, severity, category, rater)\n",
    "            # for mdic in MQM_dic[key]:\n",
    "            #     for t, s in zip(mdic['target_token'], mdic['score_token']):\n",
    "            #         target_token.append(t)\n",
    "            #         score_token.append(s)\n",
    "            MQM_data[key] = {'system':system, 'seg_id':int(seg_id), 'lang':lang,\n",
    "                             'rater':rater, 'source':source,\n",
    "                             'target':target, 'target_token':target_token,\n",
    "                             'score_token':score_token,\n",
    "                             'category':category, 'severity':severity,\n",
    "                             'score':score}\n",
    "            \n",
    "        return MQM_data, MQM_dic\n",
    "    \n",
    "    def get_google_MQM_data(self):\n",
    "        google_MQM_data = []\n",
    "        for lang in self.get_langs():\n",
    "            fname = self.get_MQM_fname(MQM_avg_20_files, lang)\n",
    "            rlines = self.load_file(fname)\n",
    "            avg_data = []\n",
    "            for i, line in enumerate(rlines):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                line = line.rstrip()\n",
    "                split_line = line.split(' ')\n",
    "                system = split_line[0]\n",
    "                score = split_line[1]\n",
    "                seg_id = split_line[2]\n",
    "                avg_data.append({'lang':lang,\n",
    "                                 'system':system,\n",
    "                                 'score':float(score)*(-1.0),\n",
    "                                 'seg_id':int(seg_id)})\n",
    "            google_MQM_data.extend(avg_data)\n",
    "        return google_MQM_data\n",
    "        \n",
    "    def make_MQM(self):\n",
    "        langs = self.get_langs()\n",
    "        all_MQM_data = []\n",
    "        for lang in tqdm(langs):\n",
    "            MQM_data = self.get_MQM_data(lang)\n",
    "            return MQM_data\n",
    "            all_MQM_data.extend(MQM_data)\n",
    "        return all_MQM_data\n",
    "    \n",
    "    def check_no_tag(self):\n",
    "        langs = self.get_langs()\n",
    "        no_tags = {}\n",
    "        for lang in langs:\n",
    "            no_tags[lang] = []\n",
    "            fname = self.get_MQM_fname(MQM_tag_20_files, lang)\n",
    "            rlines = self.load_file(fname)\n",
    "            for i, line in enumerate(rlines):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                line = line.rstrip()\n",
    "                split_line = line.split('\\t')\n",
    "                system = split_line[0]\n",
    "                doc = split_line[1]\n",
    "                doc_id = split_line[2]\n",
    "                seg_id = int(split_line[3])\n",
    "                rater = split_line[4]\n",
    "                source = split_line[5]\n",
    "                target = split_line[6]\n",
    "                category = split_line[7]\n",
    "                severity = split_line[8]\n",
    "                untagged_hyp = target.replace('<v>', '').replace('</v>', '')\n",
    "                if untagged_hyp == target and severity not in ['no-error', 'No-error', 'Neutral']:\n",
    "                    no_tags[lang].append({'hyp':target, 'severity':severity,\n",
    "                                          'category':category, 'rater':rater})\n",
    "        return no_tags\n",
    "                \n",
    "importer = MQM_importer20('/ahc/work3/kosuke-t/WMT/wmt20_test.json',\n",
    "                          google_avg=True, my_avg=False, emb_label=False, \n",
    "                          emb_only_sev=False, google_score=True, split_dev=True)\n",
    "MQM_data, MQM_dic = importer.make_MQM()\n",
    "# no_tags = importer.check_no_tag()\n",
    "print('loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maj_hyp(dic_obj):\n",
    "    most_num = 0\n",
    "    maj_hyp = ''\n",
    "    for hyp, num in dic_obj.items():\n",
    "        if num > most_num:\n",
    "            most_num = num\n",
    "            maj_hyp = hyp\n",
    "    return hyp\n",
    "\n",
    "def check_maj_num(dic_obj):\n",
    "    count = 0\n",
    "    most_num = 0\n",
    "    for num in dic_obj.values():\n",
    "        if num > most_num:\n",
    "            most_num = num\n",
    "    for num in dic_obj.values():\n",
    "        if num == most_num:\n",
    "            count += 1\n",
    "            if count > 1:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def get_except_kyes(MQM_dic):\n",
    "    bad_index = []\n",
    "    correct_hyps = {}\n",
    "    need_fix_keys = []\n",
    "    except_data = []\n",
    "    except_data2 = []\n",
    "    for i, key in enumerate(MQM_dic.keys()):\n",
    "        ok_rater_index = []\n",
    "        majority_hyp_dic = {}\n",
    "        mj = {}\n",
    "        untaggs = []\n",
    "        for k, rater_dic in enumerate(MQM_dic[key]):\n",
    "            tagged_hyps = rater_dic['target']\n",
    "            untagged_hyps = [hyp.replace('<v>', '').replace('</v>', '') for hyp in tagged_hyps]\n",
    "            untaggs.extend(untagged_hyps)\n",
    "            for uh in untagged_hyps:\n",
    "                if uh not in mj:\n",
    "                    mj[uh] = 0\n",
    "                mj[uh] += 1\n",
    "            ok_hyps = []\n",
    "            maj_hyps = {}\n",
    "            for tag_hyp, untag_hyp, sev in zip(tagged_hyps, untagged_hyps, rater_dic['severity']):\n",
    "                if untag_hyp not in maj_hyps:\n",
    "                    maj_hyps[untag_hyp] = 1\n",
    "                else:\n",
    "                    maj_hyps[untag_hyp] += 1\n",
    "                if sev not in ['no-error', 'No-error', 'Neutral'] and tag_hyp == untag_hyp:\n",
    "                    bad_index.append({'key':key, 'index':k})\n",
    "                else:\n",
    "                    ok_hyps.append(untag_hyp)\n",
    "            if len(ok_hyps) == 0:\n",
    "                continue\n",
    "            if any([ok_hyps[0] != ok_hyps[j] for j in range(len(ok_hyps))]):\n",
    "                bad_index.append({'key':key, 'index':k})\n",
    "                h = get_maj_hyp(maj_hyps)\n",
    "                if h not in majority_hyp_dic:\n",
    "                    majority_hyp_dic[h] = 1\n",
    "                else:\n",
    "                    majority_hyp_dic[h] += 1\n",
    "            else:\n",
    "                ok_rater_index.append({'key':key, 'index':k, 'untagged_hyp':ok_hyps[0]})\n",
    "                if ok_hyps[0] not in majority_hyp_dic:\n",
    "                    majority_hyp_dic[ok_hyps[0]] = 1\n",
    "                else:\n",
    "                    majority_hyp_dic[ok_hyps[0]] += 1\n",
    "        if any([ok_rater_index[0]['untagged_hyp'] != ok_rater_index[j]['untagged_hyp'] for j in range(len(ok_rater_index))]):\n",
    "            if check_maj_num(majority_hyp_dic):\n",
    "                correct_hyps[key] = get_maj_hyp(majority_hyp_dic)\n",
    "            else:\n",
    "                except_data.append(key)\n",
    "        else:\n",
    "            correct_hyps[key] = ok_rater_index[0]['untagged_hyp']\n",
    "\n",
    "        if not check_maj_num(mj):\n",
    "            except_data2.append(key)\n",
    "        if any([untaggs[0] != untaggs[i] for i in range(len(untaggs))]):\n",
    "            need_fix_keys.append(key)\n",
    "    return except_data, correct_hyps, need_fix_keys\n",
    "\n",
    "def fix_hyps(MQM_dic, except_data, correct_hyps):\n",
    "    pass\n",
    "\n",
    "except_data, correct_hyps, need_fix_keys = get_except_kyes(MQM_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "key = need_fix_keys[0]\n",
    "\n",
    "targets = MQM_data[key]['target']\n",
    "correct_untag_words = correct_hyps[key].split()\n",
    "fixed_targets = []\n",
    "# def check_tag_continuous(diff_words):\n",
    "#     b_index = -2\n",
    "#     i_index = -2\n",
    "#     e_index = -2\n",
    "#     for idx, w in enumerate(diff_words):\n",
    "#         if w.startswith('+'):\n",
    "#             if b_index == -2:\n",
    "#                 b_index = idx\n",
    "#             elif e_index != -2:\n",
    "#                 return False\n",
    "#             else:\n",
    "#                 i_index = idx \n",
    "#         else:\n",
    "#             if b_index != -2 and e_index == -2:\n",
    "#                 e_index = idx\n",
    "#     return True\n",
    "\n",
    "# def get_tag_index(diff_words):\n",
    "#     tag_start_index = -2\n",
    "#     tag_end_index = -2\n",
    "#     for idx, w in enumerate(diff_words):\n",
    "#         if w.startswith('+'):\n",
    "#             if tag_start_index == -2:\n",
    "#                 tag_start_index = idx\n",
    "#             tag_end_index = idx\n",
    "#         elif tag_start_index != -2 and tag_end_index == -2:\n",
    "#             tag_end_index = idx - 1\n",
    "#     return tag_start_index, tag_end_index\n",
    "\n",
    "# for target in targets:\n",
    "#     untagged_target_words = target.replace('<v>', '').replace('</v>', '').split()\n",
    "#     diff_words = list(difflib.ndiff(untagged_target_words, correct_untag_words))\n",
    "#     diff_words = [dw[0]+dw[2:] for dw in diff_words if not dw.startswith('-')]\n",
    "#     # assert check_tag_continuous(diff_words)\n",
    "#     fixed_target = ''\n",
    "#     tag_start_index, tag_end_index = get_tag_index(diff_words)\n",
    "#     for i, w in enumerate(diff_words):\n",
    "#         if i == tag_start_index and i == tag_end_index:\n",
    "#             fixed_target += '<v>' + w[1:] + '</v> '\n",
    "#         elif i == tag_start_index:\n",
    "#             fixed_target += '<v>' + w[1:] + ' '\n",
    "#         elif i == tag_end_index:\n",
    "#             fixed_target += w[1:] + '</v> '\n",
    "#         else:\n",
    "#             fixed_target += w[1:] + ' '\n",
    "#     fixed_target = fixed_target[:-1]\n",
    "#     fixed_targets.append(fixed_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Michael Jacksons ehemaliger Bodyguard hat behauptet, der verstorbene Snger habe einige seiner exzentrischen Eigenschaften mit wohl durchdachter Absicht kultiviert, die Aufmerksamkeit der Medien auf sich zu lenken.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_hyps[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Jacksons ehemaliger Bodyguard hat behauptet, der verstorbene Snger habe einige seiner exzentrischen Eigenschaften mit der Absicht kultiviert, die Aufmerksamkeit der Medien auf sich zu lenken.\n",
      "Michael Jacksons ehemaliger Bodyguard hat behauptet, der verstorbene Snger habe einige seiner exzentrischen Eigenschaften mit <v>wohl durchdachter</v> Absicht kultiviert, die Aufmerksamkeit der Medien auf sich zu lenken.\n",
      "Michael Jacksons ehemaliger Bodyguard hat behauptet, der verstorbene Snger habe einige seiner exzentrischen Eigenschaften mit der Absicht kultiviert, die Aufmerksamkeit der Medien auf sich zu lenken.\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])\n",
    "print(targets[1])\n",
    "print(targets[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Jacksons ehemaliger Bodyguard hat behauptet, der verstorbene Snger habe einige seiner exzentrischen Eigenschaften mit <v>wohl durchdachter</v> Absicht kultiviert, die Aufmerksamkeit der Medien auf sich zu lenken.\n",
      "Michael Jacksons ehemaliger Bodyguard hat behauptet, der verstorbene Snger habe einige seiner exzentrischen Eigenschaften mit wohl durchdachter Absicht kultiviert, die Aufmerksamkeit der Medien auf sich zu lenken.\n",
      "Michael Jacksons ehemaliger Bodyguard hat behauptet, der verstorbene Snger habe einige seiner exzentrischen Eigenschaften mit <v>wohl durchdachter</v> Absicht kultiviert, die Aufmerksamkeit der Medien auf sich zu lenken.\n"
     ]
    }
   ],
   "source": [
    "print(fixed_targets[0])\n",
    "print(fixed_targets[1])\n",
    "print(fixed_targets[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  Michael',\n",
       " '  Jacksons',\n",
       " '  ehemaliger',\n",
       " '  Bodyguard',\n",
       " '  hat',\n",
       " '  behauptet,',\n",
       " '  der',\n",
       " '  verstorbene',\n",
       " '  Snger',\n",
       " '  habe',\n",
       " '  einige',\n",
       " '  seiner',\n",
       " '  exzentrischen',\n",
       " '  Eigenschaften',\n",
       " '  mit',\n",
       " '- der',\n",
       " '+ wohl',\n",
       " '+ durchdachter',\n",
       " '  Absicht',\n",
       " '  kultiviert,',\n",
       " '  die',\n",
       " '  Aufmerksamkeit',\n",
       " '  der',\n",
       " '  Medien',\n",
       " '  auf',\n",
       " '  sich',\n",
       " '  zu',\n",
       " '  lenken.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(difflib.ndiff(target_words, correct_untag_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st1 = 'Michael Jackson trug Klebeband auf der Nase, um Titelseiten zu <v>bekommen,</v> behauptet ehemaliger Bodyguard'\n",
    "st2 = 'Michael Jackson trug Klebeband auf der Nase, um Titelseiten zu <v>bekommen,</v> behauptet ehemaliger Bodyguard'\n",
    "st1.replace('<v>', '').replace('</v>', '') == st2.replace('<v>', '').replace('</v>', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n",
      "Eine Bste des Sozialreformers wurde whrend einer Roadshow des BJP-Chefs Amit Shah in der Stadt im Mai zerstrt.\n",
      "Eine Bste des Sozialreformers wurde whrend einer Roadshow des BJP-Chefs Amit Shah in der Stadt im Mai zerstrt.\n",
      "Eine Bste des Sozialreformers wurde whrend einer Roadshow des BJP-Chefs Amit Shah in der Stadt im Mai zerstrt.\n"
     ]
    }
   ],
   "source": [
    "def remove_tag(string):\n",
    "    return string.replace('<v>', '').replace('</v>', '')\n",
    "hyp1 = 'Eine Bste des Sozialreformers wurde whrend einer <v>Roadshow</v> des BJP-Chefs Amit Shah in der Stadt im Mai zerstrt.'\n",
    "hyp2 = 'Eine Bste des Sozialreformers wurde whrend einer <v>Roadshow </v>des BJP-Chefs Amit Shah in der Stadt im Mai zerstrt.'\n",
    "hyp3 = 'Eine Bste des Sozialreformers wurde whrend einer <v>Roadshow</v> des BJP-Chefs Amit Shah in der Stadt im Mai zerstrt.'\n",
    "untag_hyp1 = remove_tag(hyp1)\n",
    "untag_hyp2 = remove_tag(hyp2)\n",
    "untag_hyp3= remove_tag(hyp3)\n",
    "print(untag_hyp1 == untag_hyp2, untag_hyp1 == untag_hyp3, untag_hyp2 == untag_hyp3)\n",
    "print(untag_hyp1)\n",
    "print(untag_hyp2)\n",
    "print(untag_hyp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ehemaliger Bodyguard berichtet: Michael Jackson trug Klebeband auf seiner Nase, um auf die Titelseiten zu kommen'],\n",
       " ['Ehemaliger Bodyguard berichtet: Michael Jackson trug Klebeband auf seiner Nase, um auf die Titelseiten zu kommen'],\n",
       " ['Ehemaliger Bodyguard berichtet: Michael Jackson trug Klebeband auf seiner Nase, um auf die Titelseiten zu kommen']]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data[1]['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 137, 17048, 78481, 1331, 228357, 2988, 27, 542, 168, 139379, 2731, 4, 68, 1439, 6, 69431, 13286, 46, 53516, 46015, 9, 1456, 77613, 58669, 155302, 19, 4, 909, 39505, 15386, 20951, 12783, 165, 99, 27417, 916, 776, 112, 15152, 175203, 108197, 5, 155, 2], [0, 137, 17048, 78481, 1331, 228357, 2988, 27, 542, 168, 139379, 2731, 4, 68, 1439, 6, 69431, 13286, 46, 53516, 46015, 9, 1456, 77613, 58669, 155302, 19, 4, 909, 39505, 15386, 20951, 12783, 165, 99, 27417, 916, 776, 112, 15152, 175203, 108197, 5, 155, 2], [0, 137, 17048, 78481, 1331, 228357, 2988, 27, 542, 168, 139379, 2731, 4, 68, 1439, 6, 69431, 13286, 46, 53516, 46015, 9, 1456, 77613, 58669, 155302, 19, 4, 909, 39505, 15386, 20951, 12783, 165, 99, 27417, 916, 776, 112, 15152, 175203, 108197, 5, 155, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(my_data[0]['target_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 14180/14180 [12:33<00:00, 18.81it/s]\n",
      "100%|| 20000/20000 [23:23<00:00, 14.25it/s]\n"
     ]
    }
   ],
   "source": [
    "for lang in ggl_dic.keys():\n",
    "    for g in tqdm(ggl_dic[lang]):\n",
    "        g_sys = g['system']\n",
    "        g_seg_id = g['seg_id']\n",
    "        g_score = g['score']\n",
    "        found_flag = False\n",
    "        closest_sys = ''\n",
    "        min_lv = 1000\n",
    "        for m in my_dic[lang]:\n",
    "            m_sys = m['system']\n",
    "            m_seg_id = m['seg_id']\n",
    "            m_score = m['score']\n",
    "            m_score = float(Decimal(str(m_score)).quantize(Decimal('0.111111'), rounding=ROUND_HALF_UP))\n",
    "            if g_sys == m_sys and g_seg_id == m_seg_id:\n",
    "                found_flag = True\n",
    "                if g_score == m_score:\n",
    "                    continue\n",
    "                else:         \n",
    "                    print('Scores don\\'t match! -> {}, {}, {}, {}'.format(g_sys, g_seg_id, g_score, m_score))\n",
    "            else:\n",
    "                lv = levenshtein(g_sys, m_sys)\n",
    "                if min_lv > lv:\n",
    "                    min_lv = lv\n",
    "                    closest_sys = m_sys\n",
    "        if found_flag == False:\n",
    "            for m in my_data:\n",
    "                if closest_sys == m['system']:\n",
    "                    if g_seg_id == m['seg_id']:\n",
    "                        m_score = m['score']\n",
    "                        m_score = float(Decimal(str(m_score)).quantize(Decimal('0.111111'), rounding=ROUND_HALF_UP))\n",
    "                        print('No sys seg_id match! -> {}, {}, {}, {}, {}, {}'.format(lang,\n",
    "                                                                                     g_sys, \n",
    "                                                                                     closest_sys,\n",
    "                                                                                     g_seg_id,\n",
    "                                                                                     g_score,\n",
    "                                                                                     m_score))\n",
    "\n",
    "                        from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading and processing wmt data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en-de': 14180, 'zh-en': 20000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving clean file.\n",
      "Cleaning up old ratings file.\n",
      "Reading and processing wmt data...\n",
      "Saving clean file.\n",
      "Cleaning up old ratings file.\n",
      "Reading and processing wmt data...\n",
      "Saving clean file.\n",
      "Cleaning up old ratings file.\n",
      "Reading and processing wmt data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en-de': 35658, 'zh-en': 66396}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving clean file.\n",
      "Cleaning up old ratings file.\n",
      "Reading and processing wmt data...\n",
      "Saving clean file.\n",
      "Cleaning up old ratings file.\n",
      "Reading and processing wmt data...\n",
      "Saving clean file.\n",
      "Cleaning up old ratings file.\n",
      "Reading and processing wmt data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en-de': 35658, 'zh-en': 66396}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving clean file.\n",
      "Cleaning up old ratings file.\n",
      "Reading and processing wmt data...\n",
      "Saving clean file.\n",
      "Cleaning up old ratings file.\n",
      "Reading and processing wmt data...\n",
      "Saving clean file.\n",
      "Cleaning up old ratings file.\n",
      "Reading and processing wmt data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en-de': 35658, 'zh-en': 66396}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving clean file.\n",
      "Cleaning up old ratings file.\n",
      "Reading and processing wmt data...\n",
      "Saving clean file.\n",
      "Cleaning up old ratings file.\n",
      "Reading and processing wmt data...\n",
      "Saving clean file.\n",
      "Cleaning up old ratings file.\n",
      "Reading and processing wmt data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en-de': 35658, 'zh-en': 66396}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving clean file.\n",
      "Cleaning up old ratings file.\n",
      "Reading and processing wmt data...\n",
      "Saving clean file.\n",
      "Cleaning up old ratings file.\n",
      "Reading and processing wmt data...\n",
      "Saving clean file.\n",
      "Cleaning up old ratings file.\n"
     ]
    }
   ],
   "source": [
    "DATA_HOME = '/ahc/work3/kosuke-t'\n",
    "LANGWO_LETTERS = 'LANGWO'\n",
    "LANG_LETTERS = 'L_A_N_G'\n",
    "LANG1_LETTERS = 'LANG1'\n",
    "LANG2_LETTERS = 'LANG2'\n",
    "SYSTEM_LETTERS = 'SYSTEM'\n",
    "src_20_files = os.path.join(DATA_HOME, 'WMT/WMT20_data/txt/sources/newstest2020-{}-src.{}.txt'.format(LANGWO_LETTERS, LANG1_LETTERS))\n",
    "ref_20_files = os.path.join(DATA_HOME, 'WMT/WMT20_data/txt/references/newstest2020-{}-ref.{}.txt'.format(LANGWO_LETTERS, LANG2_LETTERS))\n",
    "hyp_20_files =  os.path.join(DATA_HOME, 'WMT/WMT20_data/txt/system-outputs/{}/newstest2020.{}.{}.txt'.format(LANG_LETTERS, LANG_LETTERS, SYSTEM_LETTERS))\n",
    "MQM_avg_20_files = os.path.join(DATA_HOME, 'WMT/wmt-mqm-human-evaluation/newstest2020/{}/mqm_newstest2020_{}.avg_seg_scores.tsv'.format(LANGWO_LETTERS, LANGWO_LETTERS))\n",
    "MQM_tag_20_files = os.path.join(DATA_HOME, 'WMT/wmt-mqm-human-evaluation/newstest2020/{}/mqm_newstest2020_{}.tsv'.format(LANGWO_LETTERS, LANGWO_LETTERS))\n",
    "\n",
    "\n",
    "MQM_1hot_only_severity_to_vec = {\"no-error\":np.asarray([1, 0, 0]), \n",
    "                                 \"No-error\":np.asarray([1, 0, 0]),\n",
    "                                 \"Neutral\":np.asarray([0, 1, 0]),\n",
    "                                 \"Minor\":np.asarray([0, 1, 0]),\n",
    "                                 \"Major\":np.asarray([0, 0, 1])}\n",
    "MQM_tag_list = [\"No-error\", \n",
    "                \"Neutral\",\n",
    "                \"Minor\",\n",
    "                \"Major\"]\n",
    "\n",
    "class MQM_importer20():\n",
    "    def __init__(self, save_dir, mqm_tag=False, avg=False, emb_label=False, emb_only_sev=True, no_error_score=np.asarray([1, 0, 0]),\n",
    "                 tokenizer_name='xlm-roberta-large', original_score=True, split_dev=False, dev_ratio=0.1):\n",
    "        self.save_dir = save_dir\n",
    "        self.save_tmp = save_dir + '_tmp'\n",
    "        self.year = '20'\n",
    "        self.mqm_tag = mqm_tag\n",
    "        self.avg = avg\n",
    "        self.emb_label = emb_label\n",
    "        self.emb_only_sev = emb_only_sev\n",
    "        self.no_error_score = no_error_score\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        self.original_score = original_score\n",
    "        self.split_dev = split_dev\n",
    "        self.dev_ratio = dev_ratio\n",
    "        if not mqm_tag:\n",
    "            self.avg = True\n",
    "        \n",
    "    def parse_eval_file_name(self, fname):\n",
    "        \"\"\"Extracts language pairs from the names of human rating files.\"\"\"\n",
    "        wmt_pattern = re.compile(r\"^ad-seg-scores-([a-z]{2}-[a-z]{2})\\.csv\")\n",
    "        match = re.match(wmt_pattern, fname)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def list_lang_pairs(self):\n",
    "        \"\"\"List all language pairs included in the WMT files for 2018.\"\"\"\n",
    "        return ['en-de', 'zh-en']\n",
    "\n",
    "    def get_src_ref(self, lang):\n",
    "        src_lang = lang[:2]\n",
    "        tgt_lang = lang[-2:]\n",
    "        \n",
    "        src_file = src_20_files.replace(LANGWO_LETTERS, lang.replace('-', '')).replace(LANG1_LETTERS, src_lang)\n",
    "        ref_file = ref_20_files.replace(LANGWO_LETTERS, lang.replace('-', '')).replace(LANG2_LETTERS, tgt_lang)\n",
    "        \n",
    "        with open(src_file, \"r\", encoding=\"utf-8\") as f_src:\n",
    "            src_segments = f_src.readlines()\n",
    "        with open(ref_file, \"r\", encoding=\"utf-8\") as f_ref:\n",
    "            ref_segments = f_ref.readlines()\n",
    "\n",
    "        src_segments = [s.strip() for s in src_segments]\n",
    "        ref_segments = [s.strip() for s in ref_segments]\n",
    "\n",
    "        return src_segments, ref_segments\n",
    "\n",
    "    def to_json(self, year, lang, src, ref, ref_id, hyp, tokens_score, mean_score, seg_id, sys_name,\n",
    "                variance, category, severity):\n",
    "        json_dict = {\"year\": year, \"lang\": lang, \"source\": src, \n",
    "                     \"reference\": ref, \"ref_id\":ref_id, \"candidate\": hyp,\n",
    "                     \"score\": tokens_score, \"mean_score\": mean_score, \"segment_id\": seg_id, \n",
    "                     \"system\": sys_name, \"variance\": variance, \n",
    "                     'category':category, 'severity':severity}\n",
    "        return json.dumps(json_dict)\n",
    "    \n",
    "    def to_dict_example(self, year, lang, src, ref, ref_id, hyp, tokens_score, mean_score, seg_id, sys_name,\n",
    "                        variance, category, severity):\n",
    "        json_dict = {\"year\": year, \"lang\": lang, \"source\": src, \n",
    "                     \"reference\": ref, \"ref_id\":ref_id, \"candidate\": hyp,\n",
    "                     \"score\": tokens_score, \"mean_score\": mean_score, \"segment_id\": seg_id, \n",
    "                     \"system\": sys_name, \"variance\": variance,\n",
    "                     'category':category, 'severity':severity}\n",
    "        return json_dict\n",
    "    \n",
    "    def get_sys(self, lang):\n",
    "        sys_segments = {}\n",
    "        src_lang = lang[:2]\n",
    "        tgt_lang = lang[-2:]\n",
    "        folder = '/home/is/kosuke-t/project_disc/WMT/WMT20_data/txt/system-outputs'\n",
    "        paths = os.path.join(folder, lang, \"newstest2020.{}.*.txt\".format(lang))\n",
    "        all_files = glob.glob(paths)\n",
    "        for fname in all_files:\n",
    "            fname_base = os.path.basename(fname)\n",
    "            sys_name = re.sub(r'newstest2020\\.{}.|\\.txt'.format(lang), '',fname_base)\n",
    "            with open(fname, mode='r', encoding='utf-8') as r:\n",
    "                data = r.readlines()\n",
    "            data = [d.strip() for d in data]\n",
    "            sys_segments[sys_name] = data\n",
    "        return sys_segments\n",
    "    \n",
    "    def parse_rating(self, rating_line):\n",
    "        if self.mqm_tag:\n",
    "            rating_tuple = tuple(rating_line.split(\"\\t\"))\n",
    "            if len(rating_tuple) != 9:\n",
    "                if len(rating_tuple) == 10 and rating_tuple[7] == 'Other':\n",
    "                    sys_name = rating_tuple[0]\n",
    "                    doc = rating_tuple[1]\n",
    "                    doc_id = rating_tuple[2]\n",
    "                    seg_id = rating_tuple[3]\n",
    "                    rater = rating_tuple[4]\n",
    "                    source = rating_tuple[5]\n",
    "                    target = rating_tuple[6]\n",
    "                    category = rating_tuple[8]\n",
    "                    severity = 'Other/' + rating_tuple[9]\n",
    "                else:\n",
    "                    print(rating_tuple[7])\n",
    "                    print(len(rating_tuple))\n",
    "                    print(rating_tuple)\n",
    "                    print(rating_line)\n",
    "            else:\n",
    "                sys_name, doc, doc_id, seg_id, rater, source, target, category, severity = rating_tuple\n",
    "            severity = severity.rstrip()\n",
    "            return sys_name, seg_id, rater, source, target, category, severity \n",
    "        else:\n",
    "            rating_tuple = tuple(rating_line.split(\" \"))\n",
    "            if len(rating_tuple) != 3:\n",
    "                if len(rating_tuple) != 2:\n",
    "                    logger.info(\"len of rating_tuple = {}\".format(len(rating_tuple)))\n",
    "                    logger.info(rating_tuple)\n",
    "                else:\n",
    "                    if '\\t' in rating_tuple[0]:\n",
    "                        rating_tuple = (rating_tuple[0].split('\\t')[0], \n",
    "                                        rating_tuple[0].split('\\t')[1],\n",
    "                                        rating_tuple[1])\n",
    "                \n",
    "            sys_name, score, seg_id = rating_tuple\n",
    "            \n",
    "            try:\n",
    "                seg_id = int(seg_id)\n",
    "                if score == \"None\":\n",
    "                    score = None\n",
    "                else:\n",
    "                    score = float(score)\n",
    "            except:\n",
    "                print(rating_tuple)\n",
    "                print(rating_line)\n",
    "            \n",
    "            return sys_name, seg_id, score\n",
    "    \n",
    "    def get_MQM(self, lang):\n",
    "        lang_wo = lang.replace('-', '')\n",
    "        if not self.mqm_tag:\n",
    "            fname = os.path.join('/ahc/work3/kosuke-t/WMT/wmt-mqm-human-evaluation/newstest2020',\n",
    "                                 lang_wo, 'mqm_newstest2020_{}.avg_seg_scores.tsv'.format(lang_wo))\n",
    "        else:\n",
    "            fname = os.path.join('/ahc/work3/kosuke-t/WMT/wmt-mqm-human-evaluation/newstest2020',\n",
    "                             lang_wo, 'mqm_newstest2020_{}.tsv'.format(lang_wo))\n",
    "        with open(fname, mode='r', encoding='utf-8') as r:\n",
    "            data = r.readlines()\n",
    "        return data\n",
    "    \n",
    "    def organize_mqm_tag(self, MQM_data, src_segments):\n",
    "        rater_dic = {}\n",
    "        for line in MQM_data[1:]:\n",
    "            sys_name, seg_id, rater, source, target, category, severity = self.parse_rating(line)\n",
    "            if sys_name == 'Nemo':\n",
    "                sys_name = 'NVIDIA-NeMo'\n",
    "            elif sys_name in ['metricsystem1', 'metricsystem2', 'metricsystem3', 'metricsystem4', 'metricsystem5', \n",
    "                              'ref-A', 'ref-B', 'ref-C', 'ref-D']:\n",
    "                continue\n",
    "            seg_id = int(seg_id)\n",
    "            untagged_hyp = self.get_untagged_hyp(target)\n",
    "            key = (sys_name, seg_id, untagged_hyp)\n",
    "            if key not in rater_dic:\n",
    "                rater_dic[key] = {'rater':[], 'target':[], 'category':[], 'severity':[]}\n",
    "            rater_dic[key]['rater'].append(rater)\n",
    "            rater_dic[key]['target'].append(target)\n",
    "            rater_dic[key]['category'].append(category)\n",
    "            rater_dic[key]['severity'].append(severity)\n",
    "            rater_dic[key]['seg_id'] = seg_id\n",
    "            rater_dic[key]['sys_name'] = sys_name\n",
    "            rater_dic[key]['untagged_hyp'] = untagged_hyp\n",
    "            \n",
    "            if 'source' in rater_dic[key]:\n",
    "                continue\n",
    "            if source != src_segments[seg_id-1]:\n",
    "                min_lv_seg_id = 0\n",
    "                min_lv_value = 1000\n",
    "                for idx, s in enumerate(src_segments):\n",
    "                    lv_value = levenshtein(source, s)\n",
    "                    if lv_value < min_lv_value:\n",
    "                        min_lv_value = lv_value\n",
    "                        min_lv_seg_id = idx\n",
    "                if seg_id-1 != min_lv_seg_id:\n",
    "                    print(min_lv_value)\n",
    "                    print(min_lv_seg_id)\n",
    "                    print(source)\n",
    "                    print(src_segments[seg_id-1])\n",
    "                    print(src_segments[min_lv_seg_id])\n",
    "                    rater_dic[key]['source'] = src_segments[min_lv_seg_id]\n",
    "                else:\n",
    "                    rater_dic[key]['source'] = src_segments[min_lv_seg_id]\n",
    "            else:\n",
    "                rater_dic[key]['source'] = source\n",
    "        return rater_dic\n",
    "    \n",
    "    def get_mqm_tagged_score(self, category, severity, return_scaler=False):\n",
    "        if self.emb_label and not return_scaler:\n",
    "            if self.emb_only_sev:\n",
    "                if severity in MQM_1hot_only_severity_to_vec:\n",
    "                    return MQM_1hot_only_severity_to_vec[severity]\n",
    "                elif category in MQM_1hot_only_severity_to_vec:\n",
    "                    return MQM_1hot_only_severity_to_vec[category]\n",
    "                else:\n",
    "                    NotImplementedError\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            if self.original_score:\n",
    "                scores = [0.0, 0.1, 1.0, 5.0, 25.0]\n",
    "            else:\n",
    "                scores = [0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "\n",
    "            if severity == 'Major':\n",
    "                if category == 'Non-translation' or category  == 'Non-translation!':\n",
    "                    return scores[4]\n",
    "                else:\n",
    "                    return scores[3]\n",
    "            elif severity == 'Minor':\n",
    "                if category == 'Fluency/Punctuation':\n",
    "                    return scores[1]\n",
    "                else:\n",
    "                    return scores[2]\n",
    "            else:\n",
    "                if category == 'Major':\n",
    "                    if severity == 'Non-translation':\n",
    "                        return scores[4]\n",
    "                    else:\n",
    "                        return scores[3]\n",
    "                elif category == 'Minor':\n",
    "                    if severity == 'Fluency/Punctuation':\n",
    "                        return scores[1]\n",
    "                    else:\n",
    "                        return scores[2]\n",
    "                else:\n",
    "                    return scores[0]\n",
    "    \n",
    "    def get_untagged_hyp(self, tagged_hyp):\n",
    "        return tagged_hyp.replace('<v>', '').replace('</v>', '')\n",
    "    \n",
    "    def get_untagged_token(self, tagged_hyp):\n",
    "        # token_tagged_hyp = self.tokenizer.encode(tagged_hyp)\n",
    "        untagged_hyp = self.get_untagged_hyp(tagged_hyp)\n",
    "        token_untagged_hyp = self.tokenizer.encode(untagged_hyp)\n",
    "        return token_untagged_hyp\n",
    "    \n",
    "    def get_pos_start_tag(self, tagged_hyp):\n",
    "        token_untagged_hyp = self.get_untagged_token(tagged_hyp) \n",
    "        token_tagged_hyp = self.tokenizer.encode(tagged_hyp)\n",
    "        for idx, t_tagged in enumerate(token_tagged_hyp):\n",
    "            if t_tagged == token_untagged_hyp[idx]:\n",
    "                continue\n",
    "            else:\n",
    "                return idx\n",
    "        \n",
    "    def get_pos_end_tag(self, tagged_hyp):\n",
    "        # token_untagged_hyp = self.get_untagged_token(tagged_hyp)\n",
    "        # token_tagged_hyp = self.tokenizer.encode(tagged_hyp)\n",
    "        # start_tag_flag = False\n",
    "        # start_idx = self.get_pos_start_tag(tagged_hyp)\n",
    "        # inside_tag_flag = False\n",
    "        # untagged_index = start_idx\n",
    "        # for i, t_tagged in enumerate(token_tagged_hyp):\n",
    "        #     if i <= start_idx:\n",
    "        #         continue\n",
    "        #     elif t_tagged == token_untagged_hyp[untagged_index] and inside_tag_flag == False:\n",
    "        #         inside_tag_flag = True\n",
    "        #         untagged_index += 1 \n",
    "        #     elif t_tagged == token_untagged_hyp[untagged_index] and inside_tag_flag == True:\n",
    "        #         untagged_index += 1\n",
    "        #     elif t_tagged != token_untagged_hyp[untagged_index] and inside_tag_flag == True:\n",
    "        #         return i\n",
    "        token_untagged_hyp = self.get_untagged_token(tagged_hyp)[::-1]\n",
    "        token_tagged_hyp = self.tokenizer.encode(tagged_hyp)[::-1]\n",
    "        for idx, t_tagged in enumerate(token_tagged_hyp):\n",
    "            if t_tagged == token_untagged_hyp[idx]:\n",
    "                continue\n",
    "            else:\n",
    "                return len(token_untagged_hyp)-idx\n",
    "            \n",
    "    def get_scored_token(self, tagged_hyp, category, severity):\n",
    "        score = self.get_mqm_tagged_score(category, severity)\n",
    "        untagged_score = self.no_error_score if self.emb_label else 0.0\n",
    "        if tagged_hyp.replace('<v>', '').replace('</v>', '') == tagged_hyp:\n",
    "            return [untagged_score for t in self.tokenizer.encode(tagged_hyp)]\n",
    "        \n",
    "        untagged_token = self.get_untagged_token(tagged_hyp)\n",
    "        start_idx = self.get_pos_start_tag(tagged_hyp)\n",
    "        end_idx = self.get_pos_end_tag(tagged_hyp)\n",
    "        scored_token = []\n",
    "        for i in range(len(untagged_token)):\n",
    "            if i < start_idx:\n",
    "                scored_token.append(untagged_score)\n",
    "            elif i <= end_idx:\n",
    "                scored_token.append(score)\n",
    "            else:\n",
    "                scored_token.append(untagged_score)\n",
    "        return scored_token\n",
    "    \n",
    "    def split_train_dev_no_leak(self, examples):\n",
    "        src_seg_id_dic = {}\n",
    "        n_rows, n_train = len(examples), int((1 - self.dev_ratio) * len(examples))\n",
    "        for ex in examples:\n",
    "            key = (ex['source'], ex['segment_id'])\n",
    "            if key not in src_seg_id_dic:\n",
    "                src_seg_id_dic[key] = []\n",
    "            src_seg_id_dic[key].append(ex)\n",
    "        n_train_ex = 0\n",
    "        train_examples_dic = {}\n",
    "        train_examples = []\n",
    "        while True:\n",
    "            picked_key = random.choice(list(src_seg_id_dic.keys()))\n",
    "            if picked_key in train_examples_dic:\n",
    "                continue\n",
    "            else:\n",
    "                train_examples_dic[picked_key] = src_seg_id_dic[picked_key]\n",
    "                train_examples.extend(src_seg_id_dic[picked_key])\n",
    "                n_train_ex += len(src_seg_id_dic[picked_key])\n",
    "            if n_train_ex >= n_train:\n",
    "                break\n",
    "        dev_examples = []\n",
    "        for key in src_seg_id_dic.keys():\n",
    "            if key in train_examples_dic:\n",
    "                continue\n",
    "            else:\n",
    "                dev_examples.extend(src_seg_id_dic[key])\n",
    "        with open(self.save_tmp.replace('.json', '_train.json'), mode='a+', encoding='utf-8') as w:        \n",
    "            for example in train_examples:\n",
    "                w.write(json.dumps(example))\n",
    "                w.write(\"\\n\")\n",
    "        with open(self.save_tmp.replace('.json', '_dev.json'), mode='a+', encoding='utf-8') as w:        \n",
    "            for example in dev_examples:\n",
    "                w.write(json.dumps(example))\n",
    "                w.write(\"\\n\")\n",
    "        # print(len(train_examples), n_train, self.dev_ratio, len(examples), len(dev_examples))\n",
    "        return train_examples, dev_examples\n",
    "    \n",
    "    def make_MQM(self):\n",
    "        size_of_data = {lang:0 for lang in self.list_lang_pairs()}\n",
    "        variance_of_data = {lang:[] for lang in self.list_lang_pairs()}\n",
    "        variance_of_data['all'] = []\n",
    "        for lang in self.list_lang_pairs():\n",
    "            src_segments, ref_segments = self.get_src_ref(lang)\n",
    "            sys_segments = self.get_sys(lang)\n",
    "            MQM_data = self.get_MQM(lang)\n",
    "            with open(self.save_tmp.replace('.json', '_full.json'), mode='a+', encoding='utf-8') as w:\n",
    "                if self.mqm_tag:\n",
    "                    rater_dic = self.organize_mqm_tag(MQM_data, src_segments)\n",
    "                    size_of_data[lang] = len(rater_dic)\n",
    "                    if self.avg:\n",
    "                        for key in rater_dic.keys():\n",
    "                            scores = []\n",
    "                            for idx in range(len(rater_dic[key]['rater'])):\n",
    "                                scores.append(self.get_mqm_tagged_score(rater_dic[key]['category'][idx], rater_dic[key]['severity'][idx]))\n",
    "                            rater_dic[key]['score'] = np.mean(scores)\n",
    "                            rater_dic[key]['variance'] = sum([(s-np.mean(scores))**2 for s in scores])\n",
    "                            variance_of_data[lang].append(rater_dic[key]['variance'])\n",
    "                            variance_of_data['all'].append(rater_dic[key]['variance'])\n",
    "                    else:\n",
    "                        for i, key in enumerate(rater_dic.keys()):\n",
    "                            score_tag = []\n",
    "                            scores = []\n",
    "                            for idx in range(len(rater_dic[key]['rater'])):\n",
    "                                tagged_hyp = rater_dic[key]['target'][idx]\n",
    "                                category = rater_dic[key]['category'][idx]\n",
    "                                severity = rater_dic[key]['severity'][idx]\n",
    "                                if len(score_tag) == 0:\n",
    "                                    score_tag = np.asarray(self.get_scored_token(tagged_hyp, category, severity), dtype=np.float64)\n",
    "                                else:\n",
    "                                    score_tag += np.asarray(self.get_scored_token(tagged_hyp, category, severity), dtype=np.float64)\n",
    "                                scores.append(self.get_mqm_tagged_score(category, severity, return_scaler=True))\n",
    "                            score_tag = score_tag / len(rater_dic[key]['rater'])\n",
    "                            if 'mean_score' not in rater_dic[key]:\n",
    "                                rater_dic[key]['mean_score'] = np.mean(scores)\n",
    "                            rater_dic[key]['variance'] = sum([(s-np.mean(scores))**2 for s in scores])\n",
    "                            variance_of_data[lang].append(rater_dic[key]['variance'])\n",
    "                            variance_of_data['all'].append(rater_dic[key]['variance'])\n",
    "                            rater_dic[key]['score'] = score_tag.tolist()\n",
    "                    examples = []\n",
    "                    for key in rater_dic.keys():\n",
    "                        seg_id = rater_dic[key]['seg_id']\n",
    "                        sys_name = rater_dic[key]['sys_name']\n",
    "                        ref_segment = ref_segments[seg_id-1]\n",
    "                        src_segment = rater_dic[key]['source']\n",
    "                        sys_segment = rater_dic[key]['untagged_hyp']\n",
    "                        score = rater_dic[key]['score']\n",
    "                        if 'mean_score' in rater_dic[key]:\n",
    "                            mean_score = rater_dic[key]['mean_score']\n",
    "                        else:\n",
    "                            mean_score = 0.0\n",
    "                        variance = rater_dic[key]['variance']\n",
    "                        example = self.to_json(self.year, lang, \n",
    "                                                   src_segment, ref_segment, 'ref-A',\n",
    "                                                   sys_segment, score, mean_score,\n",
    "                                                   seg_id, sys_name,\n",
    "                                               variance, '', '')\n",
    "                        w.write(example)\n",
    "                        w.write(\"\\n\")\n",
    "                        size_of_data[lang] += 1\n",
    "                        examples.append(self.to_dict_example(self.year, lang, src_segment, \n",
    "                                                             ref_segment, 'ref-A', sys_segment, \n",
    "                                                             score, mean_score, seg_id, sys_name,\n",
    "                                                             variance, '', ''))\n",
    "                    if self.split_dev:\n",
    "                        self.split_train_dev_no_leak(examples)\n",
    "                else:\n",
    "                    examples = []   \n",
    "                    for line in MQM_data[1:]:\n",
    "                        sys_name, seg_id, score = self.parse_rating(line)\n",
    "                        if score == None:\n",
    "                            continue\n",
    "                        src_segment = src_segments[seg_id-1]\n",
    "                        ref_segment = ref_segments[seg_id-1]\n",
    "\n",
    "                        try:\n",
    "                            if sys_name == 'Nemo':\n",
    "                                sys_name = 'NVIDIA-NeMo'\n",
    "                            elif sys_name in ['metricsystem1', 'metricsystem2', 'metricsystem3', 'metricsystem4', 'metricsystem5', \n",
    "                                              'ref-A', 'ref-B', 'ref-C', 'ref-D']:\n",
    "                                continue\n",
    "                            sys_segment = sys_segments[sys_name][seg_id-1]\n",
    "                        except:\n",
    "                            if not sys_name in dic_of_sys:\n",
    "                                dic_of_sys[sys_name] = sys_segments.keys()\n",
    "                                print(sys_name)\n",
    "                                print(sys_segments.keys())\n",
    "                        example = self.to_json(self.year, lang, src_segment, \n",
    "                                               ref_segment, 'ref-A', sys_segment, \n",
    "                                               score, 0.0, seg_id, sys_name,\n",
    "                                               0, '', '')\n",
    "                        w.write(example)\n",
    "                        w.write(\"\\n\")\n",
    "                        size_of_data[lang] += 1\n",
    "                        examples.append(self.to_dict_example(self.year, lang, src_segment, \n",
    "                                                             ref_segment, 'ref-A', sys_segment, \n",
    "                                                             score, 0.0, seg_id, sys_name,\n",
    "                                                             0, '', ''))\n",
    "                    if self.split_dev:\n",
    "                        self.split_train_dev_no_leak(examples)\n",
    "        print(size_of_data)\n",
    "        return variance_of_data\n",
    "\n",
    "    def postprocess(self, remove_null_refs=True, average_duplicates=True):\n",
    "        \"\"\"Postprocesses a JSONL file of ratings downloaded from WMT.\"\"\"\n",
    "        \n",
    "        def read_and_rerwite(df_file):\n",
    "            logger.info(\"Reading and processing wmt data...\")\n",
    "            with open(df_file, \"r\") as f:\n",
    "                ratings_df = pd.read_json(f, lines=True)\n",
    "            ratings_df.rename(columns={\"rating\": \"score\"}, inplace=True)\n",
    "            logger.info(\"Saving clean file.\")\n",
    "            with open(df_file.replace('_tmp', ''), \"w\") as f:\n",
    "                ratings_df.to_json(f, orient=\"records\", lines=True)\n",
    "            logger.info(\"Cleaning up old ratings file.\")\n",
    "            os.remove(df_file)\n",
    "        \n",
    "        if self.split_dev:\n",
    "            read_and_rerwite(self.save_tmp.replace('.json', '_train.json'))\n",
    "            read_and_rerwite(self.save_tmp.replace('.json', '_dev.json'))\n",
    "            read_and_rerwite(self.save_tmp.replace('.json', '_full.json'))\n",
    "        else:\n",
    "            read_and_rerwite(self.save_tmp.replace('.json', '_full.json'))\n",
    "            \n",
    "    # def _shuffle_leaky(self, all_ratings_df, n_train):\n",
    "    #     \"\"\"Shuffles and splits the ratings allowing overlap in the ref sentences.\"\"\"\n",
    "    #     all_ratings_df = all_ratings_df.sample(frac=1, random_state=555)\n",
    "    #     all_ratings_df = all_ratings_df.reset_index(drop=True)\n",
    "    #     train_ratings_df = all_ratings_df.iloc[:n_train].copy()\n",
    "    #     dev_ratings_df = all_ratings_df.iloc[n_train:].copy()\n",
    "    #     assert len(train_ratings_df) + len(dev_ratings_df) == len(all_ratings_df)\n",
    "    #     return train_ratings_df, dev_ratings_df \n",
    "    \n",
    "    def _shuffle_no_leak(self, all_ratings_df, dev_ratio):\n",
    "        \"\"\"Splits and shuffles such that there is no train/dev example with the same ref.\"\"\"\n",
    "\n",
    "        def is_split_leaky(ix):\n",
    "            return (all_ratings_df.iloc[ix].reference == all_ratings_df.iloc[ix-1].reference)\n",
    "\n",
    "        assert 0 < n_train < len(all_ratings_df.index)\n",
    "\n",
    "        # Clusters the examples by reference sentence.\n",
    "        sentences = all_ratings_df.reference.sample(frac=1, random_state=555).unique()\n",
    "        sentence_to_ix = {s: i for i, s in enumerate(sentences)}\n",
    "        all_ratings_df[\"__sentence_ix__\"] = [sentence_to_ix[s] for s in all_ratings_df.reference]\n",
    "        all_ratings_df = all_ratings_df.sort_values(by=\"__sentence_ix__\")\n",
    "        all_ratings_df.drop(columns=[\"__sentence_ix__\"], inplace=True)\n",
    "\n",
    "        # Moves the split point until there is no leakage.\n",
    "        split_ix = n_train\n",
    "        n_dev_sentences = len(all_ratings_df.iloc[split_ix:].reference.unique())\n",
    "        if n_dev_sentences == 1 and is_split_leaky(split_ix):\n",
    "            raise ValueError(\"Failed splitting data--not enough distinct dev sentences to prevent leak.\")\n",
    "        while is_split_leaky(split_ix):\n",
    "            split_ix += 1\n",
    "        if n_train != split_ix:\n",
    "            logger.info(\"Moved split point from {} to {} to prevent sentence leaking\".format(n_train, split_ix))\n",
    "\n",
    "        # Shuffles the train and dev sets separately.\n",
    "        train_ratings_df = all_ratings_df.iloc[:split_ix].copy()\n",
    "        train_ratings_df = train_ratings_df.sample(frac=1, random_state=555)\n",
    "        dev_ratings_df = all_ratings_df.iloc[split_ix:].copy()\n",
    "        dev_ratings_df = dev_ratings_df.sample(frac=1, random_state=555)\n",
    "        assert len(train_ratings_df) + len(dev_ratings_df) == len(all_ratings_df)\n",
    "\n",
    "        # Checks that there is no leakage.\n",
    "        train_sentences = train_ratings_df.reference.unique()\n",
    "        dev_sentences = dev_ratings_df.reference.unique()\n",
    "        logger.info(\"Using {} and {} unique sentences for train and dev.\".format(len(train_sentences), len(dev_sentences)))\n",
    "        assert not bool(set(train_sentences) & set(dev_sentences))\n",
    "\n",
    "        return train_ratings_df, dev_ratings_df\n",
    "    \n",
    "    \n",
    "#     def shuffle_split(self,\n",
    "#                       train_file=None,\n",
    "#                       dev_file=None,\n",
    "#                       dev_ratio=0.1,\n",
    "#                       prevent_leaks=True):\n",
    "        \n",
    "#         \"\"\"Splits a JSONL WMT ratings file into train/dev.\"\"\"\n",
    "#         logger.info(\"\\n*** Splitting WMT data in train/dev.\")\n",
    "        \n",
    "#         ratings_file = self.save_dir\n",
    "        \n",
    "#         assert os.path.isfile(ratings_file), \"WMT ratings file not found!\"\n",
    "#         base_file = ratings_file + \"_raw\"\n",
    "#         os.replace(ratings_file, base_file)\n",
    "\n",
    "#         logger.info(\"Reading wmt data...\")\n",
    "#         with open(base_file, \"r\") as f:\n",
    "#             ratings_df = pd.read_json(f, lines=True)\n",
    "\n",
    "#         logger.info(\"Doing the shuffle / split.\")\n",
    "#         n_rows, n_train = len(ratings_df), int((1 - dev_ratio) * len(ratings_df))\n",
    "#         logger.info(\"Will attempt to set aside {} out of {} rows for dev.\".format(n_rows - n_train, n_rows))\n",
    "#         train_df, dev_df = self._shuffle_no_leak(ratings_df, dev_ratio)\n",
    "#         logger.info(\"Split train and dev files with {} and {} records.\".format(len(train_df), len(dev_df)))\n",
    "\n",
    "#         logger.info(\"Saving clean file.\")\n",
    "#         if not train_file:\n",
    "#             train_file = ratings_file.replace(\".json\", \"_train.json\")\n",
    "#         with open(train_file, \"w\") as f:\n",
    "#             train_df.to_json(f, orient=\"records\", lines=True)\n",
    "#         if not dev_file:\n",
    "#             dev_file = ratings_file.replace(\".json\", \"_dev.json\")\n",
    "#         with open(dev_file, \"w\") as f:\n",
    "#             dev_df.to_json(f, orient=\"records\", lines=True)\n",
    "\n",
    "#         logger.info(\"Cleaning up old ratings file.\")\n",
    "#         logger.info(\"Created train and dev files with {} and {} records.\".format(len(train_df), len(dev_df)))\n",
    "#         os.remove(base_file)\n",
    "\n",
    "importer = MQM_importer20('/ahc/work3/kosuke-t/WMT/wmt20_mqm_averaged.json',\n",
    "                          mqm_tag=False, avg=True, emb_label=False, emb_only_sev=False, original_score=True, split_dev=True)\n",
    "variance_of_data = importer.make_MQM()\n",
    "importer.postprocess()\n",
    "\n",
    "importer = MQM_importer20('/ahc/work3/kosuke-t/WMT/wmt20_mqm_my_averaged.json',\n",
    "                          mqm_tag=True, avg=True, emb_label=False, emb_only_sev=False, original_score=True, split_dev=True)\n",
    "variance_of_data = importer.make_MQM()\n",
    "importer.postprocess()\n",
    "\n",
    "importer = MQM_importer20('/ahc/work3/kosuke-t/WMT/wmt20_mqm_tagged_xlm-roberta-large.json',\n",
    "                          mqm_tag=True, avg=False, emb_label=False, emb_only_sev=False, original_score=True, split_dev=True)\n",
    "variance_of_data = importer.make_MQM()\n",
    "importer.postprocess()\n",
    "\n",
    "importer = MQM_importer20('/ahc/work3/kosuke-t/WMT/wmt20_mqm_tagged_score_adjusted_xlm-roberta-large.json',\n",
    "                          mqm_tag=True, avg=False, emb_label=False, emb_only_sev=False, original_score=False, split_dev=True)\n",
    "variance_of_data = importer.make_MQM()\n",
    "importer.postprocess()\n",
    "\n",
    "importer = MQM_importer20('/ahc/work3/kosuke-t/WMT/wmt20_mqm_emb_label_only_severity_xlm-roberta-large.json', \n",
    "                          mqm_tag=True, avg=False, emb_label=True, emb_only_sev=True, no_error_score=np.asarray([1, 0, 0]), original_score=False, split_dev=True)\n",
    "variance_of_data = importer.make_MQM()\n",
    "importer.postprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "rm: cannot remove '/ahc/work3/kosuke-t/WMT/wmt20_mqm_my_averaged.json': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# ! rm \"/ahc/work3/kosuke-t/WMT/wmt20_mqm_my_averaged.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.0996e+04, 2.8420e+03, 6.5300e+03, 4.2220e+03, 2.7910e+03,\n",
       "        1.4560e+03, 8.6400e+02, 6.8400e+02, 2.9700e+02, 1.5300e+02,\n",
       "        7.6000e+01, 4.9000e+01, 2.8000e+01, 1.1000e+01, 9.0000e+00,\n",
       "        4.0000e+00, 6.0000e+00, 4.0000e+00, 4.0000e+00, 1.0000e+00]),\n",
       " array([ 0.        ,  1.22727273,  2.45454545,  3.68181818,  4.90909091,\n",
       "         6.13636364,  7.36363636,  8.59090909,  9.81818182, 11.04545455,\n",
       "        12.27272727, 13.5       , 14.72727273, 15.95454545, 17.18181818,\n",
       "        18.40909091, 19.63636364, 20.86363636, 22.09090909, 23.31818182,\n",
       "        24.54545455]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(variance_of_data['all'], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.0646e+04, 7.5150e+03, 3.3020e+03, 1.2140e+03, 3.5900e+02,\n",
       "        1.0200e+02, 3.3000e+01, 1.2000e+01, 1.0000e+01, 5.0000e+00]),\n",
       " array([ 0.        ,  2.45454545,  4.90909091,  7.36363636,  9.81818182,\n",
       "        12.27272727, 14.72727273, 17.18181818, 19.63636364, 22.09090909,\n",
       "        24.54545455]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(variance_of_data['zh-en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No-error\n",
      "10\n",
      "('hyp.Facebook-AI', 'abcnews.420140', '1', '715', 'rater2', 'UN says thousands of anti-Pakistan militants in Afghanistan', 'UN: Tausende militante Anti-Pakistan-Kmpfer in Afghanistan', 'No-error', 'No-error', '\\n')\n",
      "hyp.Facebook-AI\tabcnews.420140\t1\t715\trater2\tUN says thousands of anti-Pakistan militants in Afghanistan\tUN: Tausende militante Anti-Pakistan-Kmpfer in Afghanistan\tNo-error\tNo-error\t\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'severity' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 212>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m#     def make_MQM(self):\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m#         size_of_data = {lang:0 for lang in self.list_lang_pairs()}\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m#         variance_of_data = {lang:[] for lang in self.list_lang_pairs()}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# variance_of_data = importer.make_MQM()\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# importer.postprocess()\u001b[39;00m\n\u001b[1;32m    210\u001b[0m importer \u001b[38;5;241m=\u001b[39m MQM_importer21(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/ahc/work3/kosuke-t/WMT/wmt21_mqm_my_averaged.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    211\u001b[0m                           mqm_tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, avg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, original_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, split_dev\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 212\u001b[0m variance_of_data \u001b[38;5;241m=\u001b[39m \u001b[43mimporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_MQM\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m importer\u001b[38;5;241m.\u001b[39mpostprocess()\n\u001b[1;32m    215\u001b[0m importer \u001b[38;5;241m=\u001b[39m MQM_importer21(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/ahc/work3/kosuke-t/WMT/wmt21_mqm_tagged_xlm-roberta-large.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    216\u001b[0m                           mqm_tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, avg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, original_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, split_dev\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mMQM_importer20.make_MQM\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_tmp\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_full.json\u001b[39m\u001b[38;5;124m'\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m w:\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmqm_tag:\n\u001b[0;32m--> 364\u001b[0m         rater_dic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morganize_mqm_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMQM_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_segments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m         size_of_data[lang] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(rater_dic)\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg:\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mMQM_importer21.organize_mqm_tag\u001b[0;34m(self, MQM_data, src_segments)\u001b[0m\n\u001b[1;32m     84\u001b[0m rater_dic \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m MQM_data[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m---> 86\u001b[0m     sys_name, seg_id, rater, source, target, category, severity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_rating\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sys_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNemo\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     88\u001b[0m         sys_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNVIDIA-NeMo\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mMQM_importer20.parse_rating\u001b[0;34m(self, rating_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m         sys_name, doc, doc_id, seg_id, rater, source, target, category, severity \u001b[38;5;241m=\u001b[39m rating_tuple\n\u001b[0;32m--> 128\u001b[0m     severity \u001b[38;5;241m=\u001b[39m \u001b[43mseverity\u001b[49m\u001b[38;5;241m.\u001b[39mrstrip()\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sys_name, seg_id, rater, source, target, category, severity \n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'severity' referenced before assignment"
     ]
    }
   ],
   "source": [
    "DATA_HOME = '/ahc/work3/kosuke-t'\n",
    "LANGWO_LETTERS = 'LANGWO'\n",
    "LANG_LETTERS = 'L_A_N_G'\n",
    "LANG1_LETTERS = 'LANG1'\n",
    "LANG2_LETTERS = 'LANG2'\n",
    "SYSTEM_LETTERS = 'SYSTEM'\n",
    "src_21_files = os.path.join(DATA_HOME, 'WMT/WMT21-data/sources/newstest2021.{}.src.{}'.format(LANG_LETTERS, LANG1_LETTERS))\n",
    "ref_21_files = os.path.join(DATA_HOME, 'WMT/WMT21-data/references/newstest2021.{}.ref.ref-A.{}'.format(LANG_LETTERS, LANG2_LETTERS))\n",
    "hyp_21_files =  os.path.join(DATA_HOME, 'WMT/WMT21-data/system-outputs/newstest2021/{}/newstest2021.{}.hyp.{}.en'.format(LANG_LETTERS, LANG_LETTERS, SYSTEM_LETTERS, LANG2_LETTERS))\n",
    "MQM_avg_21_files = os.path.join(DATA_HOME, 'WMT/wmt-mqm-human-evaluation/newstest2021/{}/mqm_newstest2021_{}.avg_seg_scores.tsv'.format(LANGWO_LETTERS, LANGWO_LETTERS))\n",
    "MQM_tag_21_files = os.path.join(DATA_HOME, 'WMT/wmt-mqm-human-evaluation/newstest2021/{}/mqm_newstest2021_{}.tsv'.format(LANGWO_LETTERS, LANGWO_LETTERS))\n",
    "                               # project_disc/WMT/wmt-mqm-human-evaluation/newstest2021/ende/mqm_newstest2021_ende.tsv\n",
    "class MQM_importer21(MQM_importer20):\n",
    "    def __init__(self, save_dir, mqm_tag=False, avg=False, emb_label=False, emb_only_sev=True, no_error_score=np.asarray([1, 0, 0]),\n",
    "                 tokenizer_name='xlm-roberta-large', original_score=True, split_dev=False, dev_ratio=0.1):\n",
    "        super(MQM_importer21, self).__init__(save_dir, \n",
    "                                             mqm_tag,\n",
    "                                             avg,\n",
    "                                             emb_label,\n",
    "                                             emb_only_sev,\n",
    "                                             no_error_score,\n",
    "                                             tokenizer_name=tokenizer_name,\n",
    "                                             original_score=original_score,\n",
    "                                             split_dev=split_dev,\n",
    "                                             dev_ratio=dev_ratio)\n",
    "        self.year = '21'\n",
    "        \n",
    "    def parse_eval_file_name(self, fname):\n",
    "        \"\"\"Extracts language pairs from the names of human rating files.\"\"\"\n",
    "        wmt_pattern = re.compile(r\"^ad-seg-scores-([a-z]{2}-[a-z]{2})\\.csv\")\n",
    "        match = re.match(wmt_pattern, fname)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_src_ref(self, lang):\n",
    "        src_lang = lang[:2]\n",
    "        tgt_lang = lang[-2:]\n",
    "        src_file = src_21_files.replace(LANG_LETTERS, lang).replace(LANG1_LETTERS, src_lang)\n",
    "        ref_file = ref_21_files.replace(LANG_LETTERS, lang).replace(LANG2_LETTERS, tgt_lang)\n",
    "        \n",
    "        with open(src_file, \"r\", encoding=\"utf-8\") as f_src:\n",
    "            src_segments = f_src.readlines()\n",
    "        with open(ref_file, \"r\", encoding=\"utf-8\") as f_ref:\n",
    "            ref_segments = f_ref.readlines()\n",
    "\n",
    "        src_segments = [s.strip() for s in src_segments]\n",
    "        ref_segments = [s.strip() for s in ref_segments]\n",
    "\n",
    "        return src_segments, ref_segments\n",
    "    \n",
    "    def get_sys(self, lang):\n",
    "        sys_segments = {}\n",
    "        src_lang = lang[:2]\n",
    "        tgt_lang = lang[-2:]\n",
    "        folder = '/home/is/kosuke-t/project_disc/WMT/wmt21-news-systems/txt/system-outputs'\n",
    "        paths = os.path.join(folder, \"newstest2021.{}.hyp.*.{}\".format(lang, tgt_lang))\n",
    "        all_files = glob.glob(paths)\n",
    "        for fname in all_files:\n",
    "            fname_base = os.path.basename(fname)\n",
    "            sys_name = re.sub(r'newstest2021\\.|{}\\.hyp\\.|\\.{}'.format(lang, tgt_lang), '',fname_base)\n",
    "            with open(fname, mode='r', encoding='utf-8') as r:\n",
    "                data = r.readlines()\n",
    "            data = [d.strip() for d in data]\n",
    "            sys_segments[sys_name] = data\n",
    "        return sys_segments\n",
    "    \n",
    "    def get_MQM(self, lang):\n",
    "        lang_wo = lang.replace('-', '')\n",
    "        if not self.mqm_tag:\n",
    "            fname = os.path.join('/ahc/work3/kosuke-t/WMT/wmt-mqm-human-evaluation/newstest2021',\n",
    "                                 lang_wo, 'mqm_newstest2021_{}.avg_seg_scores.tsv'.format(lang_wo))\n",
    "                \n",
    "        else:\n",
    "            fname = os.path.join('/ahc/work3/kosuke-t/WMT/wmt-mqm-human-evaluation/newstest2021',\n",
    "                             lang_wo, 'mqm_newstest2021_{}.tsv'.format(lang_wo))\n",
    "        \n",
    "        with open(fname, mode='r', encoding='utf-8') as r:\n",
    "            data = r.readlines()\n",
    "        return data\n",
    "    \n",
    "    def organize_mqm_tag(self, MQM_data, src_segments):\n",
    "        rater_dic = {}\n",
    "        for line in MQM_data[1:]:\n",
    "            sys_name, seg_id, rater, source, target, category, severity = self.parse_rating(line)\n",
    "            if sys_name == 'Nemo':\n",
    "                sys_name = 'NVIDIA-NeMo'\n",
    "            elif sys_name in ['metricsystem1', 'metricsystem2', 'metricsystem3', 'metricsystem4', 'metricsystem5', \n",
    "                              'ref-A', 'ref-B', 'ref-C', 'ref-D']:\n",
    "                continue\n",
    "            seg_id = int(seg_id)\n",
    "            untagged_hyp = self.get_untagged_hyp(target)\n",
    "            key = (sys_name, seg_id, untagged_hyp)\n",
    "            if key not in rater_dic:\n",
    "                rater_dic[key] = {'rater':[], 'target':[], 'category':[], 'severity':[]}\n",
    "            rater_dic[key]['rater'].append(rater)\n",
    "            rater_dic[key]['target'].append(target)\n",
    "            rater_dic[key]['category'].append(category)\n",
    "            rater_dic[key]['severity'].append(severity)\n",
    "            rater_dic[key]['seg_id'] = seg_id\n",
    "            rater_dic[key]['sys_name'] = sys_name\n",
    "            rater_dic[key]['untagged_hyp'] = untagged_hyp\n",
    "            \n",
    "            if 'source' in rater_dic[key]:\n",
    "                continue\n",
    "            if source != src_segments[seg_id-1]:\n",
    "                min_lv_seg_id = 0\n",
    "                min_lv_value = 1000\n",
    "                for idx, s in enumerate(src_segments):\n",
    "                    lv_value = levenshtein(source, s)\n",
    "                    if lv_value < min_lv_value:\n",
    "                        min_lv_value = lv_value\n",
    "                        min_lv_seg_id = idx\n",
    "                if seg_id-1 != min_lv_seg_id:\n",
    "                    rater_dic[key]['source'] = src_segments[min_lv_seg_id]\n",
    "                else:\n",
    "                    rater_dic[key]['source'] = src_segments[min_lv_seg_id]\n",
    "            else:\n",
    "                rater_dic[key]['source'] = source\n",
    "        return rater_dic\n",
    "    \n",
    "#     def make_MQM(self):\n",
    "#         size_of_data = {lang:0 for lang in self.list_lang_pairs()}\n",
    "#         variance_of_data = {lang:[] for lang in self.list_lang_pairs()}\n",
    "#         variance_of_data['all'] = []\n",
    "#         for lang in self.list_lang_pairs():\n",
    "#             src_segments, ref_segments = self.get_src_ref(lang)\n",
    "#             sys_segments = self.get_sys(lang)\n",
    "#             MQM_data = self.get_MQM(lang)\n",
    "#             with open(self.save_tmp, mode='w+', encoding='utf-8') as w:\n",
    "#                 if self.mqm_tag:\n",
    "#                     rater_dic = self.organize_mqm_tag(MQM_data, src_segments)\n",
    "#                     size_of_data[lang] = len(rater_dic)\n",
    "#                     if self.avg:\n",
    "#                         for key in rater_dic.keys():\n",
    "#                             scores = []\n",
    "#                             for idx in range(len(rater_dic[key]['rater'])):\n",
    "#                                 scores.append(self.get_mqm_tagged_score(rater_dic[key]['category'][idx], rater_dic[key]['severity'][idx]))\n",
    "#                             rater_dic[key]['score'] = np.mean(scores)\n",
    "#                             rater_dic[key]['variance'] = sum([(s-np.mean(scores))**2 for s in scores])\n",
    "#                             variance_of_data[lang].append(rater_dic[key]['variance'])\n",
    "#                             variance_of_data['all'].append(rater_dic[key]['variance'])\n",
    "#                     else:\n",
    "#                         for i, key in enumerate(rater_dic.keys()):\n",
    "#                             score_tag = []\n",
    "#                             scores = []\n",
    "#                             for idx in range(len(rater_dic[key]['rater'])):\n",
    "#                                 tagged_hyp = rater_dic[key]['target'][idx]\n",
    "#                                 category = rater_dic[key]['category'][idx]\n",
    "#                                 severity = rater_dic[key]['severity'][idx]\n",
    "#                                 if len(score_tag) == 0:\n",
    "#                                     score_tag = np.asarray(self.get_scored_token(tagged_hyp, category, severity), dtype=np.float64)\n",
    "#                                 else:\n",
    "#                                     score_tag += np.asarray(self.get_scored_token(tagged_hyp, category, severity), dtype=np.float64)\n",
    "#                                 scores.append(self.get_mqm_tagged_score(category, severity))\n",
    "#                             rater_dic[key]['variance'] = sum([(s-np.mean(scores))**2 for s in scores])\n",
    "#                             variance_of_data[lang].append(rater_dic[key]['variance'])\n",
    "#                             variance_of_data['all'].append(rater_dic[key]['variance'])\n",
    "#                             rater_dic[key]['score'] = score_tag.tolist()\n",
    "#                     for key in rater_dic.keys():\n",
    "#                         seg_id = rater_dic[key]['seg_id']\n",
    "#                         sys_name = rater_dic[key]['sys_name']\n",
    "#                         ref_segment = ref_segments[seg_id-1]\n",
    "#                         src_segment = rater_dic[key]['source']\n",
    "#                         sys_segment = rater_dic[key]['untagged_hyp']\n",
    "#                         score = rater_dic[key]['score']\n",
    "#                         variance = rater_dic[key]['variance']\n",
    "#                         example = self.to_MQM_json(self.year, lang, \n",
    "#                                                    src_segment, ref_segment, 'ref-A',\n",
    "#                                                    sys_segment, score,\n",
    "#                                                    seg_id, sys_name, variance)\n",
    "#                         w.write(example)\n",
    "#                         w.write(\"\\n\")\n",
    "#                         size_of_data[lang] += 1\n",
    "#                 else:        \n",
    "#                     for line in MQM_data[1:]:\n",
    "#                         sys_name, seg_id, score = self.parse_rating(line)\n",
    "#                         if score == None:\n",
    "#                             continue\n",
    "#                         src_segment = src_segments[seg_id-1]\n",
    "#                         ref_segment = ref_segments[seg_id-1]\n",
    "\n",
    "#                         try:\n",
    "#                             if sys_name == 'Nemo':\n",
    "#                                 sys_name = 'NVIDIA-NeMo'\n",
    "#                             elif sys_name in ['metricsystem1', 'metricsystem2', 'metricsystem3', 'metricsystem4', 'metricsystem5', \n",
    "#                                               'ref-A', 'ref-B', 'ref-C', 'ref-D']:\n",
    "#                                 continue\n",
    "#                             sys_segment = sys_segments[sys_name][seg_id-1]\n",
    "#                         except:\n",
    "#                             if not sys_name in dic_of_sys:\n",
    "#                                 dic_of_sys[sys_name] = sys_segments.keys()\n",
    "#                                 print(sys_name)\n",
    "#                                 print(sys_segments.keys())\n",
    "#                         example = self.to_json('20', lang, src_segment, \n",
    "#                                                ref_segment, 'ref-A', sys_segment, 0, \n",
    "#                                                score, seg_id, sys_name, 0)\n",
    "#                         w.write(example)\n",
    "#                         w.write(\"\\n\")\n",
    "#                         size_of_data[lang] += 1\n",
    "#         print(size_of_data)\n",
    "#         return variance_of_data\n",
    "    \n",
    "# importer = MQM_importer21('/ahc/work3/kosuke-t/WMT/wmt21_mqm_averaged.json',\n",
    "#                           mqm_tag=False, avg=True, original_score=True, split_dev=True)\n",
    "# variance_of_data = importer.make_MQM()\n",
    "# importer.postprocess()\n",
    "\n",
    "importer = MQM_importer21('/ahc/work3/kosuke-t/WMT/wmt21_mqm_my_averaged.json',\n",
    "                          mqm_tag=True, avg=True, original_score=True, split_dev=True)\n",
    "variance_of_data = importer.make_MQM()\n",
    "importer.postprocess()\n",
    "\n",
    "importer = MQM_importer21('/ahc/work3/kosuke-t/WMT/wmt21_mqm_tagged_xlm-roberta-large.json',\n",
    "                          mqm_tag=True, avg=False, original_score=True, split_dev=True)\n",
    "variance_of_data = importer.make_MQM()\n",
    "importer.postprocess()\n",
    "\n",
    "importer = MQM_importer21('/ahc/work3/kosuke-t/WMT/wmt21_mqm_tagged_score_adjusted_xlm-roberta-large.json',\n",
    "                          mqm_tag=True, avg=False, original_score=False, split_dev=True)\n",
    "variance_of_data = importer.make_MQM()\n",
    "importer.postprocess()\n",
    "\n",
    "importer = MQM_importer21('/ahc/work3/kosuke-t/WMT/wmt21_mqm_emb_label_only_severity_xlm-roberta-large.json',\n",
    "                          mqm_tag=True, avg=False, emb_label=True, emb_only_sev=True, no_error_score=np.asarray([1, 0, 0]), original_score=False, split_dev=True)\n",
    "variance_of_data = importer.make_MQM()\n",
    "importer.postprocess()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
