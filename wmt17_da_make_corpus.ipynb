{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#DATA_HOME = '/ahc/work3/kosuke-t/data/'\n",
    "DATA_HOME = '/home/ksudoh/kosuke-t/data_link'\n",
    "#DATA_HOME = sys.argv[-1]\n",
    "\n",
    "DA_HOME = os.path.join(DATA_HOME, 'WMT/DAseg-wmt-newstest2017/ensembled')\n",
    "HUME_HOME = os.path.join(DATA_HOME, 'WMT/wmt17-metrics-task-package/manual-evaluation/hume-testset-round-2.tsv')\n",
    "# DARR_HOME = os.path.join(DATA_HOME, 'WMT/wmt17-metrics-task-package/manual-evaluation/RR-seglevel.csv')\n",
    "SRC_HOME = os.path.join(DATA_HOME, 'WMT/wmt17-metrics-task-package/input/wmt17-metrics-task-no-hybrids/wmt17-submitted-data/txt/sources')\n",
    "REF_HOME = os.path.join(DATA_HOME, 'WMT/wmt17-metrics-task-package/input/wmt17-metrics-task-no-hybrids/wmt17-submitted-data/txt/references')\n",
    "HYP_HOME = os.path.join(DATA_HOME, 'WMT/wmt17-metrics-task-package/input/wmt17-metrics-task-no-hybrids/wmt17-submitted-data/txt/system-outputs/newstest2017')\n",
    "\n",
    "# SRC_himl_HOME = os.path\n",
    "# REF_himl_HOME = \n",
    "# HYP_himl_a_HOME = os.path.join(DATA_HOME, 'WMT/wmt17-metrics-task-package/input/wmt17-metrics-task-no-hybrids/himltest17/txt/system-outputs/himltest2017a')\n",
    "# HYP_himl_b_HOME = os.path.join(DATA_HOME, 'WMT/wmt17-metrics-task-package/input/wmt17-metrics-task-no-hybrids/himltest17/txt/system-outputshimltest2017b')\n",
    "# SAVE_PATH_DARR = os.path.join(DATA_HOME, 'WMT/wmt17_darr.pkl')\n",
    "\n",
    "SAVE_PATH_DA_GOOD_REDUP = os.path.join(DATA_HOME, 'WMT/wmt17_da_good_redup.pkl')\n",
    "SAVE_PATH_DA_SEG = os.path.join(DATA_HOME, 'WMT/wmt17_da_seg.pkl')\n",
    "\n",
    "DIR_NAME = 'WMT15_17_DA_all_all'\n",
    "#DIR_NAME = 'WMT15_17_DA_HUME'\n",
    "SAVE_SRC_TRAIN = os.path.join(DATA_HOME,'SRHDA/{}/train.src'.format(DIR_NAME))\n",
    "SAVE_REF_TRAIN = os.path.join(DATA_HOME,'SRHDA/{}/train.ref'.format(DIR_NAME))\n",
    "SAVE_HYP_TRAIN = os.path.join(DATA_HOME,'SRHDA/{}/train.hyp'.format(DIR_NAME))\n",
    "SAVE_LABEL_TRAIN = os.path.join(DATA_HOME,'SRHDA/{}/train.label'.format(DIR_NAME))\n",
    "SAVE_SRC_VALID = os.path.join(DATA_HOME,'SRHDA/{}/valid.src'.format(DIR_NAME))\n",
    "SAVE_REF_VALID = os.path.join(DATA_HOME,'SRHDA/{}/valid.ref'.format(DIR_NAME))\n",
    "SAVE_HYP_VALID = os.path.join(DATA_HOME,'SRHDA/{}/valid.hyp'.format(DIR_NAME))\n",
    "SAVE_LABEL_VALID = os.path.join(DATA_HOME,'SRHDA/{}/valid.label'.format(DIR_NAME))\n",
    "SAVE_SRC_TEST = os.path.join(DATA_HOME,'SRHDA/{}/test.src'.format(DIR_NAME))\n",
    "SAVE_REF_TEST = os.path.join(DATA_HOME,'SRHDA/{}/test.ref'.format(DIR_NAME))\n",
    "SAVE_HYP_TEST = os.path.join(DATA_HOME,'SRHDA/{}/test.hyp'.format(DIR_NAME))\n",
    "SAVE_LABEL_TEST = os.path.join(DATA_HOME,'SRHDA/{}/test.label'.format(DIR_NAME))\n",
    "\n",
    "langs_news = ['cs-en', 'de-en', 'en-cs', 'en-de', 'en-fi', 'en-lv', 'en-ru', \n",
    "              'en-tr', 'en-zh', 'fi-en', 'lv-en', 'ru-en', 'tr-en', 'zh-en']\n",
    "\n",
    "\n",
    "# systems = {'cs-en':['CUNI-Transformer.5560', \n",
    "#                     'online-A.0', \n",
    "#                     'online-B.0', \n",
    "#                     'online-G.0', \n",
    "#                     'uedin.5561'], \n",
    "#            'de-en':[], \n",
    "#            'et-en':[], \n",
    "#            'fi-en':[], \n",
    "#            'ru-en':[], \n",
    "#            'tr-en':[], \n",
    "#            'zh-en':[], \n",
    "#            'en-cs':[], \n",
    "#            'en-de':[], \n",
    "#            'en-et':[], \n",
    "#            'en-fi':[], \n",
    "#            'en-ru':[], \n",
    "#            'en-tr':[], \n",
    "#            'en-zh':[]}\n",
    "\n",
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "import csv\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from  tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    data = []\n",
    "    with open(filename, mode='r', encoding='utf-8') as r:\n",
    "        data = r.read().split(os.linesep)\n",
    "        if data[-1] == '':\n",
    "            data.pop(-1)\n",
    "    return data\n",
    "\n",
    "SRC_files = {lang:load_file(os.path.join(SRC_HOME, 'newstest2017-{0}{1}-src.{0}'.format(lang.split('-')[0], lang.split('-')[1])))  for lang in langs_news}\n",
    "REF_files = {lang:load_file(os.path.join(REF_HOME, 'newstest2017-{0}{1}-ref.{1}'.format(lang.split('-')[0], lang.split('-')[1]))) for lang in langs_news}\n",
    "HYP_files = {lang:{} for lang in langs_news}\n",
    "\n",
    "for lang in langs_news:\n",
    "    for fname in os.listdir(os.path.join(HYP_HOME, lang)):\n",
    "        if not fname.startswith('newstest2017'):\n",
    "            continue\n",
    "        # extract system id from fname\n",
    "        system_id = copy.deepcopy(fname).replace('newstest2017.', '').replace('.{}'.format(lang), '')\n",
    "        # add\n",
    "        HYP_files[lang][system_id] = load_file(os.path.join(os.path.join(HYP_HOME, lang), fname))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â†“DARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DArr = load_file(DARR_HOME)\n",
    "# corpus = []\n",
    "# for idx, da_data in enumerate(DArr):\n",
    "#     if idx == 0:\n",
    "#         continue\n",
    "#     lang = da_data.split(' ')[0]\n",
    "#     sid = int(da_data.split(' ')[2])\n",
    "#     better_sys = da_data.split(' ')[3]\n",
    "#     worse_sys = da_data.split(' ')[4]\n",
    "#     corpus.append({'lang': lang, \n",
    "#                    'sid':sid,\n",
    "#                    'src': SRC_files[lang][sid-1], \n",
    "#                    'ref': REF_files[lang][sid-1], \n",
    "#                    'hyp1': HYP_files[lang][better_sys][sid-1], \n",
    "#                    'hyp2': HYP_files[lang][worse_sys][sid-1], \n",
    "#                    'better':'hyp1'})\n",
    "# print('saving {}'.format(SAVE_PATH_DARR))\n",
    "# with open(SAVE_PATH_DARR, mode='wb') as w:\n",
    "#     pickle.dump(corpus, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DA for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_good_redup = {lang:'' for lang in langs_news}\n",
    "DA_data_good_redup = {lang:[] for lang in langs_news}\n",
    "for lang in langs_news:\n",
    "    file_path = os.path.join(DA_HOME, 'ad-{}-good-stnd.csv'.format(lang.replace('-', '')))\n",
    "    if os.path.isfile(file_path):\n",
    "        filename_good_redup[lang] = file_path\n",
    "        DA_data_good_redup[lang] = load_file(file_path) \n",
    "    else:\n",
    "#         print('{} does not exist'.format(file_path))\n",
    "        pass\n",
    "filename_seg_scores = os.path.join(DA_HOME, 'ad-seg-scores-ensembled.csv')\n",
    "DA_data_seg_scores = load_file(filename_seg_scores)\n",
    "\n",
    "def make_corpus_good_stnd_redup(langs, DA_data):\n",
    "    corpus = []\n",
    "    type_set = set()\n",
    "    for lang in langs:\n",
    "        for idx, row in enumerate(DA_data[lang]):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "\n",
    "            type_id = row.split('\\t')[8]\n",
    "            score = float(row.split('\\t')[10])\n",
    "            sid = int(row.split('\\t')[9])\n",
    "            system_id = row.split('\\t')[6]\n",
    "\n",
    "            type_set.add(type_id)\n",
    "\n",
    "            if type_id != 'SYSTEM':\n",
    "                continue\n",
    "            \n",
    "            if system_id in HYP_files[lang]:\n",
    "                corpus.append({'lang':lang,\n",
    "                               'sid':sid,\n",
    "                               'year':17,\n",
    "                               'src':SRC_files[lang][sid-1],\n",
    "                               'ref':REF_files[lang][sid-1],\n",
    "                               'hyp':HYP_files[lang][system_id][sid-1],\n",
    "                               'label':score})               \n",
    "    return corpus, type_set\n",
    "\n",
    "\n",
    "def make_corpus_seg_scores(DA_data):\n",
    "    corpus = []\n",
    "    sys_dic = {}\n",
    "    for idx, row in enumerate(DA_data):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        if re.search('SRC TRG HIT N.raw N.z SID SYS RAW.SCR Z.SCR', row):\n",
    "#             print(idx)\n",
    "            continue\n",
    "        lang = '{}-{}'.format(row.split(' ')[0], row.split(' ')[1]) \n",
    "        system_id = row.split(' ')[6]\n",
    "        sid = int(row.split(' ')[5])\n",
    "        score = float(row.split(' ')[8])\n",
    "        n = int(row.split(' ')[3])\n",
    "        key = (lang, system_id)\n",
    "        if re.search('\\+', system_id):\n",
    "            system_id = system_id.split('+')[0]\n",
    "            key = (lang, system_id)\n",
    "            if key not in sys_dic:\n",
    "                sys_dic[key] = 1\n",
    "            else:\n",
    "                sys_dic[key] += 1\n",
    "            \n",
    "        corpus.append({'lang':lang,\n",
    "                       'sid':sid,\n",
    "                       'year':17,\n",
    "                       'src':SRC_files[lang][sid-1],\n",
    "                       'ref':REF_files[lang][sid-1],\n",
    "                       'hyp':HYP_files[lang][system_id][sid-1],\n",
    "                       'label':score})\n",
    "    return corpus, sys_dic\n",
    "\n",
    "corpus_good_redup, type_set = make_corpus_good_stnd_redup(langs_news, DA_data_good_redup)\n",
    "corpus_seg_scores, sys_dic = make_corpus_seg_scores(DA_data_seg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good redup\n",
      "-- corpus size for each language pair ---\n",
      "cs-en has 8456 instances\n",
      "de-en has 8608 instances\n",
      "en-cs has 0 instances\n",
      "en-de has 0 instances\n",
      "en-fi has 0 instances\n",
      "en-lv has 0 instances\n",
      "en-ru has 8709 instances\n",
      "en-tr has 0 instances\n",
      "en-zh has 8627 instances\n",
      "fi-en has 8403 instances\n",
      "lv-en has 8801 instances\n",
      "ru-en has 8758 instances\n",
      "tr-en has 8505 instances\n",
      "zh-en has 8606 instances\n",
      "\n",
      "seg scores\n",
      "-- corpus size for each language pair ---\n",
      "cs-en has 560 instances\n",
      "de-en has 560 instances\n",
      "en-cs has 0 instances\n",
      "en-de has 0 instances\n",
      "en-fi has 0 instances\n",
      "en-lv has 0 instances\n",
      "en-ru has 560 instances\n",
      "en-tr has 0 instances\n",
      "en-zh has 560 instances\n",
      "fi-en has 560 instances\n",
      "lv-en has 560 instances\n",
      "ru-en has 560 instances\n",
      "tr-en has 560 instances\n",
      "zh-en has 560 instances\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('good redup')\n",
    "print('-- corpus size for each language pair ---')\n",
    "lang_count = {lang:0 for lang in langs_news}\n",
    "for corpus in corpus_good_redup:\n",
    "    lang = corpus['lang']\n",
    "    lang_count[lang] += 1\n",
    "for lang in langs_news:\n",
    "    print('{} has {} instances'.format(lang, lang_count[lang]))\n",
    "print()\n",
    "\n",
    "print('seg scores')\n",
    "print('-- corpus size for each language pair ---')\n",
    "lang_count = {lang:0 for lang in langs_news}\n",
    "for corpus in corpus_seg_scores:\n",
    "    lang = corpus['lang']\n",
    "    lang_count[lang] += 1\n",
    "for lang in langs_news:\n",
    "    print('{} has {} instances'.format(lang, lang_count[lang]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving /home/ksudoh/kosuke-t/data_link/WMT/wmt17_da_good_redup.pkl\n",
      "saving /home/ksudoh/kosuke-t/data_link/WMT/wmt17_da_seg.pkl\n"
     ]
    }
   ],
   "source": [
    "print('saving {}'.format(SAVE_PATH_DA_GOOD_REDUP))\n",
    "with open(SAVE_PATH_DA_GOOD_REDUP, mode='wb') as w:\n",
    "    pickle.dump(corpus_good_redup, w)\n",
    "    \n",
    "print('saving {}'.format(SAVE_PATH_DA_SEG))\n",
    "with open(SAVE_PATH_DA_SEG, mode='wb') as w:\n",
    "    pickle.dump(corpus_seg_scores, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        print('{} does not exist'.format(filename))\n",
    "        exit(-2)\n",
    "    data = None\n",
    "    with open(filename, mode='rb') as r:\n",
    "        data = pickle.load(r)\n",
    "    return data\n",
    "\n",
    "# return True when duplicated\n",
    "def dup_check(train_data, valid_data):\n",
    "    flag = False\n",
    "    duplicate_dic = {}\n",
    "    dup_index = []\n",
    "    for i, val in enumerate(valid_data):\n",
    "        key = (val['lang'], val['year'], val['sid'])\n",
    "        if key not in duplicate_dic:\n",
    "            duplicate_dic[key] = [i]\n",
    "        else:\n",
    "            duplicate_dic[key].append(i)\n",
    "    for i, trn in enumerate(train_data):\n",
    "        key = (trn['lang'], trn['year'], trn['sid'])\n",
    "        if key in duplicate_dic:\n",
    "            flag = True\n",
    "            dup_index.append({'train':i, 'valid':duplicate_dic[key]})\n",
    "    return flag, dup_index\n",
    "            \n",
    "\n",
    "def split_data(Alldata, ratio, exception_index, duplication=False):\n",
    "    \n",
    "    all_index = [i for i in range(len(Alldata))]\n",
    "    valid_index = random.sample(list(set(all_index)-set(exception_index)), int((len(Alldata)-len(exception_index))*ratio))\n",
    "    train_index = list(set(all_index)-set(valid_index))\n",
    "\n",
    "    train_data = []\n",
    "    valid_data = []\n",
    "    for idx in all_index:\n",
    "        if idx in train_index:\n",
    "            train_data.append(copy.deepcopy(Alldata[idx]))\n",
    "        else:\n",
    "            valid_data.append(copy.deepcopy(Alldata[idx]))\n",
    "    \n",
    "    return train_data, valid_data\n",
    "\n",
    "def get_dup_index(Alldata):\n",
    "    exception_index = []\n",
    "    dup_set = {}\n",
    "    for idx, data in enumerate(Alldata):\n",
    "        key = (data['lang'], data['year'], data['sid'])\n",
    "        if key not in dup_set:\n",
    "            dup_set[key] = [idx]\n",
    "        else:\n",
    "            exception_index.extend(dup_set[key])\n",
    "            exception_index.append(idx)\n",
    "    exception_index = sorted(list(set(exception_index)))\n",
    "    return exception_index, dup_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.1\n",
    "\n",
    "SAVE_HOME = os.path.join(DATA_HOME, 'WMT')\n",
    "\n",
    "Alldata15_16 = []\n",
    "Alldata15_16.extend(load_pickle(os.path.join(SAVE_HOME, 'wmt15_da.pkl')))\n",
    "Alldata15_16.extend(load_pickle(os.path.join(SAVE_HOME, 'wmt16_da.pkl')))\n",
    "Alldata_langs = {}\n",
    "for data in Alldata15_16:\n",
    "    lang = data['lang']\n",
    "#     if lang == 'en-ru':\n",
    "#         continue\n",
    "    if lang not in Alldata_langs:\n",
    "        Alldata_langs[lang] = []\n",
    "    Alldata_langs[lang].append(data)\n",
    "\n",
    "train_data_langs = {}\n",
    "valid_data_langs = {}\n",
    "for lang in Alldata_langs.keys():\n",
    "#     print('splitting {} data'.format(lang))\n",
    "    exception_index, dup_set = get_dup_index(Alldata_langs[lang])\n",
    "    train_data, valid_data = split_data(Alldata_langs[lang], valid_ratio, exception_index, duplication=False)\n",
    "    train_data_langs[lang] = train_data\n",
    "    valid_data_langs[lang] = valid_data\n",
    "\n",
    "# Da = load_pickle(os.path.join(SAVE_HOME, 'wmt17_da_seg.pkl'))\n",
    "Da = corpus_seg_scores\n",
    "test_data_langs = {}\n",
    "for data in Da:\n",
    "    lang = data['lang']\n",
    "    if lang not in test_data_langs:\n",
    "        test_data_langs[lang] = []\n",
    "    test_data_langs[lang].append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = []\n",
    "ref_train = []\n",
    "hyp_train = []\n",
    "label_train = []\n",
    "\n",
    "src_valid = []\n",
    "ref_valid = []\n",
    "hyp_valid = []\n",
    "label_valid = []\n",
    "\n",
    "src_test = []\n",
    "ref_test = []\n",
    "hyp_test = []\n",
    "label_test = []\n",
    "\n",
    "for lang in train_data_langs.keys():\n",
    "    for tdata in train_data_langs[lang]:\n",
    "        src_train.append('{}\\t{}'.format(tdata['src'], lang))\n",
    "        ref_train.append('{}\\t{}'.format(tdata['ref'], lang))\n",
    "        hyp_train.append('{}\\t{}'.format(tdata['hyp'], lang))\n",
    "        label_train.append('{}\\t{}'.format(tdata['label'], lang))\n",
    "for lang in valid_data_langs.keys():\n",
    "    for vdata in valid_data_langs[lang]:\n",
    "        src_valid.append('{}\\t{}'.format(vdata['src'], lang))\n",
    "        ref_valid.append('{}\\t{}'.format(vdata['ref'], lang))\n",
    "        hyp_valid.append('{}\\t{}'.format(vdata['hyp'], lang))\n",
    "        label_valid.append('{}\\t{}'.format(vdata['label'], lang))   \n",
    "for lang in test_data_langs.keys():    \n",
    "    for tsdata in test_data_langs[lang]:\n",
    "        src_test.append('{}\\t{}'.format(tsdata['src'], lang))\n",
    "        ref_test.append('{}\\t{}'.format(tsdata['ref'], lang))\n",
    "        hyp_test.append('{}\\t{}'.format(tsdata['hyp'], lang))\n",
    "        label_test.append('{}\\t{}'.format(tsdata['label'], lang))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeout(filename, obj):\n",
    "    with open(filename, mode='w', encoding='utf-8') as w:\n",
    "        for d in obj:\n",
    "            w.write(d+os.linesep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writeout(SAVE_SRC_TRAIN, src_train)\n",
    "# writeout(SAVE_REF_TRAIN, ref_train)\n",
    "# writeout(SAVE_HYP_TRAIN, hyp_train)\n",
    "# writeout(SAVE_LABEL_TRAIN, label_train)\n",
    "\n",
    "# writeout(SAVE_SRC_VALID, src_valid)\n",
    "# writeout(SAVE_REF_VALID, ref_valid)\n",
    "# writeout(SAVE_HYP_VALID, hyp_valid)\n",
    "# writeout(SAVE_LABEL_VALID, label_valid)\n",
    "\n",
    "# writeout(SAVE_SRC_TEST, src_test)\n",
    "# writeout(SAVE_REF_TEST, ref_test)\n",
    "# writeout(SAVE_HYP_TEST, hyp_test)\n",
    "# writeout(SAVE_LABEL_TEST, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cs-en': 560,\n",
       " 'de-en': 560,\n",
       " 'fi-en': 560,\n",
       " 'lv-en': 560,\n",
       " 'ru-en': 560,\n",
       " 'tr-en': 560,\n",
       " 'zh-en': 560,\n",
       " 'en-ru': 560,\n",
       " 'en-zh': 560}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs_dic = {}\n",
    "for row in src_test:\n",
    "    lang = row.split('\\t')[-1]\n",
    "    if lang not in langs_dic:\n",
    "        langs_dic[lang] = 1\n",
    "    else:\n",
    "        langs_dic[lang] += 1\n",
    "langs_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cs-en': 954,\n",
       " 'de-en': 954,\n",
       " 'en-ru': 954,\n",
       " 'fi-en': 954,\n",
       " 'ru-en': 954,\n",
       " 'ro-en': 504,\n",
       " 'tr-en': 504}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs_dic = {}\n",
    "for lang, data in train_data_langs.items():\n",
    "    langs_dic[lang] = len(data)\n",
    "langs_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5274"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "954*5+504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5360"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "536+4824"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4810"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5344-534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
