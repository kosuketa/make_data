{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2880d857-0620-4d87-8f97-a8591e1c4810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is/kosuke-t/.pyenv/versions/3.8.13/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import abc\n",
    "import glob\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import tarfile\n",
    "import urllib\n",
    "import urllib.request\n",
    "import subprocess\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import sys\n",
    "from logging import getLogger\n",
    "import pandas as pd\n",
    "import six\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "# import tensorflow.compat.v1 as tf\n",
    "from distutils.dir_util import copy_tree\n",
    "from logging import getLogger, StreamHandler, DEBUG\n",
    "logger = getLogger(__name__)\n",
    "handler = StreamHandler()\n",
    "handler.setLevel(DEBUG)\n",
    "logger.setLevel(DEBUG)\n",
    "logger.addHandler(handler)\n",
    "logger.propagate = False\n",
    "from transformers import AutoTokenizer\n",
    "from polyleven import levenshtein\n",
    "from tqdm import tqdm\n",
    "from decimal import Decimal, ROUND_HALF_UP, ROUND_HALF_EVEN\n",
    "import difflib\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3a4d6c-c12a-4fc6-ad94-5d4dafa49b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MQMデータ取り扱い時の注意点\n",
    "・srcファイルとMQMファイル内のsrc間でのpunctuationの違い\n",
    "・同じsystemの出力で同じhypなはずなのに、 MQMファイル内でraterごと、またはrater内でも異なる\n",
    "・\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "422b3a65-a178-4276-8740-e159ccfe4310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing 2900 (seg_id, system) pairs, 22656 sentences\n",
      "removing 7666 (seg_id, system) pairs, 64033 sentences\n",
      "23614 example for /ahc/work3/kosuke-t/WMT/wmt20_mqm_clean_xlm_low_agreement_full.json\n",
      "21254 example for /ahc/work3/kosuke-t/WMT/wmt20_mqm_clean_xlm_low_agreement_train.json\n",
      "2360 example for /ahc/work3/kosuke-t/WMT/wmt20_mqm_clean_xlm_low_agreement_dev.json\n"
     ]
    }
   ],
   "source": [
    "DATA_HOME = '/ahc/work3/kosuke-t/WMT'\n",
    "\n",
    "MQM_1hot_only_severity_to_vec = {\"no-error\":np.asarray([1, 0, 0]), \n",
    "                                 \"No-error\":np.asarray([1, 0, 0]),\n",
    "                                 \"Neutral\":np.asarray([0, 1, 0]),\n",
    "                                 \"Minor\":np.asarray([0, 1, 0]),\n",
    "                                 \"Major\":np.asarray([0, 0, 1])}\n",
    "MQM_tag_list = [\"No-error\", \n",
    "                \"Neutral\",\n",
    "                \"Minor\",\n",
    "                \"Major\"]\n",
    "\n",
    "\n",
    "class MQM_importer20():\n",
    "    def __init__(self, save_dir,\n",
    "                 emb_label=False, emb_only_sev=True, \n",
    "                 no_error_score=np.asarray([1, 0, 0]),\n",
    "                 tokenizer_name='xlm-roberta-large',\n",
    "                 google_score=True, split_dev=False,\n",
    "                 dev_ratio=0.1, remove_strange=False,\n",
    "                 agreement='low'):\n",
    "        self.save_dir = save_dir\n",
    "        self.save_tmp = save_dir + '_tmp'\n",
    "        self.year = '20'\n",
    "        self.emb_label = emb_label\n",
    "        self.emb_only_sev = emb_only_sev\n",
    "        self.no_error_score = no_error_score\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        self.google_score = google_score\n",
    "        self.split_dev = split_dev\n",
    "        self.dev_ratio = dev_ratio\n",
    "        self.remove_strange = remove_strange\n",
    "        self.agreement = agreement\n",
    "\n",
    "        self.src_files = os.path.join(DATA_HOME,\n",
    "                                      'WMT20_data/txt/sources/newstest2020-{}-src.{}.txt'.format('LANGWO_LETTERS', 'LANG1_LETTERS'))\n",
    "        self.ref_files = os.path.join(DATA_HOME,\n",
    "                                      'WMT20_data/txt/references/newstest2020-{}-ref.{}.txt'.format('LANGWO_LETTERS', 'LANG2_LETTERS'))\n",
    "        self.hyp_files = os.path.join(DATA_HOME,\n",
    "                                      'WMT20_data/txt/system-outputs/{}/newstest2020.{}.{}.txt'.format('LANG', 'LANG', 'SYSTEM'))\n",
    "        self.MQM_avg_files = os.path.join(DATA_HOME,\n",
    "                                          'wmt-mqm-human-evaluation/newstest2020/{}/mqm_newstest2020_{}.avg_seg_scores.tsv'.format('LANGWO_LETTERS', 'LANGWO_LETTERS'))\n",
    "        self.MQM_tag_files = os.path.join(DATA_HOME,\n",
    "                                          'wmt-mqm-human-evaluation/newstest2020/{}/mqm_newstest2020_{}.tsv'.format('LANGWO_LETTERS', 'LANGWO_LETTERS'))\n",
    "        self.langs = ['en-de', 'zh-en']\n",
    "        self.systems = {'en-de': ['eTranslation.737',\n",
    "                                  'Human-A.0',\n",
    "                                  'Human-B.0',\n",
    "                                  'Human-P.0',\n",
    "                                  'Huoshan_Translate.832',\n",
    "                                  'Online-A.1574',\n",
    "                                  'Online-B.1590',\n",
    "                                  'OPPO.1535',\n",
    "                                  'Tencent_Translation.1520',\n",
    "                                  'Tohoku-AIP-NTT.890'],\n",
    "                        'zh-en': ['DeepMind.381', \n",
    "                                  'DiDi_NLP.401',\n",
    "                                  'Human-A.0',\n",
    "                                  'Human-B.0',\n",
    "                                  'Huoshan_Translate.919',\n",
    "                                  'Online-B.1605',\n",
    "                                  'OPPO.1422',\n",
    "                                  'Tencent_Translation.1249',\n",
    "                                  'THUNLP.1498',\n",
    "                                  'WeChat_AI.1525']}\n",
    "        \n",
    "    def get_MQM_fname(self, fname, lang):\n",
    "        lang1 = lang.split('-')[0]\n",
    "        lang2 = lang.split('-')[1]\n",
    "        langwo = lang.replace('-', '')\n",
    "        return fname.replace('LANGWO_LETTERS', langwo, 2)\n",
    "    \n",
    "    def load_file(self, fname):\n",
    "        with open(fname, mode='r', encoding='utf-8') as r:\n",
    "            return r.readlines()\n",
    "        \n",
    "    def get_langs(self):\n",
    "        return self.langs\n",
    "    \n",
    "    def get_srcs(self, lang):\n",
    "        langwo = lang.replace('-', '')\n",
    "        lang1 = lang[:2]\n",
    "        fname = self.src_files.replace('LANGWO_LETTERS', langwo).replace('LANG1_LETTERS', lang1)\n",
    "        src_data = self.load_file(fname)\n",
    "        src_data = [s.rstrip() for s in src_data]\n",
    "        return src_data\n",
    "    \n",
    "    def get_refs(self, lang):\n",
    "        langwo = lang.replace('-', '')\n",
    "        lang2 = lang[-2:]\n",
    "        fname = self.ref_files.replace('LANGWO_LETTERS', langwo).replace('LANG2_LETTERS', lang2)\n",
    "        ref_data = self.load_file(fname)\n",
    "        ref_data = [r.rstrip() for r in ref_data]\n",
    "        return ref_data\n",
    "    \n",
    "    def get_hyps(self, lang):\n",
    "        hyp_data = {}\n",
    "        for sys in self.systems[lang]:\n",
    "            fname = self.hyp_files.replace('LANG', lang, 2).replace('SYSTEM', sys)\n",
    "            hyp_data[sys] = self.load_file(fname)\n",
    "            hyp_data[sys] = [h.rstrip() for h in hyp_data[sys]]\n",
    "        return hyp_data\n",
    "        \n",
    "    def get_score(self, category, severity):\n",
    "        if self.google_score:\n",
    "            scores = [0.0, 0.1, 1.0, 5.0, 25.0]\n",
    "        else:\n",
    "            scores = [0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "        if severity == 'Major':\n",
    "            if category == 'Non-translation!' or category == 'Non-translation':\n",
    "                return scores[4]\n",
    "            else:\n",
    "                return scores[3]\n",
    "        elif severity == 'Minor':\n",
    "            if category == 'Fluency/Punctuation':\n",
    "                return scores[1]\n",
    "            else:\n",
    "                return scores[2]\n",
    "        elif severity in ['no-error', 'No-error', 'Neutral']:\n",
    "            return scores[0]\n",
    "        else:\n",
    "            print(severity, category)\n",
    "            raise NotImplementedError\n",
    "    \n",
    "    def get_untagged_hyp(self, tagged_hyp):\n",
    "        return tagged_hyp.replace('<v>', '').replace('</v>', '')\n",
    "    \n",
    "    def get_untagged_token(self, tagged_hyp):\n",
    "        untagged_hyp = self.get_untagged_hyp(tagged_hyp)\n",
    "        token_untagged_hyp = self.tokenizer.encode(untagged_hyp)\n",
    "        return token_untagged_hyp\n",
    "    \n",
    "    def get_src_ref_token(self, sent):\n",
    "        return self.tokenizer.encode(sent, truncation=True)\n",
    "    \n",
    "    def get_pos_start_tag(self, tagged_hyp):\n",
    "        token_untagged_hyp = self.get_untagged_token(tagged_hyp) \n",
    "        token_tagged_hyp = self.tokenizer.encode(tagged_hyp)\n",
    "        for idx, t_tagged in enumerate(token_tagged_hyp):\n",
    "            if t_tagged == token_untagged_hyp[idx]:\n",
    "                continue\n",
    "            else:\n",
    "                return idx\n",
    "        \n",
    "    def get_pos_end_tag(self, tagged_hyp):\n",
    "        token_untagged_hyp = self.get_untagged_token(tagged_hyp)[::-1]\n",
    "        token_tagged_hyp = self.tokenizer.encode(tagged_hyp)[::-1]\n",
    "        for idx, t_tagged in enumerate(token_tagged_hyp):\n",
    "            if t_tagged == token_untagged_hyp[idx]:\n",
    "                continue\n",
    "            else:\n",
    "                return len(token_untagged_hyp)-idx\n",
    "\n",
    "    def get_score_token(self, target, category, severity):\n",
    "        score = self.get_score(category, severity)\n",
    "        tagged_hyp = target\n",
    "        untagged_hyp = tagged_hyp.replace('<v>', '').replace('</v>', '')\n",
    "        untagged_token = self.get_untagged_token(tagged_hyp)\n",
    "        untagged_score = self.no_error_score if self.emb_label else 0.0\n",
    "        if severity in ['no-error', 'No-error', 'Neutral']:\n",
    "            scored_token = [untagged_score for _ in untagged_token]\n",
    "            return np.expand_dims(np.asarray(scored_token), axis=0)\n",
    "        else: \n",
    "            start_idx = self.get_pos_start_tag(tagged_hyp)\n",
    "            end_idx = self.get_pos_end_tag(tagged_hyp)\n",
    "            scored_token = []    \n",
    "            for i in range(len(untagged_token)):\n",
    "                try:\n",
    "                    if i < start_idx:\n",
    "                        scored_token.append(untagged_score)\n",
    "                    elif i <= end_idx:\n",
    "                        scored_token.append(score)\n",
    "                    else:\n",
    "                        scored_token.append(untagged_score)\n",
    "                except:\n",
    "                    from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "            return np.expand_dims(np.asarray(scored_token), axis=0)\n",
    "    \n",
    "    def get_clean_keys(self, lines):\n",
    "        MQM_data = {}\n",
    "        clean_keys = []\n",
    "        bad_keys = []\n",
    "        for i, line in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            line = line.rstrip()\n",
    "            split_line = line.split('\\t')\n",
    "            system = split_line[0]\n",
    "            seg_id = int(split_line[3])\n",
    "            target = split_line[6]\n",
    "            key = (seg_id, system)\n",
    "            if key not in MQM_data:\n",
    "                MQM_data[key] = []\n",
    "            MQM_data[key].append(target)\n",
    "        for key, md in MQM_data.items():\n",
    "            if all([self.get_untagged_hyp(md[0]) == self.get_untagged_hyp(md[i]) for i in range(len(md))]):\n",
    "                clean_keys.append(key)\n",
    "            else:\n",
    "                bad_keys.append(key)\n",
    "        # print(bad_keys)\n",
    "        # return \n",
    "        if self.remove_strange:\n",
    "            print('removing {} (seg_id, system) pairs, {} sentences'.format(len(bad_keys), sum([len(MQM_data[k]) for k in bad_keys])))\n",
    "        return clean_keys\n",
    "    \n",
    "    def get_maj_avg(self, token_score_np):\n",
    "        n_rater = token_score_np.shape[0]\n",
    "        seq_len = token_score_np.shape[1]\n",
    "        n_maj = math.ceil(n_rater/2)\n",
    "        count_up_ls = [0]*seq_len\n",
    "        return_score_token = [0]*seq_len\n",
    "        for token_score in token_score_np:\n",
    "            for i, score in enumerate(token_score):\n",
    "                if score > 0.0:\n",
    "                    count_up_ls[i] += 1\n",
    "                return_score_token[i] += score\n",
    "        for i, count in enumerate(count_up_ls):\n",
    "            if count < n_maj:\n",
    "                return_score_token[i] = 0.0\n",
    "            return_score_token[i] /= n_rater\n",
    "        return return_score_token\n",
    "    \n",
    "    def get_heavy_avg(self, token_score_np):\n",
    "        # n_rater = token_score_np.shape[0]\n",
    "        # seq_len = token_score_np.shape[1]\n",
    "        # n_maj = math.ceil(n_rater/2)\n",
    "        # return_score_token = [0]*seq_len\n",
    "        pass\n",
    "        # return return_score_token\n",
    "    \n",
    "    def get_MQM_keyed_data(self, lang):\n",
    "        MQM_data = {}\n",
    "        MQM_dic = {}\n",
    "        fname = self.get_MQM_fname(self.MQM_tag_files, lang)\n",
    "        rlines = self.load_file(fname)\n",
    "        clean_keys = self.get_clean_keys(rlines)\n",
    "        for i, line in enumerate(rlines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            line = line.rstrip()\n",
    "            split_line = line.split('\\t')\n",
    "            system = split_line[0]\n",
    "            doc = split_line[1]\n",
    "            doc_id = split_line[2]\n",
    "            seg_id = int(split_line[3])\n",
    "            rater = split_line[4]\n",
    "            source = split_line[5]\n",
    "            target = split_line[6]\n",
    "            category = split_line[7]\n",
    "            severity = split_line[8]\n",
    "            score = self.get_score(category, severity)\n",
    "            # score_token, target_token = self.get_score_token(target, category, severity)\n",
    "            key = (lang, system, seg_id)\n",
    "            if (seg_id, system) not in clean_keys and self.remove_strange:\n",
    "                continue\n",
    "            if key in MQM_dic:\n",
    "                flag = False\n",
    "                for d in MQM_dic[key]:\n",
    "                    if d['rater'] == rater:\n",
    "                        d['severity'].append(severity)\n",
    "                        d['category'].append(category)\n",
    "                        d['score'].append(score)\n",
    "                        d['target'].append(target)\n",
    "                        flag = True\n",
    "                if flag == False:\n",
    "                    MQM_dic[key].append({'system':system, 'seg_id':seg_id, \n",
    "                                         'lang': lang,\n",
    "                                         'rater':rater, 'source':source,\n",
    "                                         'target':[target],\n",
    "                                         'category':[category],\n",
    "                                         'severity':[severity],\n",
    "                                         'score':[score]})\n",
    "            else:\n",
    "                MQM_dic[key] = [{'system':system, 'seg_id':seg_id,\n",
    "                                 'lang': lang,\n",
    "                                 'rater':rater, 'source':source,\n",
    "                                 'target':[target],\n",
    "                                 'category':[category],\n",
    "                                 'severity':[severity],\n",
    "                                 'score':[score]}]\n",
    "        ret_dic = {}     \n",
    "        for key in MQM_dic.keys():\n",
    "            lang = key[0]\n",
    "            system = key[1]\n",
    "            seg_id = key[2]\n",
    "            score = np.mean([sum(a['score']) for a in MQM_dic[key]])\n",
    "            source = MQM_dic[key][0]['source']\n",
    "            rater = []\n",
    "            target = []\n",
    "            category = []\n",
    "            severity = []\n",
    "            for md in MQM_dic[key]:\n",
    "                for tgt, cat, sev in zip(md['target'], md['category'], md['severity']):\n",
    "                    untagged_tgt = tgt.replace('<v>', '').replace('</v>', '')\n",
    "                    if untagged_tgt == tgt and sev not in ['no-error','No-error','Neutral']:\n",
    "                        continue\n",
    "                    tgt_token = self.get_untagged_token(tgt)\n",
    "                    score_token = self.get_score_token(tgt,cat,sev)\n",
    "                    tmp_key = (key[0], key[1], key[2], untagged_tgt)\n",
    "                    if tmp_key not in ret_dic:\n",
    "                        ret_dic[tmp_key] = {'lang':lang,\n",
    "                                            'system':system,\n",
    "                                            'seg_id':int(seg_id),\n",
    "                                            'src':source,\n",
    "                                            'hyp':untagged_tgt,\n",
    "                                            'avg_score':score,\n",
    "                                            'target_token':tgt_token,\n",
    "                                            'rater':[md['rater']],\n",
    "                                            'tagged_hyp':[tgt],\n",
    "                                            'severity':[sev],\n",
    "                                            'category':[cat],\n",
    "                                            'score_token':score_token}\n",
    "                    else:\n",
    "                        ret_dic[tmp_key]['rater'].append(md['rater'])\n",
    "                        ret_dic[tmp_key]['tagged_hyp'].append(tgt)\n",
    "                        ret_dic[tmp_key]['severity'].append(sev)\n",
    "                        ret_dic[tmp_key]['category'].append(cat)\n",
    "                        try:\n",
    "                            ret_dic[tmp_key]['score_token'] = np.append(ret_dic[tmp_key]['score_token'],\n",
    "                                                                        score_token,\n",
    "                                                                        axis=0)\n",
    "                        except:\n",
    "                            from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "                        \n",
    "        for key in ret_dic.keys():\n",
    "            rater_num_dic = {}\n",
    "            for r in ret_dic[tmp_key]['rater']:\n",
    "                if r not in rater_num_dic:\n",
    "                    rater_num_dic[r] = 0\n",
    "                rater_num_dic[r] += 1\n",
    "            rater_num = len(list(rater_num_dic.keys()))\n",
    "            try:\n",
    "                ret_dic[key]['avg_score_token'] = (np.sum(np.asarray(ret_dic[key]['score_token']),\n",
    "                                                          axis=0)/rater_num).tolist()\n",
    "                ret_dic[key]['maj_avg_score_token'] = self.get_maj_avg(ret_dic[key]['score_token'])\n",
    "                ret_dic[key]['heavy_avg_score_token'] = self.get_heavy_avg(ret_dic[key]['score_token'])\n",
    "                ret_dic[key]['score_token'] = ret_dic[key]['score_token'].tolist()\n",
    "            except:\n",
    "                from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "        return ret_dic\n",
    "    \n",
    "    def get_MQM_data(self, data_idx=0):\n",
    "        langs = self.get_langs()\n",
    "        all_data = []\n",
    "        for lang in langs:\n",
    "            MQM_data = self.get_MQM_keyed_data(lang)\n",
    "            srcs = self.get_srcs(lang)\n",
    "            refs = self.get_refs(lang)\n",
    "            for key in MQM_data.keys():\n",
    "                sys = key[1]\n",
    "                seg_id = key[2]\n",
    "                hyp = key[3]\n",
    "                src = srcs[seg_id - 1]\n",
    "                ref = refs[seg_id - 1]\n",
    "                src_token = self.get_src_ref_token(src)\n",
    "                ref_token = self.get_src_ref_token(ref)\n",
    "                hyp_token = MQM_data[key]['target_token']\n",
    "                avg_score = MQM_data[key]['avg_score']\n",
    "                rater = MQM_data[key]['rater']\n",
    "                tagged_hyp = MQM_data[key]['tagged_hyp']\n",
    "                severity = MQM_data[key]['severity']\n",
    "                category = MQM_data[key]['category']\n",
    "                score_token = MQM_data[key]['score_token']\n",
    "                if self.agreement == 'low':\n",
    "                    avg_score_token = MQM_data[key]['avg_score_token']\n",
    "                elif self.agreement == 'high':\n",
    "                    avg_score_token = MQM_data[key]['maj_avg_score_token']\n",
    "                elif self.agreement == 'heavy':\n",
    "                    avg_score_token = MQM_data[key]['heavy_avg_score_token']\n",
    "                all_data.append(self.to_example(data_idx,\n",
    "                                                lang,\n",
    "                                                sys,\n",
    "                                                seg_id,\n",
    "                                                src,\n",
    "                                                ref,\n",
    "                                                \"ref-A\",\n",
    "                                                hyp,\n",
    "                                                tagged_hyp,\n",
    "                                                src_token,\n",
    "                                                ref_token,\n",
    "                                                hyp_token,\n",
    "                                                score_token,\n",
    "                                                avg_score_token,\n",
    "                                                avg_score,\n",
    "                                                rater,\n",
    "                                                severity,\n",
    "                                                category))\n",
    "                data_idx += 1\n",
    "        return all_data\n",
    "    \n",
    "    def get_google_MQM_data(self):\n",
    "        google_MQM_data = []\n",
    "        for lang in self.get_langs():\n",
    "            fname = self.get_MQM_fname(self.MQM_avg_files, lang)\n",
    "            rlines = self.load_file(fname)\n",
    "            avg_data = []\n",
    "            for i, line in enumerate(rlines):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                line = line.rstrip()\n",
    "                split_line = line.split(' ')\n",
    "                system = split_line[0]\n",
    "                score = split_line[1]\n",
    "                seg_id = split_line[2]\n",
    "                avg_data.append({'lang':lang,\n",
    "                                 'system':system,\n",
    "                                 'score':float(score)*(-1.0),\n",
    "                                 'seg_id':int(seg_id)})\n",
    "            google_MQM_data.extend(avg_data)\n",
    "        return google_MQM_data\n",
    "    \n",
    "    def to_json(self, data_idx, lang, sys_name, seg_id, src, ref, ref_id,\n",
    "                hyp, tagged_hyp, src_token, ref_token, hyp_token, token_score, mean_token_score,\n",
    "                mean_score,\n",
    "                rater, severity, category):\n",
    "        json_dict = {\"data_idx\":data_idx,\n",
    "                     \"year\": \"20\", \"lang\": lang, \"system\": sys_name,\n",
    "                     \"seg_id\": seg_id, \"src\": src, \n",
    "                     \"ref\": ref, \"ref_id\":ref_id,\n",
    "                     \"hyp\": hyp, \"tagged_hyp\":tagged_hyp,\n",
    "                     \"src_token\" : src_token,\n",
    "                     \"ref_token\" : ref_token, \n",
    "                     \"hyp_token\": hyp_token,\n",
    "                     \"token_score\": token_score,\n",
    "                     \"mean_token_score\" : mean_token_score,\n",
    "                     \"mean_score\": mean_score,\n",
    "                     \"rater\":rater,\n",
    "                     'severity':severity, 'category':category}\n",
    "        return json.dumps(json_dict)\n",
    "    \n",
    "    def to_example(self, data_idx, lang, sys_name, seg_id, src, ref, ref_id,\n",
    "                hyp, tagged_hyp, src_token, ref_token, hyp_token, token_score, mean_token_score, mean_score,\n",
    "                rater, severity, category):\n",
    "        return {\"data_idx\":data_idx, \"lang\": lang, \"system\": sys_name,\n",
    "                \"seg_id\": seg_id, \"src\": src, \n",
    "                \"ref\": ref, \"ref_id\":ref_id,\n",
    "                \"hyp\": hyp, \"tagged_hyp\":tagged_hyp,\n",
    "                \"src_token\": src_token,\n",
    "                \"ref_token\" : ref_token,\n",
    "                \"hyp_token\": hyp_token,\n",
    "                \"token_score\": token_score,\n",
    "                \"mean_token_score\": mean_token_score,\n",
    "                \"mean_score\": mean_score,\n",
    "                \"rater\":rater,\n",
    "                'severity':severity, 'category':category}\n",
    "    \n",
    "    def split_example(self, all_example):\n",
    "        all_dic = {}\n",
    "        train_example = []\n",
    "        dev_example = []\n",
    "        train_size = int(len(all_example)*(1-self.dev_ratio))\n",
    "        \n",
    "        for example in all_example:\n",
    "            key = (example['lang'], example['seg_id'])\n",
    "            if key not in all_dic:\n",
    "                all_dic[key] = []\n",
    "            all_dic[key].append(example)\n",
    "        key_list = list(all_dic.keys())\n",
    "        random.shuffle(key_list)\n",
    "        for key in key_list:\n",
    "            if len(train_example) < train_size:\n",
    "                train_example.extend(all_dic[key])\n",
    "            else:\n",
    "                dev_example.extend(all_dic[key])\n",
    "        return train_example, dev_example\n",
    "    \n",
    "    def write_file(self, examples, fname):\n",
    "        with open(fname, mode='w', encoding='utf-8') as w:\n",
    "            for ex in examples:\n",
    "                json_ex = self.to_json(ex['data_idx'], \n",
    "                                       ex['lang'],ex['system'],\n",
    "                                       ex['seg_id'],ex['src'],\n",
    "                                       ex['ref'],ex['ref_id'],\n",
    "                                       ex['hyp'], ex['tagged_hyp'], \n",
    "                                       ex['src_token'],\n",
    "                                       ex['ref_token'],\n",
    "                                       ex['hyp_token'],\n",
    "                                       ex['token_score'],\n",
    "                                       ex['mean_token_score'],\n",
    "                                       ex['mean_score'], \n",
    "                                       ex['rater'],\n",
    "                                       ex['severity'], ex['category'])\n",
    "                w.write(json_ex)\n",
    "                w.write('\\n')\n",
    "        print('{} example for {}'.format(len(examples), fname))\n",
    "                \n",
    "    def make_MQM_corpus(self):\n",
    "        all_example = self.get_MQM_data()\n",
    "        self.write_file(all_example, self.save_dir.replace('.json', '_full.json'))\n",
    "        \n",
    "        if self.split_dev:\n",
    "            train_example, dev_example = self.split_example(all_example)\n",
    "            self.write_file(train_example, self.save_dir.replace('.json', '_train.json'))\n",
    "            self.write_file(dev_example, self.save_dir.replace('.json', '_dev.json'))\n",
    "        \n",
    "    \n",
    "    def check_no_tag(self):\n",
    "        langs = self.get_langs()\n",
    "        no_tags = {}\n",
    "        for lang in langs:\n",
    "            no_tags[lang] = []\n",
    "            fname = self.get_MQM_fname(self.MQM_tag_20_files, lang)\n",
    "            rlines = self.load_file(fname)\n",
    "            for i, line in enumerate(rlines):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                line = line.rstrip()\n",
    "                split_line = line.split('\\t')\n",
    "                system = split_line[0]\n",
    "                doc = split_line[1]\n",
    "                doc_id = split_line[2]\n",
    "                seg_id = int(split_line[3])\n",
    "                rater = split_line[4]\n",
    "                source = split_line[5]\n",
    "                target = split_line[6]\n",
    "                category = split_line[7]\n",
    "                severity = split_line[8]\n",
    "                untagged_hyp = target.replace('<v>', '').replace('</v>', '')\n",
    "                if untagged_hyp == target and severity not in ['no-error', 'No-error', 'Neutral']:\n",
    "                    no_tags[lang].append({'hyp':target, 'severity':severity,\n",
    "                                          'category':category, 'rater':rater})\n",
    "        return no_tags\n",
    "\n",
    "importer = MQM_importer20('/ahc/work3/kosuke-t/WMT/wmt20_mqm_clean_xlm_low_agreement.json',\n",
    "                          emb_label=False, \n",
    "                          emb_only_sev=False,\n",
    "                          tokenizer_name='xlm-roberta-large',\n",
    "                          google_score=True, \n",
    "                          split_dev=True,\n",
    "                          remove_strange=True,\n",
    "                          agreement='low')\n",
    "importer.make_MQM_corpus()\n",
    "# importer = MQM_importer20('/ahc/work3/kosuke-t/WMT/wmt20_mqm_clean_xlm_high_agreement.json',\n",
    "#                           emb_label=False, \n",
    "#                           emb_only_sev=False,\n",
    "#                           tokenizer_name='xlm-roberta-large',\n",
    "#                           google_score=True, \n",
    "#                           split_dev=True,\n",
    "#                           remove_strange=True,\n",
    "#                           agreement='high')\n",
    "# importer.make_MQM_corpus()\n",
    "# importer = MQM_importer20('/ahc/work3/kosuke-t/WMT/wmt20_mqm_clean_xlm_high_agreement.json',\n",
    "#                           emb_label=False, \n",
    "#                           emb_only_sev=False,\n",
    "#                           tokenizer_name='xlm-roberta-large',\n",
    "#                           google_score=True, \n",
    "#                           split_dev=True,\n",
    "#                           remove_strange=True,\n",
    "#                           agreement='heavy')\n",
    "# importer.make_MQM_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbfbaf1-7dd0-4fb0-9b70-6df9c87635fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer = MQM_importer20('/ahc/work3/kosuke-t/WMT/wmt20_mqm.json',emb_label=False, \n",
    "#                           emb_only_sev=False, google_score=True, split_dev=True)\n",
    "# MQM_data = importer.get_MQM_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff2708a-98eb-46d2-b0f8-0b1623ef42e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a2d27-f7b9-467e-bd5d-974ec50a0bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c7cff9-6143-4a62-9092-b0861565993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HOME = '/ahc/work3/kosuke-t/WMT'\n",
    "\n",
    "MQM_1hot_only_severity_to_vec = {\"no-error\":np.asarray([1, 0, 0]), \n",
    "                                 \"No-error\":np.asarray([1, 0, 0]),\n",
    "                                 \"Neutral\":np.asarray([0, 1, 0]),\n",
    "                                 \"Minor\":np.asarray([0, 1, 0]),\n",
    "                                 \"Major\":np.asarray([0, 0, 1])}\n",
    "MQM_tag_list = [\"No-error\", \n",
    "                \"Neutral\",\n",
    "                \"Minor\",\n",
    "                \"Major\"]\n",
    "\n",
    "\n",
    "class MQM_importer21(MQM_importer20):\n",
    "    def __init__(self, save_dir, \n",
    "                 emb_label=False, emb_only_sev=True, \n",
    "                 no_error_score=np.asarray([1, 0, 0]),\n",
    "                 tokenizer_name='xlm-roberta-large',\n",
    "                 google_score=True, split_dev=False,\n",
    "                 dev_ratio=0.1, remove_strange=False,\n",
    "                 agreement='low'):\n",
    "        super(MQM_importer21, self).__init__(save_dir, \n",
    "                                             emb_label,\n",
    "                                             emb_only_sev,\n",
    "                                             no_error_score,\n",
    "                                             tokenizer_name=tokenizer_name,\n",
    "                                             google_score=google_score,\n",
    "                                             split_dev=split_dev,\n",
    "                                             dev_ratio=dev_ratio, \n",
    "                                             remove_strange=remove_strange,\n",
    "                                             agreement=agreement)\n",
    "        self.year = '21'\n",
    "        self.src_files = os.path.join(DATA_HOME,\n",
    "                                      'WMT21-data/sources/newstest2021.{}.src.{}'.format('LANG_LETTERS', 'LANG1_LETTERS'))\n",
    "        self.ref_files = os.path.join(DATA_HOME,\n",
    "                                      'WMT21-data/references/newstest2021.{}.ref.ref-A.{}'.format('LANG_LETTERS', 'LANG2_LETTERS'))\n",
    "        self.MQM_avg_files = os.path.join(DATA_HOME,\n",
    "                                          'wmt-mqm-human-evaluation/newstest2021/{}/mqm_newstest2021_{}.avg_seg_scores.tsv'.format('LANGWO_LETTERS', 'LANGWO_LETTERS'))\n",
    "        self.MQM_tag_files = os.path.join(DATA_HOME,\n",
    "                                          'wmt-mqm-human-evaluation/newstest2021/{}/mqm_newstest2021_{}.tsv'.format('LANGWO_LETTERS', 'LANGWO_LETTERS'))\n",
    "        self.langs = ['en-de', 'zh-en']\n",
    "        self.systems = {'en-de': ['hyp.Facebook-AI',\n",
    "                                  'hyp.HuaweiTSC',\n",
    "                                  'hyp.Nemo',\n",
    "                                  'hyp.Online-W',\n",
    "                                  'hyp.UEdin',\n",
    "                                  'hyp.VolcTrans-AT',\n",
    "                                  'hyp.VolcTrans-GLAT',\n",
    "                                  'hyp.eTranslation',\n",
    "                                  'hyp.metricsystem1',\n",
    "                                  'hyp.metricsystem2',\n",
    "                                  'hyp.metricsystem3',\n",
    "                                  'hyp.metricsystem4',\n",
    "                                  'hyp.metricsystem5',\n",
    "                                  'ref.A',\n",
    "                                  'ref.B',\n",
    "                                  'ref.C',\n",
    "                                  'ref.D'],\n",
    "                        'zh-en': ['hyp.Borderline',\n",
    "                                  'hyp.DIDI-NLP',\n",
    "                                  'hyp.Facebook-AI',\n",
    "                                  'hyp.IIE-MT',\n",
    "                                  'hyp.MiSS',\n",
    "                                  'hyp.NiuTrans',\n",
    "                                  'hyp.Online-W',\n",
    "                                  'hyp.SMU',\n",
    "                                  'hyp.metricsystem1',\n",
    "                                  'hyp.metricsystem2',\n",
    "                                  'hyp.metricsystem3',\n",
    "                                  'hyp.metricsystem4',\n",
    "                                  'hyp.metricsystem5',\n",
    "                                  'ref.A',\n",
    "                                  'ref.B']}\n",
    "    def get_srcs(self, lang):\n",
    "        lang1 = lang[:2]\n",
    "        fname = self.src_files.replace('LANG_LETTERS', lang).replace('LANG1_LETTERS', lang1)\n",
    "        src_data = self.load_file(fname)\n",
    "        src_data = [s.rstrip() for s in src_data]\n",
    "        return src_data\n",
    "    \n",
    "    def get_refs(self, lang):\n",
    "        lang2 = lang[-2:]\n",
    "        fname = self.ref_files.replace('LANG_LETTERS', lang).replace('LANG2_LETTERS', lang2)\n",
    "        ref_data = self.load_file(fname)\n",
    "        ref_data = [r.rstrip() for r in ref_data]\n",
    "        return ref_data\n",
    "    \n",
    "    # def get_hyps(self, lang):\n",
    "    #     hyp_data = {}\n",
    "    #     for sys in self.systems[lang]:\n",
    "    #         fname = self.hyp_files.replace('LANG', lang, 2).replace('SYSTEM', sys)\n",
    "    #         hyp_data[sys] = self.load_file(fname)\n",
    "    #         hyp_data[sys] = [h.rstrip() for h in hyp_data[sys]]\n",
    "    #     return hyp_data\n",
    "                \n",
    "importer = MQM_importer21('/ahc/work3/kosuke-t/WMT/wmt21_mqm_clean_xlm_high_agreement.json',\n",
    "                          emb_label=False, \n",
    "                          emb_only_sev=False,\n",
    "                          tokenizer_name='xlm-roberta-large',\n",
    "                          google_score=True, \n",
    "                          split_dev=True,\n",
    "                          remove_strange=True,\n",
    "                          agreement='high')\n",
    "importer.make_MQM_corpus()\n",
    "# no_tags = importer.check_no_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4d11d94-874d-4335-9267-1abe7d14d500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing 0 (seg_id, system) pairs, 0 sentences\n",
      "removing 0 (seg_id, system) pairs, 0 sentences\n",
      "removing 2900 (seg_id, system) pairs, 22656 sentences\n",
      "removing 7666 (seg_id, system) pairs, 64033 sentences\n",
      "38792 example for /ahc/work3/kosuke-t/WMT/WMT20_TED21_mqm_clean_mbert_full.json\n",
      "34921 example for /ahc/work3/kosuke-t/WMT/WMT20_TED21_mqm_clean_mbert_train.json\n",
      "3871 example for /ahc/work3/kosuke-t/WMT/WMT20_TED21_mqm_clean_mbert_dev.json\n"
     ]
    }
   ],
   "source": [
    "class TED_importer(MQM_importer21):\n",
    "    def __init__(self, save_dir,\n",
    "                 emb_label=False, emb_only_sev=True, \n",
    "                 no_error_score=np.asarray([1, 0, 0]),\n",
    "                 tokenizer_name='xlm-roberta-large',\n",
    "                 google_score=True, split_dev=False, \n",
    "                 combine_20=True, dev_ratio=0.1,\n",
    "                 remove_strange=False, agreement='low'):\n",
    "        super(TED_importer, self).__init__(save_dir, \n",
    "                                           emb_label,\n",
    "                                           emb_only_sev,\n",
    "                                           no_error_score,\n",
    "                                           tokenizer_name=tokenizer_name,\n",
    "                                           google_score=google_score,\n",
    "                                           split_dev=split_dev,\n",
    "                                           dev_ratio=dev_ratio,\n",
    "                                           remove_strange=remove_strange,\n",
    "                                           agreement=agreement)\n",
    "        self.importer20 = MQM_importer20(save_dir, \n",
    "                                           emb_label,\n",
    "                                             emb_only_sev,\n",
    "                                             no_error_score,\n",
    "                                             tokenizer_name=tokenizer_name,\n",
    "                                             google_score=google_score,\n",
    "                                             split_dev=split_dev,\n",
    "                                             dev_ratio=dev_ratio, \n",
    "                                         remove_strange=remove_strange,\n",
    "                                         agreement=agreement)\n",
    "        self.combine_20 = combine_20\n",
    "        self.year = 'TED21'\n",
    "        self.src_files = os.path.join(DATA_HOME,\n",
    "                                      'WMT21-data/sources/tedtalks.{}.src.{}'.format('LANG_LETTERS', 'LANG1_LETTERS'))\n",
    "        self.ref_files = os.path.join(DATA_HOME,\n",
    "                                      'WMT21-data/references/tedtalks.{}.ref.ref-A.{}'.format('LANG_LETTERS', 'LANG2_LETTERS'))\n",
    "        self.MQM_avg_files = os.path.join(DATA_HOME,\n",
    "                                          'wmt-mqm-human-evaluation/ted/{}/mqm_ted_{}.avg_seg_scores.tsv'.format('LANGWO_LETTERS', 'LANGWO_LETTERS'))\n",
    "        self.MQM_tag_files = os.path.join(DATA_HOME,\n",
    "                                          'wmt-mqm-human-evaluation/ted/{}/mqm_ted_{}.tsv'.format('LANGWO_LETTERS', 'LANGWO_LETTERS'))\n",
    "        self.langs = ['en-de', 'zh-en']\n",
    "        self.systems = {'ende': ['Facebook-AI',\n",
    "                                 'HuaweiTSC',\n",
    "                                 'Nemo',\n",
    "                                 'Online-W',\n",
    "                                 'UEdin',\n",
    "                                 'VolcTrans-AT',\n",
    "                                 'VolcTrans-GLAT',\n",
    "                                 'eTranslation',\n",
    "                                 'metricsystem1',\n",
    "                                 'metricsystem2',\n",
    "                                 'metricsystem3',\n",
    "                                 'metricsystem4',\n",
    "                                 'metricsystem5',\n",
    "                                 'ref'],\n",
    "                        'zhen': ['Borderline',\n",
    "                                 'DIDI-NLP',\n",
    "                                 'Facebook-AI',\n",
    "                                 'IIE-MT',\n",
    "                                 'MiSS',\n",
    "                                 'NiuTrans',\n",
    "                                 'Online-W',\n",
    "                                 'SMU',\n",
    "                                 'metricsystem1',\n",
    "                                 'metricsystem2',\n",
    "                                 'metricsystem3',\n",
    "                                 'metricsystem4',\n",
    "                                 'metricsystem5',\n",
    "                                 'ref',\n",
    "                                 'refB']}\n",
    "        \n",
    "    def make_MQM_corpus(self):\n",
    "        all_example = self.get_MQM_data()\n",
    "        if self.combine_20:\n",
    "            example_20 = self.importer20.get_MQM_data(len(all_example))\n",
    "            all_example.extend(example_20)\n",
    "        self.write_file(all_example, self.save_dir.replace('.json', '_full.json'))\n",
    "        \n",
    "        if self.split_dev:\n",
    "            train_example, dev_example = self.split_example(all_example)\n",
    "            self.write_file(train_example, self.save_dir.replace('.json', '_train.json'))\n",
    "            self.write_file(dev_example, self.save_dir.replace('.json', '_dev.json'))\n",
    "        \n",
    "        \n",
    "importer = TED_importer('/ahc/work3/kosuke-t/WMT/WMT20_TED21_mqm_clean_xlm_high_agreement.json',\n",
    "                        emb_label=False, \n",
    "                        emb_only_sev=False,\n",
    "                        tokenizer_name='xlm-roberta-large',\n",
    "                        google_score=True,\n",
    "                        split_dev=True,\n",
    "                        combine_20=True,\n",
    "                        dev_ratio=0.1,\n",
    "                        remove_strange=True,\n",
    "                        agreement='high')\n",
    "importer.make_MQM_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef933f2b-1eec-4ea2-8bfa-0344ca62f6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58312 52492 5820\n",
      "58312 example for /ahc/work3/kosuke-t/WMT/wmt20-21_mqm_clean_full.json\n",
      "52492 example for /ahc/work3/kosuke-t/WMT/wmt20-21_mqm_clean_train.json\n",
      "5820 example for /ahc/work3/kosuke-t/WMT/wmt20-21_mqm_clean_dev.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle \n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "\n",
    "fname1 = '/ahc/work3/kosuke-t/WMT/WMT20_TED21_mqm_clean_full.json'\n",
    "fname2 = '/ahc/work3/kosuke-t/WMT/wmt21_mqm_clean_full.json'\n",
    "\n",
    "def to_json(data_idx, lang, sys_name, seg_id, src, ref, ref_id,\n",
    "                hyp, tagged_hyp, src_token, ref_token, hyp_token, token_score, mean_token_score,\n",
    "                mean_score,\n",
    "                rater, severity, category):\n",
    "        json_dict = {\"data_idx\":data_idx,\n",
    "                     \"year\": \"20\", \"lang\": lang, \"system\": sys_name,\n",
    "                     \"seg_id\": seg_id, \"src\": src, \n",
    "                     \"ref\": ref, \"ref_id\":ref_id,\n",
    "                     \"hyp\": hyp, \"tagged_hyp\":tagged_hyp,\n",
    "                     \"src_token\" : src_token,\n",
    "                     \"ref_token\" : ref_token, \n",
    "                     \"hyp_token\": hyp_token,\n",
    "                     \"token_score\": token_score,\n",
    "                     \"mean_token_score\" : mean_token_score,\n",
    "                     \"mean_score\": mean_score,\n",
    "                     \"rater\":rater,\n",
    "                     'severity':severity, 'category':category}\n",
    "        return json.dumps(json_dict)\n",
    "\n",
    "def write_file(examples, fname):\n",
    "    with open(fname, mode='w', encoding='utf-8') as w:\n",
    "        for ex in examples:\n",
    "            json_ex = to_json(int(ex['data_idx']), \n",
    "                                   ex['lang'],ex['system'],\n",
    "                                   int(ex['seg_id']),ex['src'],\n",
    "                                   ex['ref'],ex['ref_id'],\n",
    "                                   ex['hyp'], ex['tagged_hyp'], \n",
    "                                   ex['src_token'],\n",
    "                                   ex['ref_token'],\n",
    "                                   ex['hyp_token'],\n",
    "                                   ex['token_score'],\n",
    "                                   ex['mean_token_score'],\n",
    "                                   ex['mean_score'], \n",
    "                                   ex['rater'],\n",
    "                                   ex['severity'], ex['category'])\n",
    "            w.write(json_ex)\n",
    "            w.write('\\n')\n",
    "    print('{} example for {}'.format(len(examples), fname))\n",
    "\n",
    "def df_to_example(df):\n",
    "    all_example = []\n",
    "    for i in range(len(df)):\n",
    "        all_example.append(df.iloc[i])\n",
    "    return all_example\n",
    "    \n",
    "def split_example(all_example, dev_ratio=0.1):\n",
    "    all_dic = {}\n",
    "    train_example = []\n",
    "    dev_example = []\n",
    "    train_size = int(len(all_example)*(1-dev_ratio))\n",
    "\n",
    "    for example in all_example:\n",
    "        key = (example['lang'], example['seg_id'])\n",
    "        if key not in all_dic:\n",
    "            all_dic[key] = []\n",
    "        all_dic[key].append(example)\n",
    "    key_list = list(all_dic.keys())\n",
    "    random.shuffle(key_list)\n",
    "    for key in key_list:\n",
    "        if len(train_example) < train_size:\n",
    "            train_example.extend(all_dic[key])\n",
    "        else:\n",
    "            dev_example.extend(all_dic[key])\n",
    "    return train_example, dev_example\n",
    "    \n",
    "def load_data(fname):\n",
    "    with open(fname, \"r\") as f:\n",
    "        df = pd.read_json(f, lines=True)\n",
    "    return df\n",
    "\n",
    "df1 = load_data(fname1)\n",
    "df2 = load_data(fname2)\n",
    "df = pd.concat([df1, df2])\n",
    "all_example = df_to_example(df)\n",
    "train_example, dev_example = split_example(all_example)\n",
    "\n",
    "write_file(all_example, '/ahc/work3/kosuke-t/WMT/wmt20-21_mqm_clean_full.json')\n",
    "write_file(train_example, '/ahc/work3/kosuke-t/WMT/wmt20-21_mqm_clean_train.json')\n",
    "write_file(dev_example, '/ahc/work3/kosuke-t/WMT/wmt20-21_mqm_clean_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc629a78-c373-4b9d-a005-aab0b30e9794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['src', 'mt', 'ref', 'score', 'system', 'testset', 'lp']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pprint\n",
    "fname = '/ahc/work3/kosuke-t/WMT/WMT21-data/wmt-enru-newstest2021.csv'\n",
    "\n",
    "def get_tmp_dic(src, mt, ref, score, sys, testset, lang):\n",
    "    return {'src':src, 'hyp':mt, 'ref':ref, 'label':float(score),\n",
    "            'system':sys, 'testset':testset, 'lang':lang}\n",
    "\n",
    "def load_csv_MQM(fname):\n",
    "    data = []\n",
    "    with open(fname, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)    \n",
    "        for i, row in enumerate(reader):\n",
    "            if i == 0:\n",
    "                print(row)\n",
    "                continue\n",
    "            data.append(get_tmp_dic(row[0], row[1], row[2], row[3],\n",
    "                                    row[4], row[5], row[6]))\n",
    "    return data\n",
    "\n",
    "data = load_csv_MQM(fname)\n",
    "with open('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd8e91db-8820-4267-8b06-5055f8fc9c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-400: 1,\n",
       " -350: 0,\n",
       " -300: 0,\n",
       " -250: 0,\n",
       " -200: 0,\n",
       " -150: 3,\n",
       " -100: 4,\n",
       " -50: 22,\n",
       " 0: 72,\n",
       " 50: 496,\n",
       " 100: 7834}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_data = sorted(data, key=lambda x: x['label'])\n",
    "count = {k:0 for k in range(-400, 101, 50)}\n",
    "for d in sorted_data:\n",
    "    score = d['label']\n",
    "    for k in count.keys():\n",
    "        if score <= k:\n",
    "            count[k] += 1\n",
    "            break\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e2c83-abf7-4bc2-95d1-110a8e9da7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
